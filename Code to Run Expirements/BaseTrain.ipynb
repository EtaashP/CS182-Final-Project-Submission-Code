{"cells":[{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1886},"executionInfo":{"elapsed":2287179,"status":"error","timestamp":1764750875202,"user":{"displayName":"Etaash Tripathi Patel","userId":"11287900455267433447"},"user_tz":480},"id":"mYSAERkkgOXn","outputId":"bac2690f-c13a-4716-de07-5d3b8c884027"},"outputs":[{"output_type":"stream","name":"stdout","text":["device: cuda\n","total runs: 1\n","\n","================================================================\n","run 1/1 | lr=0.001 wd=0.02 bs=512 drop_path_rate=0.2\n","================================================================\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(\n","/tmp/ipython-input-3764318045.py:294: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n","  scaler = torch.cuda.amp.GradScaler(enabled=torch.cuda.is_available())\n","/tmp/ipython-input-3764318045.py:250: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n"]},{"output_type":"stream","name":"stdout","text":["epoch 001/150 | loss 2.1736 | train_acc 20.22% | test_acc 23.84% | dl_time 1.58s | train_time 25.88s | eval_time 4.13s\n","epoch 002/150 | loss 1.8961 | train_acc 29.06% | test_acc 33.79% | dl_time 1.58s | train_time 25.89s | eval_time 4.59s\n","epoch 003/150 | loss 1.6990 | train_acc 36.61% | test_acc 42.49% | dl_time 1.58s | train_time 26.15s | eval_time 4.59s\n","epoch 004/150 | loss 1.5046 | train_acc 44.86% | test_acc 50.21% | dl_time 1.58s | train_time 26.26s | eval_time 5.15s\n","epoch 005/150 | loss 1.3669 | train_acc 50.28% | test_acc 52.44% | dl_time 1.58s | train_time 26.26s | eval_time 4.02s\n","epoch 006/150 | loss 1.3106 | train_acc 52.60% | test_acc 53.53% | dl_time 1.58s | train_time 25.53s | eval_time 4.04s\n","epoch 007/150 | loss 1.2293 | train_acc 55.61% | test_acc 57.32% | dl_time 1.58s | train_time 25.89s | eval_time 4.27s\n","epoch 008/150 | loss 1.1939 | train_acc 56.78% | test_acc 59.17% | dl_time 1.58s | train_time 25.47s | eval_time 4.02s\n","epoch 009/150 | loss 1.1460 | train_acc 58.74% | test_acc 60.93% | dl_time 1.58s | train_time 26.17s | eval_time 4.12s\n","epoch 010/150 | loss 1.0971 | train_acc 60.82% | test_acc 62.94% | dl_time 1.58s | train_time 25.59s | eval_time 4.00s\n","epoch 011/150 | loss 1.0465 | train_acc 62.48% | test_acc 63.11% | dl_time 1.58s | train_time 25.28s | eval_time 4.23s\n","epoch 012/150 | loss 1.0086 | train_acc 63.94% | test_acc 65.49% | dl_time 1.58s | train_time 25.68s | eval_time 4.09s\n","epoch 013/150 | loss 0.9725 | train_acc 65.19% | test_acc 67.25% | dl_time 1.58s | train_time 25.62s | eval_time 4.02s\n","epoch 014/150 | loss 0.9385 | train_acc 66.47% | test_acc 68.75% | dl_time 1.58s | train_time 25.55s | eval_time 4.39s\n","epoch 015/150 | loss 0.9029 | train_acc 67.94% | test_acc 68.93% | dl_time 1.58s | train_time 25.49s | eval_time 4.14s\n","epoch 016/150 | loss 0.8617 | train_acc 69.41% | test_acc 69.78% | dl_time 1.58s | train_time 25.94s | eval_time 4.11s\n","epoch 017/150 | loss 0.8439 | train_acc 70.02% | test_acc 70.56% | dl_time 1.58s | train_time 26.06s | eval_time 4.34s\n","epoch 018/150 | loss 0.8135 | train_acc 71.35% | test_acc 72.50% | dl_time 1.58s | train_time 25.46s | eval_time 4.05s\n","epoch 019/150 | loss 0.7897 | train_acc 72.03% | test_acc 72.50% | dl_time 1.58s | train_time 25.32s | eval_time 4.08s\n","epoch 020/150 | loss 0.7694 | train_acc 72.55% | test_acc 73.41% | dl_time 1.58s | train_time 25.46s | eval_time 4.49s\n","epoch 021/150 | loss 0.7486 | train_acc 73.50% | test_acc 74.15% | dl_time 1.58s | train_time 25.25s | eval_time 3.87s\n","epoch 022/150 | loss 0.7280 | train_acc 74.39% | test_acc 75.35% | dl_time 1.58s | train_time 25.26s | eval_time 4.02s\n","epoch 023/150 | loss 0.6925 | train_acc 75.50% | test_acc 76.50% | dl_time 1.58s | train_time 25.56s | eval_time 3.96s\n","epoch 024/150 | loss 0.6881 | train_acc 75.83% | test_acc 75.89% | dl_time 1.58s | train_time 24.96s | eval_time 4.10s\n","epoch 025/150 | loss 0.6624 | train_acc 76.57% | test_acc 76.89% | dl_time 1.58s | train_time 25.66s | eval_time 3.94s\n","epoch 026/150 | loss 0.6490 | train_acc 77.06% | test_acc 76.67% | dl_time 1.58s | train_time 25.55s | eval_time 4.07s\n","epoch 027/150 | loss 0.6314 | train_acc 77.69% | test_acc 76.63% | dl_time 1.58s | train_time 25.74s | eval_time 4.31s\n","epoch 028/150 | loss 0.6115 | train_acc 78.34% | test_acc 78.43% | dl_time 1.58s | train_time 25.70s | eval_time 4.05s\n","epoch 029/150 | loss 0.5886 | train_acc 79.28% | test_acc 77.92% | dl_time 1.58s | train_time 25.64s | eval_time 4.05s\n","epoch 030/150 | loss 0.5759 | train_acc 79.56% | test_acc 78.68% | dl_time 1.58s | train_time 25.73s | eval_time 4.38s\n","epoch 031/150 | loss 0.5566 | train_acc 80.22% | test_acc 79.32% | dl_time 1.58s | train_time 25.43s | eval_time 4.03s\n","epoch 032/150 | loss 0.5416 | train_acc 80.73% | test_acc 79.02% | dl_time 1.58s | train_time 25.63s | eval_time 3.98s\n","epoch 033/150 | loss 0.5330 | train_acc 81.13% | test_acc 78.60% | dl_time 1.58s | train_time 26.20s | eval_time 4.54s\n","epoch 034/150 | loss 0.5168 | train_acc 81.46% | test_acc 78.50% | dl_time 1.58s | train_time 25.69s | eval_time 4.10s\n","epoch 035/150 | loss 0.5047 | train_acc 82.10% | test_acc 80.66% | dl_time 1.58s | train_time 25.74s | eval_time 4.02s\n","epoch 036/150 | loss 0.4896 | train_acc 82.64% | test_acc 80.35% | dl_time 1.58s | train_time 25.64s | eval_time 4.55s\n","epoch 037/150 | loss 0.4772 | train_acc 83.25% | test_acc 80.79% | dl_time 1.58s | train_time 25.42s | eval_time 4.02s\n","epoch 038/150 | loss 0.4658 | train_acc 83.50% | test_acc 80.30% | dl_time 1.58s | train_time 25.56s | eval_time 3.94s\n","epoch 039/150 | loss 0.4505 | train_acc 83.96% | test_acc 81.23% | dl_time 1.58s | train_time 25.79s | eval_time 4.17s\n","epoch 040/150 | loss 0.4361 | train_acc 84.65% | test_acc 80.85% | dl_time 1.58s | train_time 24.97s | eval_time 4.07s\n","epoch 041/150 | loss 0.4219 | train_acc 85.15% | test_acc 80.30% | dl_time 1.58s | train_time 25.70s | eval_time 4.10s\n","epoch 042/150 | loss 0.4163 | train_acc 85.23% | test_acc 81.52% | dl_time 1.58s | train_time 25.88s | eval_time 4.12s\n","epoch 043/150 | loss 0.4027 | train_acc 85.73% | test_acc 82.05% | dl_time 1.58s | train_time 25.42s | eval_time 4.33s\n","epoch 044/150 | loss 0.3850 | train_acc 86.14% | test_acc 81.33% | dl_time 1.58s | train_time 25.50s | eval_time 3.96s\n","epoch 045/150 | loss 0.3747 | train_acc 86.63% | test_acc 81.04% | dl_time 1.58s | train_time 25.63s | eval_time 4.01s\n","epoch 046/150 | loss 0.3666 | train_acc 86.98% | test_acc 81.20% | dl_time 1.58s | train_time 25.45s | eval_time 4.47s\n","epoch 047/150 | loss 0.3506 | train_acc 87.54% | test_acc 80.96% | dl_time 1.58s | train_time 25.56s | eval_time 4.08s\n","epoch 048/150 | loss 0.3397 | train_acc 87.79% | test_acc 82.23% | dl_time 1.58s | train_time 25.52s | eval_time 4.07s\n","epoch 049/150 | loss 0.3283 | train_acc 88.39% | test_acc 81.36% | dl_time 1.58s | train_time 25.53s | eval_time 4.49s\n","epoch 050/150 | loss 0.3182 | train_acc 88.58% | test_acc 81.44% | dl_time 1.58s | train_time 25.08s | eval_time 4.05s\n","epoch 051/150 | loss 0.3093 | train_acc 88.97% | test_acc 81.72% | dl_time 1.58s | train_time 25.40s | eval_time 4.11s\n","epoch 052/150 | loss 0.3024 | train_acc 89.25% | test_acc 81.09% | dl_time 1.58s | train_time 25.51s | eval_time 4.10s\n","epoch 053/150 | loss 0.2883 | train_acc 89.65% | test_acc 82.35% | dl_time 1.58s | train_time 24.99s | eval_time 3.96s\n","epoch 054/150 | loss 0.2745 | train_acc 90.11% | test_acc 82.13% | dl_time 1.58s | train_time 24.99s | eval_time 3.88s\n","epoch 055/150 | loss 0.2636 | train_acc 90.48% | test_acc 82.28% | dl_time 1.58s | train_time 25.00s | eval_time 3.90s\n","epoch 056/150 | loss 0.2548 | train_acc 90.90% | test_acc 82.33% | dl_time 1.58s | train_time 24.88s | eval_time 4.26s\n","epoch 057/150 | loss 0.2494 | train_acc 91.07% | test_acc 82.51% | dl_time 1.58s | train_time 25.08s | eval_time 3.98s\n","epoch 058/150 | loss 0.2326 | train_acc 91.64% | test_acc 82.45% | dl_time 1.58s | train_time 24.72s | eval_time 3.87s\n","epoch 059/150 | loss 0.2252 | train_acc 91.93% | test_acc 82.43% | dl_time 1.58s | train_time 25.30s | eval_time 4.00s\n","epoch 060/150 | loss 0.2184 | train_acc 92.05% | test_acc 82.33% | dl_time 1.58s | train_time 24.83s | eval_time 3.99s\n","epoch 061/150 | loss 0.2055 | train_acc 92.52% | test_acc 82.59% | dl_time 1.58s | train_time 24.83s | eval_time 3.99s\n","epoch 062/150 | loss 0.2019 | train_acc 92.83% | test_acc 82.72% | dl_time 1.58s | train_time 25.77s | eval_time 4.08s\n","epoch 063/150 | loss 0.1899 | train_acc 93.12% | test_acc 82.02% | dl_time 1.58s | train_time 25.23s | eval_time 4.31s\n","epoch 064/150 | loss 0.1880 | train_acc 93.29% | test_acc 82.42% | dl_time 1.58s | train_time 25.21s | eval_time 3.93s\n","epoch 065/150 | loss 0.1791 | train_acc 93.58% | test_acc 82.91% | dl_time 1.58s | train_time 25.16s | eval_time 4.04s\n","epoch 066/150 | loss 0.1687 | train_acc 93.91% | test_acc 82.88% | dl_time 1.58s | train_time 25.33s | eval_time 4.13s\n","epoch 067/150 | loss 0.1570 | train_acc 94.38% | test_acc 82.70% | dl_time 1.58s | train_time 24.83s | eval_time 3.95s\n","epoch 068/150 | loss 0.1509 | train_acc 94.68% | test_acc 82.59% | dl_time 1.58s | train_time 25.16s | eval_time 4.03s\n","epoch 069/150 | loss 0.1509 | train_acc 94.61% | test_acc 82.70% | dl_time 1.58s | train_time 25.21s | eval_time 4.15s\n","epoch 070/150 | loss 0.1331 | train_acc 95.31% | test_acc 83.33% | dl_time 1.58s | train_time 24.99s | eval_time 4.27s\n","epoch 071/150 | loss 0.1343 | train_acc 95.21% | test_acc 83.52% | dl_time 1.58s | train_time 25.14s | eval_time 4.07s\n","epoch 072/150 | loss 0.1309 | train_acc 95.38% | test_acc 83.38% | dl_time 1.58s | train_time 25.26s | eval_time 4.07s\n","epoch 073/150 | loss 0.1222 | train_acc 95.65% | test_acc 83.42% | dl_time 1.58s | train_time 25.46s | eval_time 4.39s\n","epoch 074/150 | loss 0.1217 | train_acc 95.64% | test_acc 83.83% | dl_time 1.58s | train_time 24.92s | eval_time 3.95s\n","epoch 075/150 | loss 0.1137 | train_acc 95.87% | test_acc 83.34% | dl_time 1.58s | train_time 25.00s | eval_time 4.03s\n","epoch 076/150 | loss 0.1108 | train_acc 96.06% | test_acc 83.41% | dl_time 1.58s | train_time 24.99s | eval_time 3.95s\n","epoch 077/150 | loss 0.1067 | train_acc 96.20% | test_acc 83.15% | dl_time 1.58s | train_time 24.81s | eval_time 4.35s\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-3764318045.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 382\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/tmp/ipython-input-3764318045.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    365\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"=\"\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m         \u001b[0mcfg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRunConfig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_decay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop_path_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdpr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 367\u001b[0;31m         res = run_training(cfg, epochs=args.epochs, warmup_epochs=args.warmup_epochs,\n\u001b[0m\u001b[1;32m    368\u001b[0m                            min_lr=args.min_lr, device=device, num_workers=args.num_workers)\n\u001b[1;32m    369\u001b[0m         \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipython-input-3764318045.py\u001b[0m in \u001b[0;36mrun_training\u001b[0;34m(cfg, epochs, warmup_epochs, min_lr, device, num_workers)\u001b[0m\n\u001b[1;32m    301\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m         \u001b[0mstart_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 303\u001b[0;31m         \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_one_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    304\u001b[0m         \u001b[0mend_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m         \u001b[0mtrain_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mend_train\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipython-input-3764318045.py\u001b[0m in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, loader, optimizer, scaler, device, mixup_alpha)\u001b[0m\n\u001b[1;32m    252\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 254\u001b[0;31m         \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    255\u001b[0m         \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    623\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m             )\n\u001b[0;32m--> 625\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    626\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    352\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    355\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    839\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    840\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 841\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    842\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    843\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["import math\n","import time\n","import itertools\n","import random\n","import argparse\n","from dataclasses import dataclass\n","from typing import Tuple, Dict, Any, List\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import DataLoader\n","from torch.optim import AdamW\n","from torch.optim.lr_scheduler import _LRScheduler\n","from torchvision import datasets, transforms\n","\n","# ---------------------------\n","# Utilities\n","# ---------------------------\n","\n","def set_seed(seed: int = 42):\n","    random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed_all(seed)\n","\n","def accuracy(logits: torch.Tensor, targets: torch.Tensor) -> float:\n","    preds = logits.argmax(dim=1)\n","    return (preds == targets).float().mean().item()\n","\n","# Warmup + Cosine LR\n","class WarmupCosineLR(_LRScheduler):\n","    def __init__(self, optimizer, total_steps, warmup_steps=0, min_lr=1e-5, last_epoch=-1):\n","        self.total_steps = total_steps\n","        self.warmup_steps = warmup_steps\n","        self.min_lr = min_lr\n","        super().__init__(optimizer, last_epoch)\n","\n","    def get_lr(self):\n","        step = self.last_epoch + 1\n","        lrs = []\n","        for base_lr in self.base_lrs:\n","            if step < self.warmup_steps:\n","                lr = base_lr * float(step) / float(max(1, self.warmup_steps))\n","            else:\n","                progress = (step - self.warmup_steps) / float(max(1, self.total_steps - self.warmup_steps))\n","                cosine = 0.5 * (1.0 + math.cos(math.pi * progress))\n","                lr = self.min_lr + (base_lr - self.min_lr) * cosine\n","            lrs.append(lr)\n","        return lrs\n","\n","# ---------------------------\n","# Stochastic depth (DropPath)\n","# ---------------------------\n","\n","class DropPath(nn.Module):\n","    def __init__(self, drop_prob: float = 0.0):\n","        super().__init__()\n","        self.drop_prob = drop_prob\n","\n","    def forward(self, x):\n","        if self.drop_prob == 0.0 or not self.training:\n","            return x\n","        keep_prob = 1.0 - self.drop_prob\n","        shape = (x.shape[0],) + (1,) * (x.ndim - 1)\n","        random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)\n","        random_tensor.floor_()\n","        return x.div(keep_prob) * random_tensor\n","\n","# ---------------------------\n","# ViT-Tiny for 32x32, patch=4\n","# ---------------------------\n","\n","class MLP(nn.Module):\n","    def __init__(self, dim, mlp_ratio=4, drop=0.0):\n","        super().__init__()\n","        hidden = int(dim * mlp_ratio)\n","        self.fc1 = nn.Linear(dim, hidden)\n","        self.act = nn.GELU()\n","        self.fc2 = nn.Linear(hidden, dim)\n","        self.drop = nn.Dropout(drop)\n","\n","    def forward(self, x):\n","        x = self.fc1(x)\n","        x = self.act(x)\n","        x = self.drop(x)\n","        x = self.fc2(x)\n","        x = self.drop(x)\n","        return x\n","\n","class Attention(nn.Module):\n","    def __init__(self, dim, num_heads=6, qkv_bias=True, attn_drop=0.0, proj_drop=0.0):\n","        super().__init__()\n","        assert dim % num_heads == 0\n","        self.num_heads = num_heads\n","        self.head_dim = dim // num_heads\n","        self.scale = self.head_dim ** -0.5\n","\n","        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n","        self.attn_drop = nn.Dropout(attn_drop)\n","        self.proj = nn.Linear(dim, dim)\n","        self.proj_drop = nn.Dropout(proj_drop)\n","\n","    def forward(self, x):\n","        B, N, C = x.shape\n","        qkv = self.qkv(x)  # B, N, 3C\n","        qkv = qkv.reshape(B, N, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n","        q, k, v = qkv[0], qkv[1], qkv[2]  # each: B, heads, N, head_dim\n","\n","        attn = (q @ k.transpose(-2, -1)) * self.scale\n","        attn = attn.softmax(dim=-1)\n","        attn = self.attn_drop(attn)\n","\n","        out = attn @ v  # B, heads, N, head_dim\n","        out = out.transpose(1, 2).reshape(B, N, C)\n","        out = self.proj(out)\n","        out = self.proj_drop(out)\n","        return out\n","\n","class Block(nn.Module):\n","    def __init__(self, dim, num_heads, mlp_ratio, drop=0.0, attn_drop=0.0, drop_path=0.0):\n","        super().__init__()\n","        self.norm1 = nn.LayerNorm(dim)\n","        self.attn = Attention(dim, num_heads=num_heads, attn_drop=attn_drop, proj_drop=drop)\n","        self.drop_path = DropPath(drop_path)\n","        self.norm2 = nn.LayerNorm(dim)\n","        self.mlp = MLP(dim, mlp_ratio=mlp_ratio, drop=drop)\n","\n","    def forward(self, x):\n","        x = x + self.drop_path(self.attn(self.norm1(x)))\n","        x = x + self.drop_path(self.mlp(self.norm2(x)))\n","        return x\n","\n","class PatchEmbed(nn.Module):\n","    def __init__(self, img_size=32, patch_size=4, in_chans=3, embed_dim=384):\n","        super().__init__()\n","        assert img_size % patch_size == 0\n","        self.grid = img_size // patch_size  # 8\n","        self.num_patches = self.grid * self.grid  # 64\n","        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n","\n","    def forward(self, x):\n","        x = self.proj(x)  # B, C, 8, 8\n","        x = x.flatten(2).transpose(1, 2)  # B, 64, C\n","        return x\n","\n","class ViTSmallCIFAR(nn.Module):\n","    def __init__(self, num_classes=10, img_size=32, patch_size=8,\n","                 embed_dim=192, depth=10, num_heads=3, mlp_ratio=4.0,\n","                 drop_rate=0.0, attn_drop_rate=0.0, drop_path_rate=0.0,\n","                 cls_norm=True):\n","        super().__init__()\n","        self.patch_embed = PatchEmbed(img_size, patch_size, 3, embed_dim)\n","        num_patches = self.patch_embed.num_patches\n","\n","        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n","        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim))\n","        self.pos_drop = nn.Dropout(p=drop_rate)\n","\n","        # stochastic depth decay rule\n","        dpr = torch.linspace(0, drop_path_rate, steps=depth).tolist()\n","\n","        self.blocks = nn.ModuleList([\n","            Block(embed_dim, num_heads, mlp_ratio,\n","                  drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i])\n","            for i in range(depth)\n","        ])\n","        self.norm = nn.LayerNorm(embed_dim)\n","        self.cls_norm = cls_norm\n","        self.head = nn.Linear(embed_dim, num_classes)\n","\n","        nn.init.trunc_normal_(self.pos_embed, std=0.02)\n","        nn.init.trunc_normal_(self.cls_token, std=0.02)\n","        self.apply(self._init_weights)\n","\n","    def _init_weights(self, m):\n","        if isinstance(m, nn.Linear):\n","            nn.init.trunc_normal_(m.weight, std=0.02)\n","            if m.bias is not None:\n","                nn.init.zeros_(m.bias)\n","        elif isinstance(m, nn.LayerNorm):\n","            nn.init.ones_(m.weight)\n","            nn.init.zeros_(m.bias)\n","\n","    def forward(self, x):\n","        B = x.shape[0]\n","        x = self.patch_embed(x)  # B, 64, C\n","        cls = self.cls_token.expand(B, -1, -1)\n","        x = torch.cat([cls, x], dim=1)  # B, 65, C\n","        x = x + self.pos_embed\n","        x = self.pos_drop(x)\n","        for blk in self.blocks:\n","            x = blk(x)\n","        if self.cls_norm:\n","            x = self.norm(x)\n","        cls_tok = x[:, 0]\n","        logits = self.head(cls_tok)\n","        return logits\n","\n","# ---------------------------\n","# Data\n","# ---------------------------\n","\n","def build_dataloaders(batch_size: int, num_workers: int = 4) -> Tuple[DataLoader, DataLoader]:\n","    # Standard CIFAR-10 augments\n","    train_tf = transforms.Compose([\n","        transforms.RandomCrop(32, padding=4),\n","        transforms.RandomHorizontalFlip(),\n","        transforms.ToTensor(),\n","        transforms.Normalize(mean=(0.4914, 0.4822, 0.4465),\n","                             std=(0.2470, 0.2435, 0.2616)),\n","    ])\n","    test_tf = transforms.Compose([\n","        transforms.ToTensor(),\n","        transforms.Normalize(mean=(0.4914, 0.4822, 0.4465),\n","                             std=(0.2470, 0.2435, 0.2616)),\n","    ])\n","\n","    train_ds = datasets.CIFAR10(root=\"./data\", train=True, transform=train_tf, download=True)\n","    test_ds = datasets.CIFAR10(root=\"./data\", train=False, transform=test_tf, download=True)\n","\n","    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True,\n","                              num_workers=num_workers, pin_memory=True)\n","    test_loader = DataLoader(test_ds, batch_size=512, shuffle=False,\n","                             num_workers=num_workers, pin_memory=True)\n","    return train_loader, test_loader\n","\n","# ---------------------------\n","# Train / Eval\n","# ---------------------------\n","\n","@dataclass\n","class RunConfig:\n","    lr: float\n","    weight_decay: float\n","    batch_size: int\n","    drop_path_rate: float\n","\n","def train_one_epoch(model, loader, optimizer, scaler, device, mixup_alpha=None):\n","    model.train()\n","    total_loss = 0.0\n","    total_acc = 0.0\n","    n = 0\n","    criterion = nn.CrossEntropyLoss()\n","\n","    for images, targets in loader:\n","        images = images.to(device, non_blocking=True)\n","        targets = targets.to(device, non_blocking=True)\n","\n","        optimizer.zero_grad(set_to_none=True)\n","        with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n","            logits = model(images)\n","            loss = criterion(logits, targets)\n","\n","        scaler.scale(loss).backward()\n","        scaler.step(optimizer)\n","        scaler.update()\n","\n","        bs = images.size(0)\n","        total_loss += loss.item() * bs\n","        total_acc += accuracy(logits.detach(), targets) * bs\n","        n += bs\n","\n","    return total_loss / n, total_acc / n\n","\n","@torch.no_grad()\n","def evaluate(model, loader, device):\n","    model.eval()\n","    total_acc = 0.0\n","    n = 0\n","    for images, targets in loader:\n","        images = images.to(device, non_blocking=True)\n","        targets = targets.to(device, non_blocking=True)\n","        logits = model(images)\n","        bs = images.size(0)\n","        total_acc += accuracy(logits, targets) * bs\n","        n += bs\n","    return total_acc / n\n","\n","def run_training(cfg: RunConfig, epochs: int, warmup_epochs: int, min_lr: float,\n","                 device: torch.device, num_workers: int = 4) -> Dict[str, Any]:\n","\n","    start_dataloaders = time.time()\n","    train_loader, test_loader = build_dataloaders(cfg.batch_size, num_workers)\n","    end_dataloaders = time.time()\n","    dataloader_time = end_dataloaders - start_dataloaders\n","\n","    model = ViTSmallCIFAR(\n","        num_classes=10, img_size=32, patch_size=4,\n","        embed_dim=192, depth=12, num_heads=3, mlp_ratio=4.0,\n","        drop_rate=0.0, attn_drop_rate=0.0, drop_path_rate=cfg.drop_path_rate\n","    ).to(device)\n","\n","    optimizer = AdamW(model.parameters(), lr=cfg.lr, weight_decay=cfg.weight_decay, betas=(0.9, 0.999), eps=1e-8)\n","    scaler = torch.cuda.amp.GradScaler(enabled=torch.cuda.is_available())\n","\n","    total_steps = epochs * math.ceil(50000 / cfg.batch_size)\n","    warmup_steps = warmup_epochs * math.ceil(50000 / cfg.batch_size)\n","    scheduler = WarmupCosineLR(optimizer, total_steps=total_steps, warmup_steps=warmup_steps, min_lr=min_lr)\n","\n","    best_acc = 0.0\n","    for epoch in range(epochs):\n","        start_train = time.time()\n","        train_loss, train_acc = train_one_epoch(model, train_loader, optimizer, scaler, device)\n","        end_train = time.time()\n","        train_time = end_train - start_train\n","\n","        start_eval = time.time()\n","        test_acc = evaluate(model, test_loader, device)\n","        end_eval = time.time()\n","        eval_time = end_eval - start_eval\n","\n","        best_acc = max(best_acc, test_acc)\n","\n","        # step LR scheduler per iteration equivalently by calling .step() repeated times.\n","        # Here we approximate by stepping once per epoch across epoch-length steps:\n","        # do it properly: step per batch in train loop would be ideal.\n","        # Quick fix: recompute steps done and set last_epoch accordingly.\n","        scheduler.last_epoch = (epoch + 1) * math.ceil(50000 / cfg.batch_size) - 1\n","        scheduler.step()\n","\n","        print(f\"epoch {epoch+1:03d}/{epochs} | loss {train_loss:.4f} | train_acc {train_acc*100:5.2f}% | test_acc {test_acc*100:5.2f}% | dl_time {dataloader_time:.2f}s | train_time {train_time:.2f}s | eval_time {eval_time:.2f}s\")\n","\n","    return {\n","        \"config\": cfg,\n","        \"best_acc\": best_acc\n","    }\n","\n","# ---------------------------\n","# Grid Search\n","# ---------------------------\n","\n","def main():\n","    parser = argparse.ArgumentParser()\n","    parser.add_argument(\"--epochs\", type=int, default=150)\n","    parser.add_argument(\"--warmup_epochs\", type=int, default=5)\n","    parser.add_argument(\"--min_lr\", type=float, default=1e-5)\n","    parser.add_argument(\"--num_workers\", type=int, default=4)\n","    parser.add_argument(\"--seed\", type=int, default=42)\n","\n","    # Default grids. Adjust as needed.\n","    # parser.add_argument(\"--lrs\", type=float, nargs=\"+\", default=[1e-4, 3.3e-4, 1e-3])\n","    # parser.add_argument(\"--wds\", type=float, nargs=\"+\",default=[0.02, 0.07, 0.15])\n","    # parser.add_argument(\"--bss\", type=int, nargs=\"+\", default=[128, 256, 512])\n","    # parser.add_argument(\"--dprs\", type=float, nargs=\"+\", default=[0.00, 0.10, 0.20])\n","\n","    parser.add_argument(\"--lrs\", type=float, nargs=\"+\", default=[1e-3])\n","    parser.add_argument(\"--wds\", type=float, nargs=\"+\",default=[0.02])\n","    parser.add_argument(\"--bss\", type=int, nargs=\"+\", default=[512])\n","    parser.add_argument(\"--dprs\", type=float, nargs=\"+\", default=[0.20])\n","\n","    args = parser.parse_args([])\n","    set_seed(args.seed)\n","\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    print(\"device:\", device)\n","\n","    search_space = list(itertools.product(args.lrs, args.wds, args.bss, args.dprs))\n","    print(f\"total runs: {len(search_space)}\")\n","    results: List[Dict[str, Any]] = []\n","\n","    start_all = time.time()\n","    for i, (lr, wd, bs, dpr) in enumerate(search_space, 1):\n","        print(\"\\n\" + \"=\" * 64)\n","        print(f\"run {i}/{len(search_space)} | lr={lr} wd={wd} bs={bs} drop_path_rate={dpr}\")\n","        print(\"=\" * 64)\n","        cfg = RunConfig(lr=lr, weight_decay=wd, batch_size=bs, drop_path_rate=dpr)\n","        res = run_training(cfg, epochs=args.epochs, warmup_epochs=args.warmup_epochs,\n","                           min_lr=args.min_lr, device=device, num_workers=args.num_workers)\n","        results.append(res)\n","\n","    elapsed = time.time() - start_all\n","    print(f\"\\nGrid search finished in {elapsed/60:.1f} min\\n\")\n","\n","    # Leaderboard\n","    results = sorted(results, key=lambda r: r[\"best_acc\"], reverse=True)\n","    print(\"Leaderboard (best test accuracy):\")\n","    for rank, r in enumerate(results, 1):\n","        cfg = r[\"config\"]\n","        print(f\"{rank:2d}) acc={r['best_acc']*100:5.2f}% | lr={cfg.lr} wd={cfg.weight_decay} bs={cfg.batch_size} dpr={cfg.drop_path_rate}\")\n","\n","if __name__ == \"__main__\":\n","    main()"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[],"authorship_tag":"ABX9TyMBC0FQYCRZBWFcH3jqDUwW"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}