{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25fe8f5e",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-12-04T09:24:46.446629Z",
     "iopub.status.busy": "2025-12-04T09:24:46.446417Z",
     "iopub.status.idle": "2025-12-04T09:24:47.772002Z",
     "shell.execute_reply": "2025-12-04T09:24:47.771004Z"
    },
    "papermill": {
     "duration": 1.330925,
     "end_time": "2025-12-04T09:24:47.773663",
     "exception": false,
     "start_time": "2025-12-04T09:24:46.442738",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/cifar-10-dataset/cifar-10-batches-py/data_batch_1\n",
      "/kaggle/input/cifar-10-dataset/cifar-10-batches-py/data_batch_2\n",
      "/kaggle/input/cifar-10-dataset/cifar-10-batches-py/batches.meta\n",
      "/kaggle/input/cifar-10-dataset/cifar-10-batches-py/test_batch\n",
      "/kaggle/input/cifar-10-dataset/cifar-10-batches-py/data_batch_3\n",
      "/kaggle/input/cifar-10-dataset/cifar-10-batches-py/data_batch_5\n",
      "/kaggle/input/cifar-10-dataset/cifar-10-batches-py/data_batch_4\n",
      "/kaggle/input/cifar-10-dataset/cifar-10-batches-py/readme.html\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "daf23ef1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-04T09:24:47.780095Z",
     "iopub.status.busy": "2025-12-04T09:24:47.779256Z",
     "iopub.status.idle": "2025-12-04T09:24:55.822303Z",
     "shell.execute_reply": "2025-12-04T09:24:55.821427Z"
    },
    "papermill": {
     "duration": 8.047252,
     "end_time": "2025-12-04T09:24:55.823505",
     "exception": false,
     "start_time": "2025-12-04T09:24:47.776253",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3339654b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-04T09:24:55.829170Z",
     "iopub.status.busy": "2025-12-04T09:24:55.828812Z",
     "iopub.status.idle": "2025-12-04T09:24:58.968274Z",
     "shell.execute_reply": "2025-12-04T09:24:58.967701Z"
    },
    "papermill": {
     "duration": 3.143738,
     "end_time": "2025-12-04T09:24:58.969594",
     "exception": false,
     "start_time": "2025-12-04T09:24:55.825856",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "DATA_ROOT = \"/kaggle/input/cifar-10-dataset\"\n",
    "\n",
    "transform_train = T.Compose([\n",
    "    T.RandomCrop(32, padding=4),\n",
    "    T.RandomHorizontalFlip(),\n",
    "    T.ToTensor(),\n",
    "])\n",
    "\n",
    "transform_val = T.Compose([\n",
    "    T.ToTensor(),\n",
    "])\n",
    "\n",
    "train_dataset = torchvision.datasets.CIFAR10(\n",
    "    root=DATA_ROOT,\n",
    "    train=True,\n",
    "    download=False,   # IMPORTANT: Kaggle already has the data\n",
    "    transform=transform_train,\n",
    ")\n",
    "\n",
    "val_dataset = torchvision.datasets.CIFAR10(\n",
    "    root=DATA_ROOT,\n",
    "    train=False,\n",
    "    download=False,\n",
    "    transform=transform_val,\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=128,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=256,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    pin_memory=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e03d900",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-04T09:24:58.975311Z",
     "iopub.status.busy": "2025-12-04T09:24:58.975096Z",
     "iopub.status.idle": "2025-12-04T09:24:58.980524Z",
     "shell.execute_reply": "2025-12-04T09:24:58.979792Z"
    },
    "papermill": {
     "duration": 0.009496,
     "end_time": "2025-12-04T09:24:58.981636",
     "exception": false,
     "start_time": "2025-12-04T09:24:58.972140",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, dim, num_heads, mlp_ratio=4.0, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(dim)\n",
    "        self.attn = nn.MultiheadAttention(\n",
    "            embed_dim=dim,\n",
    "            num_heads=num_heads,\n",
    "            dropout=dropout,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.norm2 = nn.LayerNorm(dim)\n",
    "\n",
    "        hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, dim),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        attn_out, _ = self.attn(self.norm1(x), self.norm1(x), self.norm1(x), need_weights=False)\n",
    "        x = x + attn_out\n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e43324bf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-04T09:24:58.987245Z",
     "iopub.status.busy": "2025-12-04T09:24:58.986666Z",
     "iopub.status.idle": "2025-12-04T09:24:58.990470Z",
     "shell.execute_reply": "2025-12-04T09:24:58.989775Z"
    },
    "papermill": {
     "duration": 0.007582,
     "end_time": "2025-12-04T09:24:58.991465",
     "exception": false,
     "start_time": "2025-12-04T09:24:58.983883",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000 10000\n"
     ]
    }
   ],
   "source": [
    "#sanity check, should output 50000 10000\n",
    "print(len(train_dataset), len(val_dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "81bd73ba",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-04T09:24:58.996734Z",
     "iopub.status.busy": "2025-12-04T09:24:58.996353Z",
     "iopub.status.idle": "2025-12-04T09:24:59.003531Z",
     "shell.execute_reply": "2025-12-04T09:24:59.002811Z"
    },
    "papermill": {
     "duration": 0.010883,
     "end_time": "2025-12-04T09:24:59.004535",
     "exception": false,
     "start_time": "2025-12-04T09:24:58.993652",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ViTSmall(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        image_size=32,\n",
    "        patch_size=4,\n",
    "        in_channels=3,\n",
    "        num_classes=10,\n",
    "        embed_dim=384,\n",
    "        depth=12,\n",
    "        num_heads=6,\n",
    "        mlp_ratio=4.0,\n",
    "        dropout=0.1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        num_patches = (image_size // patch_size) ** 2\n",
    "\n",
    "        self.patch_embed = nn.Conv2d(\n",
    "            in_channels, embed_dim,\n",
    "            kernel_size=patch_size,\n",
    "            stride=patch_size\n",
    "        )\n",
    "\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        self.pos_embed = nn.Parameter(\n",
    "            torch.zeros(1, num_patches + 1, embed_dim)\n",
    "        )\n",
    "        self.pos_drop = nn.Dropout(dropout)\n",
    "\n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerBlock(\n",
    "                embed_dim, num_heads, mlp_ratio, dropout\n",
    "            )\n",
    "            for _ in range(depth)\n",
    "        ])\n",
    "\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        self.head = nn.Linear(embed_dim, num_classes)\n",
    "\n",
    "        nn.init.trunc_normal_(self.pos_embed, std=0.02)\n",
    "        nn.init.trunc_normal_(self.cls_token, std=0.02)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B = x.size(0)\n",
    "        x = self.patch_embed(x)\n",
    "        x = x.flatten(2).transpose(1, 2)\n",
    "\n",
    "        cls = self.cls_token.expand(B, -1, -1)\n",
    "        x = torch.cat((cls, x), dim=1)\n",
    "        x = self.pos_drop(x + self.pos_embed)\n",
    "\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x)\n",
    "\n",
    "        x = self.norm(x)\n",
    "        return self.head(x[:, 0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "46a25f5e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-04T09:24:59.009887Z",
     "iopub.status.busy": "2025-12-04T09:24:59.009396Z",
     "iopub.status.idle": "2025-12-04T09:24:59.014941Z",
     "shell.execute_reply": "2025-12-04T09:24:59.014194Z"
    },
    "papermill": {
     "duration": 0.009383,
     "end_time": "2025-12-04T09:24:59.016084",
     "exception": false,
     "start_time": "2025-12-04T09:24:59.006701",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    total_loss, total = 0.0, 0\n",
    "    for x, y in loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        logits = model(x)\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "        total_loss += loss.item() * x.size(0)\n",
    "        total += x.size(0)\n",
    "    return total_loss / total\n",
    "\n",
    "\n",
    "def train_n_batches(model, optimizer, loader, n_batches):\n",
    "    model.train()\n",
    "    it = iter(loader)\n",
    "    for _ in range(n_batches):\n",
    "        try:\n",
    "            x, y = next(it)\n",
    "        except StopIteration:\n",
    "            it = iter(loader)\n",
    "            x, y = next(it)\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        loss = F.cross_entropy(model(x), y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "78f9f197",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-04T09:24:59.021182Z",
     "iopub.status.busy": "2025-12-04T09:24:59.020934Z",
     "iopub.status.idle": "2025-12-04T09:24:59.032222Z",
     "shell.execute_reply": "2025-12-04T09:24:59.031675Z"
    },
    "papermill": {
     "duration": 0.01508,
     "end_time": "2025-12-04T09:24:59.033241",
     "exception": false,
     "start_time": "2025-12-04T09:24:59.018161",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class HyperGradient:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model,\n",
    "        train_loader,\n",
    "        val_loader,\n",
    "        lr=3.3e-4,\n",
    "        wd=0.07,\n",
    "        delta=1e-4,\n",
    "        hyper_lr=1e-6,  # CHANGED: Reduced default from 1e-2 to 1e-6\n",
    "        probe_batches=5\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "\n",
    "        self.lr = lr\n",
    "        self.wd = wd\n",
    "        self.delta = delta\n",
    "        self.hyper_lr = hyper_lr\n",
    "        self.probe_batches = probe_batches\n",
    "\n",
    "    def _make_opt(self, model, lr, wd):\n",
    "        return torch.optim.AdamW(\n",
    "            model.parameters(), lr=lr, weight_decay=wd\n",
    "        )\n",
    "\n",
    "    # NEW: Helper to evaluate faster\n",
    "    @torch.no_grad()\n",
    "    def evaluate_subset(self, model, loader, limit_batches=20):\n",
    "        model.eval()\n",
    "        total_loss, total = 0.0, 0\n",
    "        for i, (x, y) in enumerate(loader):\n",
    "            if i >= limit_batches: break\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            loss = F.cross_entropy(model(x), y)\n",
    "            total_loss += loss.item() * x.size(0)\n",
    "            total += x.size(0)\n",
    "        return total_loss / total\n",
    "\n",
    "    def step(self):\n",
    "        grads = {}\n",
    "\n",
    "        for name in [\"lr\", \"wd\"]:\n",
    "            base = getattr(self, name)\n",
    "            # Ensure epsilon isn't too tiny\n",
    "            eps = self.delta * max(base, 1e-8)\n",
    "\n",
    "            losses = {}\n",
    "            for sign in [+1, -1]:\n",
    "                val = max(base + sign * eps, 1e-8)\n",
    "                clone = copy.deepcopy(self.model)\n",
    "                opt = self._make_opt(\n",
    "                    clone,\n",
    "                    lr=val if name == \"lr\" else self.lr,\n",
    "                    wd=val if name == \"wd\" else self.wd,\n",
    "                )\n",
    "\n",
    "                train_n_batches(\n",
    "                    clone, opt, self.train_loader, self.probe_batches\n",
    "                )\n",
    "                # CHANGED: Use fast subset evaluation\n",
    "                losses[sign] = self.evaluate_subset(clone, self.val_loader, limit_batches=20)\n",
    "\n",
    "            grads[name] = (losses[+1] - losses[-1]) / (2 * eps)\n",
    "\n",
    "        # CHANGED: Robust Update Logic with Clipping\n",
    "        # 1. Update LR with safety clip\n",
    "        lr_grad = grads[\"lr\"]\n",
    "        lr_update = self.hyper_lr * lr_grad\n",
    "        # Cap change to max 20% of current value to prevent explosion\n",
    "        max_lr_change = 0.20 * self.lr\n",
    "        if abs(lr_update) > max_lr_change:\n",
    "            lr_update = max_lr_change if lr_update > 0 else -max_lr_change\n",
    "        \n",
    "        self.lr -= lr_update\n",
    "\n",
    "        # 2. Update WD with safety clip\n",
    "        wd_grad = grads[\"wd\"]\n",
    "        wd_update = self.hyper_lr * wd_grad\n",
    "        max_wd_change = 0.20 * self.wd\n",
    "        if abs(wd_update) > max_wd_change:\n",
    "            wd_update = max_wd_change if wd_update > 0 else -max_wd_change\n",
    "            \n",
    "        self.wd -= wd_update\n",
    "\n",
    "        # 3. Final Clamps\n",
    "        self.lr = float(torch.clamp(torch.tensor(self.lr), 1e-6, 1e-2))\n",
    "        self.wd = float(torch.clamp(torch.tensor(self.wd), 1e-6, 1.0))\n",
    "\n",
    "        return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e6218c72",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-04T09:24:59.038471Z",
     "iopub.status.busy": "2025-12-04T09:24:59.038296Z",
     "iopub.status.idle": "2025-12-04T12:13:37.400642Z",
     "shell.execute_reply": "2025-12-04T12:13:37.399708Z"
    },
    "papermill": {
     "duration": 10118.385233,
     "end_time": "2025-12-04T12:13:37.420609",
     "exception": false,
     "start_time": "2025-12-04T09:24:59.035376",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Multi-Seed Experiment.\n",
      "Seeds: [38042, 217401, 637451, 207796, 45921]\n",
      "Config: 90 Epochs per seed | ViT-Small (Patch=8, Dim=192)\n",
      "Estimated duration: ~3.5 to 4 hours\n",
      "\n",
      "================================================================================\n",
      "STARTING SEED: 38042\n",
      "================================================================================\n",
      "[Seed 38042 | Ep 00] Train (L=1.9746 A=24.4%) | Val (L=1.8507 A=30.4%) | Hyper (LR=2.64e-04 WD=6.99e-02) | Time=18.2s [HG UPDATE: dLR=7.9e+05, dWD=1.0e+04]\n",
      "[Seed 38042 | Ep 01] Train (L=1.7575 A=33.7%) | Val (L=1.6493 A=39.6%) | Hyper (LR=2.64e-04 WD=6.99e-02) | Time=17.7s\n",
      "[Seed 38042 | Ep 02] Train (L=1.6716 A=37.9%) | Val (L=1.6174 A=41.1%) | Hyper (LR=2.11e-04 WD=6.99e-02) | Time=18.2s [HG UPDATE: dLR=2.3e+05, dWD=-3.4e+02]\n",
      "[Seed 38042 | Ep 03] Train (L=1.6090 A=40.5%) | Val (L=1.5695 A=42.9%) | Hyper (LR=2.11e-04 WD=6.99e-02) | Time=18.4s\n",
      "[Seed 38042 | Ep 04] Train (L=1.5671 A=42.1%) | Val (L=1.5531 A=43.9%) | Hyper (LR=1.69e-04 WD=6.99e-02) | Time=18.9s [HG UPDATE: dLR=7.9e+03, dWD=4.3e+03]\n",
      "[Seed 38042 | Ep 05] Train (L=1.5194 A=44.2%) | Val (L=1.4390 A=48.4%) | Hyper (LR=1.69e-04 WD=6.99e-02) | Time=18.6s\n",
      "[Seed 38042 | Ep 06] Train (L=1.4904 A=45.2%) | Val (L=1.4109 A=48.2%) | Hyper (LR=1.35e-04 WD=6.99e-02) | Time=18.4s [HG UPDATE: dLR=5.4e+03, dWD=-6.2e+02]\n",
      "[Seed 38042 | Ep 07] Train (L=1.4607 A=46.6%) | Val (L=1.4395 A=47.2%) | Hyper (LR=1.35e-04 WD=6.99e-02) | Time=18.5s\n",
      "[Seed 38042 | Ep 08] Train (L=1.4397 A=47.4%) | Val (L=1.4052 A=48.8%) | Hyper (LR=1.62e-04 WD=6.99e-02) | Time=18.8s [HG UPDATE: dLR=-6.5e+04, dWD=-3.9e+03]\n",
      "[Seed 38042 | Ep 09] Train (L=1.4299 A=48.0%) | Val (L=1.3873 A=50.1%) | Hyper (LR=1.62e-04 WD=6.99e-02) | Time=18.6s\n",
      "[Seed 38042 | Ep 10] Train (L=1.4172 A=48.3%) | Val (L=1.3922 A=49.8%) | Hyper (LR=1.30e-04 WD=6.99e-02) | Time=18.6s [HG UPDATE: dLR=4.9e+05, dWD=-4.7e+02]\n",
      "[Seed 38042 | Ep 11] Train (L=1.3860 A=49.6%) | Val (L=1.3575 A=51.0%) | Hyper (LR=1.30e-04 WD=6.99e-02) | Time=18.9s\n",
      "[Seed 38042 | Ep 12] Train (L=1.3761 A=49.9%) | Val (L=1.3791 A=50.3%) | Hyper (LR=1.56e-04 WD=6.99e-02) | Time=19.2s [HG UPDATE: dLR=-7.9e+05, dWD=-1.7e+03]\n",
      "[Seed 38042 | Ep 13] Train (L=1.3687 A=49.9%) | Val (L=1.3515 A=51.2%) | Hyper (LR=1.56e-04 WD=6.99e-02) | Time=18.6s\n",
      "[Seed 38042 | Ep 14] Train (L=1.3592 A=50.5%) | Val (L=1.3921 A=50.1%) | Hyper (LR=1.25e-04 WD=6.99e-02) | Time=18.5s [HG UPDATE: dLR=8.8e+05, dWD=-6.0e+02]\n",
      "[Seed 38042 | Ep 15] Train (L=1.3332 A=51.6%) | Val (L=1.3112 A=52.5%) | Hyper (LR=1.25e-04 WD=6.99e-02) | Time=18.5s\n",
      "[Seed 38042 | Ep 16] Train (L=1.3195 A=51.9%) | Val (L=1.3196 A=52.2%) | Hyper (LR=1.49e-04 WD=6.99e-02) | Time=18.8s [HG UPDATE: dLR=-3.9e+05, dWD=2.6e+02]\n",
      "[Seed 38042 | Ep 17] Train (L=1.3240 A=51.8%) | Val (L=1.2984 A=53.9%) | Hyper (LR=1.49e-04 WD=6.99e-02) | Time=18.6s\n",
      "[Seed 38042 | Ep 18] Train (L=1.3105 A=52.6%) | Val (L=1.2989 A=53.3%) | Hyper (LR=1.20e-04 WD=6.99e-02) | Time=18.6s [HG UPDATE: dLR=4.9e+05, dWD=-1.0e+03]\n",
      "[Seed 38042 | Ep 19] Train (L=1.2813 A=53.3%) | Val (L=1.2992 A=53.3%) | Hyper (LR=1.20e-04 WD=6.99e-02) | Time=18.7s\n",
      "[Seed 38042 | Ep 20] Train (L=1.2698 A=53.9%) | Val (L=1.2901 A=53.8%) | Hyper (LR=9.57e-05 WD=6.99e-02) | Time=18.6s [HG UPDATE: dLR=1.1e+05, dWD=2.2e+03]\n",
      "[Seed 38042 | Ep 21] Train (L=1.2509 A=54.5%) | Val (L=1.2628 A=54.2%) | Hyper (LR=9.57e-05 WD=6.99e-02) | Time=18.5s\n",
      "[Seed 38042 | Ep 22] Train (L=1.2383 A=55.4%) | Val (L=1.2440 A=55.3%) | Hyper (LR=1.15e-04 WD=6.99e-02) | Time=18.6s [HG UPDATE: dLR=-1.2e+06, dWD=4.3e+03]\n",
      "[Seed 38042 | Ep 23] Train (L=1.2372 A=55.1%) | Val (L=1.2247 A=56.0%) | Hyper (LR=1.15e-04 WD=6.99e-02) | Time=18.5s\n",
      "[Seed 38042 | Ep 24] Train (L=1.2318 A=55.2%) | Val (L=1.2425 A=55.4%) | Hyper (LR=9.18e-05 WD=6.99e-02) | Time=18.7s [HG UPDATE: dLR=1.0e+05, dWD=5.1e+02]\n",
      "[Seed 38042 | Ep 25] Train (L=1.2139 A=56.3%) | Val (L=1.2195 A=56.4%) | Hyper (LR=9.18e-05 WD=6.99e-02) | Time=18.7s\n",
      "[Seed 38042 | Ep 26] Train (L=1.1986 A=56.6%) | Val (L=1.2241 A=56.4%) | Hyper (LR=1.10e-04 WD=6.99e-02) | Time=18.5s [HG UPDATE: dLR=-3.1e+06, dWD=-1.9e+03]\n",
      "[Seed 38042 | Ep 27] Train (L=1.2051 A=56.6%) | Val (L=1.2079 A=57.1%) | Hyper (LR=1.10e-04 WD=6.99e-02) | Time=18.6s\n",
      "[Seed 38042 | Ep 28] Train (L=1.1950 A=56.9%) | Val (L=1.2026 A=57.2%) | Hyper (LR=8.82e-05 WD=6.99e-02) | Time=18.6s [HG UPDATE: dLR=9.1e+04, dWD=4.5e+02]\n",
      "[Seed 38042 | Ep 29] Train (L=1.1699 A=57.6%) | Val (L=1.1934 A=57.6%) | Hyper (LR=8.82e-05 WD=6.99e-02) | Time=18.6s\n",
      "[Seed 38042 | Ep 30] Train (L=1.1660 A=57.8%) | Val (L=1.2242 A=56.1%) | Hyper (LR=1.06e-04 WD=6.99e-02) | Time=18.5s [HG UPDATE: dLR=-1.4e+06, dWD=1.7e+02]\n",
      "[Seed 38042 | Ep 31] Train (L=1.1685 A=57.6%) | Val (L=1.1766 A=58.0%) | Hyper (LR=1.06e-04 WD=6.99e-02) | Time=18.6s\n",
      "[Seed 38042 | Ep 32] Train (L=1.1610 A=58.1%) | Val (L=1.1956 A=57.1%) | Hyper (LR=1.27e-04 WD=6.99e-02) | Time=18.4s [HG UPDATE: dLR=-8.0e+05, dWD=-2.6e+03]\n",
      "[Seed 38042 | Ep 33] Train (L=1.1672 A=57.9%) | Val (L=1.1885 A=58.0%) | Hyper (LR=1.27e-04 WD=6.99e-02) | Time=18.6s\n",
      "[Seed 38042 | Ep 34] Train (L=1.1566 A=58.3%) | Val (L=1.1808 A=58.3%) | Hyper (LR=1.02e-04 WD=6.99e-02) | Time=18.7s [HG UPDATE: dLR=8.2e+05, dWD=4.1e+01]\n",
      "[Seed 38042 | Ep 35] Train (L=1.1321 A=59.1%) | Val (L=1.1839 A=58.3%) | Hyper (LR=1.02e-04 WD=6.99e-02) | Time=18.5s\n",
      "[Seed 38042 | Ep 36] Train (L=1.1179 A=59.5%) | Val (L=1.1477 A=59.3%) | Hyper (LR=1.22e-04 WD=6.99e-02) | Time=18.4s [HG UPDATE: dLR=-5.0e+05, dWD=1.2e+03]\n",
      "[Seed 38042 | Ep 37] Train (L=1.1250 A=59.3%) | Val (L=1.1471 A=59.4%) | Hyper (LR=1.22e-04 WD=6.99e-02) | Time=18.5s\n",
      "[Seed 38042 | Ep 38] Train (L=1.1227 A=59.8%) | Val (L=1.1548 A=59.6%) | Hyper (LR=1.46e-04 WD=6.99e-02) | Time=18.7s [HG UPDATE: dLR=-5.9e+05, dWD=8.9e+02]\n",
      "[Seed 38042 | Ep 39] Train (L=1.1258 A=59.3%) | Val (L=1.1503 A=59.7%) | Hyper (LR=1.46e-04 WD=6.99e-02) | Time=18.4s\n",
      "[Seed 38042 | Ep 40] Train (L=1.1205 A=59.8%) | Val (L=1.1791 A=57.7%) | Hyper (LR=1.17e-04 WD=6.99e-02) | Time=18.6s [HG UPDATE: dLR=1.7e+06, dWD=-3.9e+02]\n",
      "[Seed 38042 | Ep 41] Train (L=1.0867 A=61.2%) | Val (L=1.1447 A=59.8%) | Hyper (LR=1.17e-04 WD=6.99e-02) | Time=18.5s\n",
      "[Seed 38042 | Ep 42] Train (L=1.0773 A=61.2%) | Val (L=1.1218 A=60.0%) | Hyper (LR=1.40e-04 WD=6.99e-02) | Time=18.4s [HG UPDATE: dLR=-7.2e+05, dWD=4.5e+02]\n",
      "[Seed 38042 | Ep 43] Train (L=1.0866 A=61.0%) | Val (L=1.1542 A=58.9%) | Hyper (LR=1.40e-04 WD=6.99e-02) | Time=18.4s\n",
      "[Seed 38042 | Ep 44] Train (L=1.0781 A=61.3%) | Val (L=1.1424 A=60.0%) | Hyper (LR=1.68e-04 WD=6.98e-02) | Time=18.5s [HG UPDATE: dLR=-1.2e+05, dWD=6.7e+03]\n",
      "[Seed 38042 | Ep 45] Train (L=1.0857 A=60.9%) | Val (L=1.1304 A=60.1%) | Hyper (LR=1.68e-04 WD=6.98e-02) | Time=18.6s\n",
      "[Seed 38042 | Ep 46] Train (L=1.0716 A=61.4%) | Val (L=1.1694 A=59.5%) | Hyper (LR=2.02e-04 WD=6.98e-02) | Time=18.5s [HG UPDATE: dLR=-4.4e+05, dWD=9.8e+02]\n",
      "[Seed 38042 | Ep 47] Train (L=1.0876 A=61.2%) | Val (L=1.1040 A=61.4%) | Hyper (LR=2.02e-04 WD=6.98e-02) | Time=18.4s\n",
      "[Seed 38042 | Ep 48] Train (L=1.0766 A=61.3%) | Val (L=1.0937 A=61.0%) | Hyper (LR=2.43e-04 WD=6.98e-02) | Time=18.5s [HG UPDATE: dLR=-2.2e+06, dWD=-2.4e+03]\n",
      "[Seed 38042 | Ep 49] Train (L=1.0795 A=61.1%) | Val (L=1.1758 A=59.3%) | Hyper (LR=2.43e-04 WD=6.98e-02) | Time=18.4s\n",
      "[Seed 38042 | Ep 50] Train (L=1.0714 A=61.6%) | Val (L=1.1054 A=61.5%) | Hyper (LR=2.91e-04 WD=6.98e-02) | Time=18.4s [HG UPDATE: dLR=-2.9e+05, dWD=-6.4e+02]\n",
      "[Seed 38042 | Ep 51] Train (L=1.0853 A=61.0%) | Val (L=1.0965 A=61.1%) | Hyper (LR=2.91e-04 WD=6.98e-02) | Time=18.5s\n",
      "[Seed 38042 | Ep 52] Train (L=1.0719 A=61.4%) | Val (L=1.0775 A=61.9%) | Hyper (LR=2.33e-04 WD=6.98e-02) | Time=18.6s [HG UPDATE: dLR=2.7e+05, dWD=-2.6e+02]\n",
      "[Seed 38042 | Ep 53] Train (L=1.0272 A=63.1%) | Val (L=1.0762 A=61.7%) | Hyper (LR=2.33e-04 WD=6.98e-02) | Time=18.5s\n",
      "[Seed 38042 | Ep 54] Train (L=1.0120 A=63.6%) | Val (L=1.0602 A=62.4%) | Hyper (LR=1.86e-04 WD=6.98e-02) | Time=18.5s [HG UPDATE: dLR=1.1e+06, dWD=1.3e+03]\n",
      "[Seed 38042 | Ep 55] Train (L=0.9685 A=65.4%) | Val (L=1.0198 A=64.3%) | Hyper (LR=1.86e-04 WD=6.98e-02) | Time=18.6s\n",
      "[Seed 38042 | Ep 56] Train (L=0.9518 A=65.8%) | Val (L=1.0153 A=64.0%) | Hyper (LR=1.49e-04 WD=6.98e-02) | Time=18.7s [HG UPDATE: dLR=3.9e+05, dWD=1.0e+03]\n",
      "[Seed 38042 | Ep 57] Train (L=0.9144 A=67.4%) | Val (L=0.9882 A=65.7%) | Hyper (LR=1.49e-04 WD=6.98e-02) | Time=18.5s\n",
      "[Seed 38042 | Ep 58] Train (L=0.8955 A=68.0%) | Val (L=0.9852 A=65.8%) | Hyper (LR=1.19e-04 WD=6.98e-02) | Time=18.3s [HG UPDATE: dLR=5.4e+05, dWD=-1.6e+03]\n",
      "[Seed 38042 | Ep 59] Train (L=0.8671 A=68.8%) | Val (L=0.9597 A=67.2%) | Hyper (LR=1.19e-04 WD=6.98e-02) | Time=18.5s\n",
      "[Seed 38042 | Ep 60] Train (L=0.8502 A=69.7%) | Val (L=0.9619 A=67.0%) | Hyper (LR=9.54e-05 WD=6.98e-02) | Time=18.5s [HG UPDATE: dLR=1.2e+05, dWD=1.6e+01]\n",
      "[Seed 38042 | Ep 61] Train (L=0.8205 A=70.6%) | Val (L=0.9491 A=67.4%) | Hyper (LR=9.54e-05 WD=6.98e-02) | Time=18.4s\n",
      "[Seed 38042 | Ep 62] Train (L=0.8077 A=71.0%) | Val (L=0.9460 A=68.1%) | Hyper (LR=7.63e-05 WD=6.98e-02) | Time=18.6s [HG UPDATE: dLR=5.6e+05, dWD=8.7e+02]\n",
      "[Seed 38042 | Ep 63] Train (L=0.7829 A=71.9%) | Val (L=0.9302 A=68.6%) | Hyper (LR=7.63e-05 WD=6.98e-02) | Time=18.5s\n",
      "[Seed 38042 | Ep 64] Train (L=0.7747 A=72.0%) | Val (L=0.9172 A=69.0%) | Hyper (LR=6.11e-05 WD=6.98e-02) | Time=18.3s [HG UPDATE: dLR=1.4e+04, dWD=1.4e+03]\n",
      "[Seed 38042 | Ep 65] Train (L=0.7509 A=73.2%) | Val (L=0.9063 A=69.0%) | Hyper (LR=6.11e-05 WD=6.98e-02) | Time=18.4s\n",
      "[Seed 38042 | Ep 66] Train (L=0.7430 A=73.7%) | Val (L=0.9270 A=69.1%) | Hyper (LR=7.33e-05 WD=6.98e-02) | Time=18.5s [HG UPDATE: dLR=-1.8e+06, dWD=-7.4e+02]\n",
      "[Seed 38042 | Ep 67] Train (L=0.7429 A=73.4%) | Val (L=0.9146 A=69.3%) | Hyper (LR=7.33e-05 WD=6.98e-02) | Time=18.4s\n",
      "[Seed 38042 | Ep 68] Train (L=0.7350 A=73.7%) | Val (L=0.9052 A=69.4%) | Hyper (LR=5.86e-05 WD=6.98e-02) | Time=18.5s [HG UPDATE: dLR=6.1e+05, dWD=2.2e+02]\n",
      "[Seed 38042 | Ep 69] Train (L=0.7183 A=74.3%) | Val (L=0.9069 A=69.1%) | Hyper (LR=5.86e-05 WD=6.98e-02) | Time=18.3s\n",
      "[Seed 38042 | Ep 70] Train (L=0.7062 A=74.7%) | Val (L=0.9028 A=69.5%) | Hyper (LR=4.69e-05 WD=6.98e-02) | Time=18.5s [HG UPDATE: dLR=2.1e+06, dWD=4.7e+02]\n",
      "[Seed 38042 | Ep 71] Train (L=0.6831 A=75.6%) | Val (L=0.8995 A=70.0%) | Hyper (LR=4.69e-05 WD=6.98e-02) | Time=18.6s\n",
      "[Seed 38042 | Ep 72] Train (L=0.6793 A=75.6%) | Val (L=0.9103 A=69.5%) | Hyper (LR=3.75e-05 WD=6.98e-02) | Time=18.7s [HG UPDATE: dLR=6.4e+05, dWD=8.7e+02]\n",
      "[Seed 38042 | Ep 73] Train (L=0.6669 A=76.2%) | Val (L=0.9043 A=70.2%) | Hyper (LR=3.75e-05 WD=6.98e-02) | Time=18.4s\n",
      "[Seed 38042 | Ep 74] Train (L=0.6543 A=76.6%) | Val (L=0.8970 A=70.2%) | Hyper (LR=3.00e-05 WD=6.98e-02) | Time=18.6s [HG UPDATE: dLR=3.5e+05, dWD=3.7e+02]\n",
      "[Seed 38042 | Ep 75] Train (L=0.6479 A=76.6%) | Val (L=0.9035 A=70.1%) | Hyper (LR=3.00e-05 WD=6.98e-02) | Time=18.7s\n",
      "[Seed 38042 | Ep 76] Train (L=0.6437 A=76.9%) | Val (L=0.9051 A=69.8%) | Hyper (LR=2.40e-05 WD=6.98e-02) | Time=18.8s [HG UPDATE: dLR=2.8e+05, dWD=7.8e+01]\n",
      "[Seed 38042 | Ep 77] Train (L=0.6325 A=77.4%) | Val (L=0.8978 A=70.4%) | Hyper (LR=2.40e-05 WD=6.98e-02) | Time=18.5s\n",
      "[Seed 38042 | Ep 78] Train (L=0.6256 A=77.5%) | Val (L=0.9051 A=70.4%) | Hyper (LR=1.92e-05 WD=6.98e-02) | Time=18.6s [HG UPDATE: dLR=2.4e+06, dWD=5.6e+02]\n",
      "[Seed 38042 | Ep 79] Train (L=0.6154 A=78.1%) | Val (L=0.8977 A=70.4%) | Hyper (LR=1.92e-05 WD=6.98e-02) | Time=18.5s\n",
      "[Seed 38042 | Ep 80] Train (L=0.6139 A=78.0%) | Val (L=0.8965 A=70.3%) | Hyper (LR=2.30e-05 WD=6.98e-02) | Time=18.6s [HG UPDATE: dLR=-1.7e+06, dWD=8.6e+02]\n",
      "[Seed 38042 | Ep 81] Train (L=0.6170 A=77.9%) | Val (L=0.9084 A=69.9%) | Hyper (LR=2.30e-05 WD=6.98e-02) | Time=18.7s\n",
      "[Seed 38042 | Ep 82] Train (L=0.6108 A=78.2%) | Val (L=0.9008 A=70.9%) | Hyper (LR=2.77e-05 WD=6.98e-02) | Time=18.6s [HG UPDATE: dLR=-2.6e+05, dWD=-6.8e+01]\n",
      "[Seed 38042 | Ep 83] Train (L=0.6120 A=78.1%) | Val (L=0.8971 A=70.6%) | Hyper (LR=2.77e-05 WD=6.98e-02) | Time=18.4s\n",
      "[Seed 38042 | Ep 84] Train (L=0.6084 A=78.2%) | Val (L=0.8954 A=70.7%) | Hyper (LR=2.21e-05 WD=6.98e-02) | Time=18.5s [HG UPDATE: dLR=8.6e+04, dWD=-5.3e+02]\n",
      "[Seed 38042 | Ep 85] Train (L=0.6048 A=78.3%) | Val (L=0.9176 A=70.2%) | Hyper (LR=2.21e-05 WD=6.98e-02) | Time=18.4s\n",
      "[Seed 38042 | Ep 86] Train (L=0.5966 A=78.7%) | Val (L=0.9095 A=70.3%) | Hyper (LR=2.66e-05 WD=6.98e-02) | Time=18.5s [HG UPDATE: dLR=-1.9e+05, dWD=8.8e+02]\n",
      "[Seed 38042 | Ep 87] Train (L=0.5980 A=78.5%) | Val (L=0.9077 A=70.6%) | Hyper (LR=2.66e-05 WD=6.98e-02) | Time=18.4s\n",
      "[Seed 38042 | Ep 88] Train (L=0.5925 A=78.8%) | Val (L=0.9088 A=70.4%) | Hyper (LR=3.19e-05 WD=6.98e-02) | Time=18.5s [HG UPDATE: dLR=-5.6e+05, dWD=5.5e+02]\n",
      "[Seed 38042 | Ep 89] Train (L=0.5960 A=78.6%) | Val (L=0.9075 A=70.8%) | Hyper (LR=3.19e-05 WD=6.98e-02) | Time=18.5s\n",
      "--> Completed Seed 38042. Saved checkpoint to 'hypergradient_results.csv'\n",
      "\n",
      "================================================================================\n",
      "STARTING SEED: 217401\n",
      "================================================================================\n",
      "[Seed 217401 | Ep 00] Train (L=1.9984 A=24.1%) | Val (L=1.7233 A=36.6%) | Hyper (LR=3.96e-04 WD=7.00e-02) | Time=18.7s [HG UPDATE: dLR=-4.1e+05, dWD=-3.0e+03]\n",
      "[Seed 217401 | Ep 01] Train (L=1.7435 A=34.7%) | Val (L=1.7138 A=37.6%) | Hyper (LR=3.96e-04 WD=7.00e-02) | Time=18.5s\n",
      "[Seed 217401 | Ep 02] Train (L=1.6468 A=39.0%) | Val (L=1.5776 A=42.3%) | Hyper (LR=4.75e-04 WD=7.01e-02) | Time=18.6s [HG UPDATE: dLR=-5.7e+05, dWD=-3.5e+03]\n",
      "[Seed 217401 | Ep 03] Train (L=1.6109 A=40.7%) | Val (L=1.5047 A=45.0%) | Hyper (LR=4.75e-04 WD=7.01e-02) | Time=18.3s\n",
      "[Seed 217401 | Ep 04] Train (L=1.5610 A=42.6%) | Val (L=1.5133 A=44.5%) | Hyper (LR=3.80e-04 WD=7.01e-02) | Time=18.7s [HG UPDATE: dLR=8.3e+05, dWD=-7.6e+03]\n",
      "[Seed 217401 | Ep 05] Train (L=1.5089 A=44.9%) | Val (L=1.4250 A=48.3%) | Hyper (LR=3.80e-04 WD=7.01e-02) | Time=18.5s\n",
      "[Seed 217401 | Ep 06] Train (L=1.4826 A=45.9%) | Val (L=1.4755 A=45.8%) | Hyper (LR=3.04e-04 WD=7.02e-02) | Time=18.5s [HG UPDATE: dLR=1.7e+05, dWD=-6.7e+03]\n",
      "[Seed 217401 | Ep 07] Train (L=1.4431 A=47.2%) | Val (L=1.3811 A=49.4%) | Hyper (LR=3.04e-04 WD=7.02e-02) | Time=18.5s\n",
      "[Seed 217401 | Ep 08] Train (L=1.4220 A=48.2%) | Val (L=1.3871 A=49.5%) | Hyper (LR=3.65e-04 WD=7.02e-02) | Time=18.6s [HG UPDATE: dLR=-1.0e+06, dWD=2.5e+02]\n",
      "[Seed 217401 | Ep 09] Train (L=1.4169 A=48.6%) | Val (L=1.4206 A=48.4%) | Hyper (LR=3.65e-04 WD=7.02e-02) | Time=18.7s\n",
      "[Seed 217401 | Ep 10] Train (L=1.4039 A=48.9%) | Val (L=1.3639 A=50.3%) | Hyper (LR=4.38e-04 WD=7.02e-02) | Time=18.5s [HG UPDATE: dLR=-9.4e+05, dWD=1.5e+03]\n",
      "[Seed 217401 | Ep 11] Train (L=1.4009 A=48.8%) | Val (L=1.3577 A=50.7%) | Hyper (LR=4.38e-04 WD=7.02e-02) | Time=18.6s\n",
      "[Seed 217401 | Ep 12] Train (L=1.3843 A=49.7%) | Val (L=1.3411 A=51.4%) | Hyper (LR=3.50e-04 WD=7.02e-02) | Time=18.5s [HG UPDATE: dLR=9.9e+04, dWD=6.0e+02]\n",
      "[Seed 217401 | Ep 13] Train (L=1.3532 A=50.8%) | Val (L=1.2992 A=52.7%) | Hyper (LR=3.50e-04 WD=7.02e-02) | Time=18.6s\n",
      "[Seed 217401 | Ep 14] Train (L=1.3344 A=51.6%) | Val (L=1.3259 A=51.8%) | Hyper (LR=4.20e-04 WD=7.01e-02) | Time=18.5s [HG UPDATE: dLR=-1.9e+05, dWD=4.9e+03]\n",
      "[Seed 217401 | Ep 15] Train (L=1.3367 A=51.4%) | Val (L=1.3004 A=52.4%) | Hyper (LR=4.20e-04 WD=7.01e-02) | Time=18.4s\n",
      "[Seed 217401 | Ep 16] Train (L=1.3226 A=52.0%) | Val (L=1.2960 A=53.7%) | Hyper (LR=5.05e-04 WD=7.01e-02) | Time=18.5s [HG UPDATE: dLR=-4.2e+05, dWD=4.0e+03]\n",
      "[Seed 217401 | Ep 17] Train (L=1.3349 A=51.6%) | Val (L=1.3362 A=51.8%) | Hyper (LR=5.05e-04 WD=7.01e-02) | Time=18.5s\n",
      "[Seed 217401 | Ep 18] Train (L=1.3134 A=52.7%) | Val (L=1.2733 A=54.7%) | Hyper (LR=6.05e-04 WD=7.01e-02) | Time=18.7s [HG UPDATE: dLR=-4.7e+05, dWD=-3.9e+03]\n",
      "[Seed 217401 | Ep 19] Train (L=1.3235 A=52.1%) | Val (L=1.3018 A=53.6%) | Hyper (LR=6.05e-04 WD=7.01e-02) | Time=18.6s\n",
      "[Seed 217401 | Ep 20] Train (L=1.3110 A=52.7%) | Val (L=1.3088 A=52.6%) | Hyper (LR=7.26e-04 WD=7.01e-02) | Time=18.7s [HG UPDATE: dLR=-1.2e+05, dWD=1.7e+03]\n",
      "[Seed 217401 | Ep 21] Train (L=1.3199 A=52.3%) | Val (L=1.2836 A=54.0%) | Hyper (LR=7.26e-04 WD=7.01e-02) | Time=18.6s\n",
      "[Seed 217401 | Ep 22] Train (L=1.3029 A=53.0%) | Val (L=1.2574 A=54.3%) | Hyper (LR=8.72e-04 WD=7.02e-02) | Time=18.6s [HG UPDATE: dLR=-4.4e+05, dWD=-3.3e+03]\n",
      "[Seed 217401 | Ep 23] Train (L=1.3127 A=52.4%) | Val (L=1.2986 A=53.1%) | Hyper (LR=8.72e-04 WD=7.02e-02) | Time=18.6s\n",
      "[Seed 217401 | Ep 24] Train (L=1.3025 A=53.0%) | Val (L=1.2711 A=54.1%) | Hyper (LR=6.97e-04 WD=7.02e-02) | Time=18.8s [HG UPDATE: dLR=1.5e+05, dWD=-3.0e+03]\n",
      "[Seed 217401 | Ep 25] Train (L=1.2496 A=54.9%) | Val (L=1.1951 A=56.8%) | Hyper (LR=6.97e-04 WD=7.02e-02) | Time=18.6s\n",
      "[Seed 217401 | Ep 26] Train (L=1.2332 A=55.4%) | Val (L=1.1998 A=57.1%) | Hyper (LR=5.58e-04 WD=7.02e-02) | Time=18.6s [HG UPDATE: dLR=1.8e+05, dWD=-4.1e+02]\n",
      "[Seed 217401 | Ep 27] Train (L=1.1877 A=57.1%) | Val (L=1.1752 A=58.0%) | Hyper (LR=5.58e-04 WD=7.02e-02) | Time=18.6s\n",
      "[Seed 217401 | Ep 28] Train (L=1.1670 A=57.8%) | Val (L=1.1864 A=58.4%) | Hyper (LR=6.70e-04 WD=7.02e-02) | Time=18.6s [HG UPDATE: dLR=-1.8e+05, dWD=-1.3e+03]\n",
      "[Seed 217401 | Ep 29] Train (L=1.1713 A=58.1%) | Val (L=1.1596 A=58.3%) | Hyper (LR=6.70e-04 WD=7.02e-02) | Time=18.4s\n",
      "[Seed 217401 | Ep 30] Train (L=1.1580 A=58.2%) | Val (L=1.1412 A=58.8%) | Hyper (LR=5.36e-04 WD=7.02e-02) | Time=18.6s [HG UPDATE: dLR=6.4e+04, dWD=3.8e+02]\n",
      "[Seed 217401 | Ep 31] Train (L=1.1096 A=60.3%) | Val (L=1.1073 A=61.2%) | Hyper (LR=5.36e-04 WD=7.02e-02) | Time=18.7s\n",
      "[Seed 217401 | Ep 32] Train (L=1.0821 A=61.0%) | Val (L=1.0728 A=61.8%) | Hyper (LR=6.43e-04 WD=7.02e-02) | Time=18.6s [HG UPDATE: dLR=-8.3e+04, dWD=1.5e+03]\n",
      "[Seed 217401 | Ep 33] Train (L=1.0954 A=60.6%) | Val (L=1.1398 A=59.4%) | Hyper (LR=6.43e-04 WD=7.02e-02) | Time=18.6s\n",
      "[Seed 217401 | Ep 34] Train (L=1.0783 A=61.2%) | Val (L=1.1096 A=60.8%) | Hyper (LR=5.14e-04 WD=7.02e-02) | Time=18.6s [HG UPDATE: dLR=5.5e+05, dWD=2.8e+03]\n",
      "[Seed 217401 | Ep 35] Train (L=1.0260 A=63.1%) | Val (L=1.0281 A=63.7%) | Hyper (LR=5.14e-04 WD=7.02e-02) | Time=18.6s\n",
      "[Seed 217401 | Ep 36] Train (L=1.0084 A=63.7%) | Val (L=1.0452 A=62.8%) | Hyper (LR=4.11e-04 WD=7.02e-02) | Time=19.0s [HG UPDATE: dLR=2.0e+05, dWD=-1.4e+03]\n",
      "[Seed 217401 | Ep 37] Train (L=0.9661 A=65.4%) | Val (L=0.9883 A=65.3%) | Hyper (LR=4.11e-04 WD=7.02e-02) | Time=18.7s\n",
      "[Seed 217401 | Ep 38] Train (L=0.9469 A=66.0%) | Val (L=0.9811 A=64.9%) | Hyper (LR=4.40e-04 WD=7.02e-02) | Time=18.6s [HG UPDATE: dLR=-2.9e+03, dWD=-1.8e+03]\n",
      "[Seed 217401 | Ep 39] Train (L=0.9342 A=66.5%) | Val (L=0.9860 A=65.0%) | Hyper (LR=4.40e-04 WD=7.02e-02) | Time=18.6s\n",
      "[Seed 217401 | Ep 40] Train (L=0.9238 A=66.8%) | Val (L=0.9737 A=65.2%) | Hyper (LR=5.28e-04 WD=7.02e-02) | Time=18.6s [HG UPDATE: dLR=-1.7e+04, dWD=-1.5e+01]\n",
      "[Seed 217401 | Ep 41] Train (L=0.9308 A=66.4%) | Val (L=1.0055 A=64.5%) | Hyper (LR=5.28e-04 WD=7.02e-02) | Time=18.5s\n",
      "[Seed 217401 | Ep 42] Train (L=0.9214 A=67.0%) | Val (L=0.9823 A=65.6%) | Hyper (LR=6.34e-04 WD=7.02e-02) | Time=18.5s [HG UPDATE: dLR=-5.1e+05, dWD=-4.1e+02]\n",
      "[Seed 217401 | Ep 43] Train (L=0.9294 A=66.6%) | Val (L=1.0066 A=64.5%) | Hyper (LR=6.34e-04 WD=7.02e-02) | Time=18.6s\n",
      "[Seed 217401 | Ep 44] Train (L=0.9179 A=67.2%) | Val (L=0.9599 A=66.2%) | Hyper (LR=5.07e-04 WD=7.02e-02) | Time=18.5s [HG UPDATE: dLR=4.0e+04, dWD=1.9e+03]\n",
      "[Seed 217401 | Ep 45] Train (L=0.8709 A=68.6%) | Val (L=0.9429 A=66.7%) | Hyper (LR=5.07e-04 WD=7.02e-02) | Time=18.5s\n",
      "[Seed 217401 | Ep 46] Train (L=0.8537 A=69.5%) | Val (L=0.9198 A=67.9%) | Hyper (LR=6.08e-04 WD=7.01e-02) | Time=18.5s [HG UPDATE: dLR=-2.9e+05, dWD=3.0e+03]\n",
      "[Seed 217401 | Ep 47] Train (L=0.8685 A=68.9%) | Val (L=0.9883 A=65.6%) | Hyper (LR=6.08e-04 WD=7.01e-02) | Time=18.5s\n",
      "[Seed 217401 | Ep 48] Train (L=0.8606 A=69.4%) | Val (L=0.9429 A=67.4%) | Hyper (LR=4.87e-04 WD=7.01e-02) | Time=18.7s [HG UPDATE: dLR=2.7e+05, dWD=-3.9e+02]\n",
      "[Seed 217401 | Ep 49] Train (L=0.8127 A=71.0%) | Val (L=0.9217 A=67.8%) | Hyper (LR=4.87e-04 WD=7.01e-02) | Time=18.6s\n",
      "[Seed 217401 | Ep 50] Train (L=0.7989 A=71.2%) | Val (L=0.9088 A=68.2%) | Hyper (LR=5.84e-04 WD=7.02e-02) | Time=18.4s [HG UPDATE: dLR=-1.4e+05, dWD=-1.1e+03]\n",
      "[Seed 217401 | Ep 51] Train (L=0.8086 A=71.0%) | Val (L=0.9283 A=67.4%) | Hyper (LR=5.84e-04 WD=7.02e-02) | Time=18.7s\n",
      "[Seed 217401 | Ep 52] Train (L=0.8034 A=71.1%) | Val (L=0.9445 A=66.9%) | Hyper (LR=7.01e-04 WD=7.02e-02) | Time=18.6s [HG UPDATE: dLR=-1.4e+05, dWD=-7.0e+02]\n",
      "[Seed 217401 | Ep 53] Train (L=0.8239 A=70.7%) | Val (L=0.9320 A=67.3%) | Hyper (LR=7.01e-04 WD=7.02e-02) | Time=18.5s\n",
      "[Seed 217401 | Ep 54] Train (L=0.8154 A=70.8%) | Val (L=0.9419 A=67.8%) | Hyper (LR=5.61e-04 WD=7.02e-02) | Time=18.5s [HG UPDATE: dLR=2.1e+05, dWD=-4.0e+03]\n",
      "[Seed 217401 | Ep 55] Train (L=0.7670 A=72.4%) | Val (L=0.9067 A=68.8%) | Hyper (LR=5.61e-04 WD=7.02e-02) | Time=18.6s\n",
      "[Seed 217401 | Ep 56] Train (L=0.7554 A=73.1%) | Val (L=0.8927 A=69.1%) | Hyper (LR=4.49e-04 WD=7.02e-02) | Time=18.8s [HG UPDATE: dLR=2.9e+04, dWD=-1.6e+03]\n",
      "[Seed 217401 | Ep 57] Train (L=0.7037 A=75.0%) | Val (L=0.9082 A=69.5%) | Hyper (LR=4.49e-04 WD=7.02e-02) | Time=18.4s\n",
      "[Seed 217401 | Ep 58] Train (L=0.6891 A=75.4%) | Val (L=0.8689 A=70.5%) | Hyper (LR=3.59e-04 WD=7.02e-02) | Time=18.6s [HG UPDATE: dLR=7.6e+05, dWD=7.8e+02]\n",
      "[Seed 217401 | Ep 59] Train (L=0.6425 A=76.9%) | Val (L=0.8795 A=70.4%) | Hyper (LR=3.59e-04 WD=7.02e-02) | Time=18.4s\n",
      "[Seed 217401 | Ep 60] Train (L=0.6303 A=77.4%) | Val (L=0.8575 A=71.0%) | Hyper (LR=4.31e-04 WD=7.02e-02) | Time=18.5s [HG UPDATE: dLR=-3.8e+05, dWD=1.3e+03]\n",
      "[Seed 217401 | Ep 61] Train (L=0.6420 A=77.1%) | Val (L=0.9068 A=69.6%) | Hyper (LR=4.31e-04 WD=7.02e-02) | Time=18.6s\n",
      "[Seed 217401 | Ep 62] Train (L=0.6374 A=77.1%) | Val (L=0.8776 A=70.7%) | Hyper (LR=5.17e-04 WD=7.02e-02) | Time=18.6s [HG UPDATE: dLR=-4.8e+05, dWD=2.7e+03]\n",
      "[Seed 217401 | Ep 63] Train (L=0.6549 A=76.3%) | Val (L=0.9100 A=69.7%) | Hyper (LR=5.17e-04 WD=7.02e-02) | Time=18.6s\n",
      "[Seed 217401 | Ep 64] Train (L=0.6441 A=76.7%) | Val (L=0.9108 A=69.3%) | Hyper (LR=6.20e-04 WD=7.02e-02) | Time=18.6s [HG UPDATE: dLR=-1.9e+04, dWD=-7.6e+02]\n",
      "[Seed 217401 | Ep 65] Train (L=0.6830 A=75.5%) | Val (L=0.9116 A=69.2%) | Hyper (LR=6.20e-04 WD=7.02e-02) | Time=18.6s\n",
      "[Seed 217401 | Ep 66] Train (L=0.6705 A=76.0%) | Val (L=0.8953 A=69.8%) | Hyper (LR=4.96e-04 WD=7.01e-02) | Time=18.6s [HG UPDATE: dLR=5.5e+05, dWD=2.8e+03]\n",
      "[Seed 217401 | Ep 67] Train (L=0.6179 A=77.8%) | Val (L=0.9035 A=69.8%) | Hyper (LR=4.96e-04 WD=7.01e-02) | Time=18.6s\n",
      "[Seed 217401 | Ep 68] Train (L=0.6143 A=77.7%) | Val (L=0.8897 A=70.3%) | Hyper (LR=3.97e-04 WD=7.01e-02) | Time=18.4s [HG UPDATE: dLR=5.8e+04, dWD=3.5e+02]\n",
      "[Seed 217401 | Ep 69] Train (L=0.5601 A=79.8%) | Val (L=0.9105 A=70.8%) | Hyper (LR=3.97e-04 WD=7.01e-02) | Time=18.6s\n",
      "[Seed 217401 | Ep 70] Train (L=0.5453 A=80.4%) | Val (L=0.9238 A=70.4%) | Hyper (LR=4.76e-04 WD=7.01e-02) | Time=18.7s [HG UPDATE: dLR=-4.4e+05, dWD=1.8e+03]\n",
      "[Seed 217401 | Ep 71] Train (L=0.5716 A=79.2%) | Val (L=0.9116 A=70.7%) | Hyper (LR=4.76e-04 WD=7.01e-02) | Time=18.5s\n",
      "[Seed 217401 | Ep 72] Train (L=0.5678 A=79.5%) | Val (L=0.9533 A=69.8%) | Hyper (LR=3.81e-04 WD=7.01e-02) | Time=18.5s [HG UPDATE: dLR=1.4e+05, dWD=-2.4e+02]\n",
      "[Seed 217401 | Ep 73] Train (L=0.5144 A=81.6%) | Val (L=0.9136 A=70.4%) | Hyper (LR=3.81e-04 WD=7.01e-02) | Time=18.7s\n",
      "[Seed 217401 | Ep 74] Train (L=0.4999 A=81.9%) | Val (L=0.9221 A=71.0%) | Hyper (LR=3.05e-04 WD=7.01e-02) | Time=18.6s [HG UPDATE: dLR=6.3e+04, dWD=3.0e+03]\n",
      "[Seed 217401 | Ep 75] Train (L=0.4545 A=83.7%) | Val (L=0.9482 A=71.2%) | Hyper (LR=3.05e-04 WD=7.01e-02) | Time=18.6s\n",
      "[Seed 217401 | Ep 76] Train (L=0.4442 A=83.9%) | Val (L=0.9539 A=71.5%) | Hyper (LR=3.66e-04 WD=7.01e-02) | Time=18.4s [HG UPDATE: dLR=-6.9e+05, dWD=3.1e+02]\n",
      "[Seed 217401 | Ep 77] Train (L=0.4661 A=83.2%) | Val (L=0.9493 A=70.9%) | Hyper (LR=3.66e-04 WD=7.01e-02) | Time=18.4s\n",
      "[Seed 217401 | Ep 78] Train (L=0.4578 A=83.4%) | Val (L=0.9473 A=71.2%) | Hyper (LR=2.93e-04 WD=7.01e-02) | Time=18.8s [HG UPDATE: dLR=1.3e+05, dWD=1.3e+03]\n",
      "[Seed 217401 | Ep 79] Train (L=0.4172 A=85.0%) | Val (L=0.9659 A=71.5%) | Hyper (LR=2.93e-04 WD=7.01e-02) | Time=18.7s\n",
      "[Seed 217401 | Ep 80] Train (L=0.4111 A=85.2%) | Val (L=0.9723 A=70.9%) | Hyper (LR=2.34e-04 WD=7.01e-02) | Time=18.6s [HG UPDATE: dLR=1.0e+05, dWD=1.5e+03]\n",
      "[Seed 217401 | Ep 81] Train (L=0.3702 A=86.7%) | Val (L=0.9669 A=71.8%) | Hyper (LR=2.34e-04 WD=7.01e-02) | Time=18.7s\n",
      "[Seed 217401 | Ep 82] Train (L=0.3633 A=86.8%) | Val (L=0.9672 A=71.6%) | Hyper (LR=2.81e-04 WD=7.01e-02) | Time=18.7s [HG UPDATE: dLR=-7.3e+05, dWD=7.2e+02]\n",
      "[Seed 217401 | Ep 83] Train (L=0.3711 A=86.6%) | Val (L=0.9662 A=71.9%) | Hyper (LR=2.81e-04 WD=7.01e-02) | Time=18.6s\n",
      "[Seed 217401 | Ep 84] Train (L=0.3741 A=86.6%) | Val (L=1.0196 A=70.9%) | Hyper (LR=3.37e-04 WD=7.00e-02) | Time=18.7s [HG UPDATE: dLR=-6.2e+05, dWD=1.3e+03]\n",
      "[Seed 217401 | Ep 85] Train (L=0.3933 A=85.7%) | Val (L=0.9953 A=71.1%) | Hyper (LR=3.37e-04 WD=7.00e-02) | Time=18.8s\n",
      "[Seed 217401 | Ep 86] Train (L=0.3927 A=85.9%) | Val (L=0.9976 A=71.0%) | Hyper (LR=2.70e-04 WD=7.01e-02) | Time=18.6s [HG UPDATE: dLR=2.3e+05, dWD=-5.8e+02]\n",
      "[Seed 217401 | Ep 87] Train (L=0.3575 A=87.1%) | Val (L=1.0185 A=71.6%) | Hyper (LR=2.70e-04 WD=7.01e-02) | Time=18.5s\n",
      "[Seed 217401 | Ep 88] Train (L=0.3469 A=87.5%) | Val (L=1.0029 A=71.9%) | Hyper (LR=3.24e-04 WD=7.01e-02) | Time=18.5s [HG UPDATE: dLR=-7.3e+05, dWD=-5.9e+02]\n",
      "[Seed 217401 | Ep 89] Train (L=0.3686 A=86.7%) | Val (L=1.0018 A=71.3%) | Hyper (LR=3.24e-04 WD=7.01e-02) | Time=18.6s\n",
      "--> Completed Seed 217401. Saved checkpoint to 'hypergradient_results.csv'\n",
      "\n",
      "================================================================================\n",
      "STARTING SEED: 637451\n",
      "================================================================================\n",
      "[Seed 637451 | Ep 00] Train (L=1.9842 A=24.4%) | Val (L=1.8073 A=31.2%) | Hyper (LR=3.96e-04 WD=7.01e-02) | Time=18.6s [HG UPDATE: dLR=-1.5e+06, dWD=-9.5e+03]\n",
      "[Seed 637451 | Ep 01] Train (L=1.7624 A=33.7%) | Val (L=1.6978 A=37.4%) | Hyper (LR=3.96e-04 WD=7.01e-02) | Time=18.6s\n",
      "[Seed 637451 | Ep 02] Train (L=1.6570 A=38.6%) | Val (L=1.6037 A=41.4%) | Hyper (LR=3.17e-04 WD=7.01e-02) | Time=18.6s [HG UPDATE: dLR=8.7e+05, dWD=-3.4e+03]\n",
      "[Seed 637451 | Ep 03] Train (L=1.5805 A=41.7%) | Val (L=1.5239 A=45.2%) | Hyper (LR=3.17e-04 WD=7.01e-02) | Time=18.6s\n",
      "[Seed 637451 | Ep 04] Train (L=1.5425 A=43.4%) | Val (L=1.4657 A=46.8%) | Hyper (LR=2.53e-04 WD=7.01e-02) | Time=18.7s [HG UPDATE: dLR=1.5e+05, dWD=1.4e+03]\n",
      "[Seed 637451 | Ep 05] Train (L=1.4980 A=45.2%) | Val (L=1.4418 A=47.2%) | Hyper (LR=2.53e-04 WD=7.01e-02) | Time=18.5s\n",
      "[Seed 637451 | Ep 06] Train (L=1.4692 A=46.2%) | Val (L=1.4171 A=48.6%) | Hyper (LR=3.04e-04 WD=7.02e-02) | Time=18.6s [HG UPDATE: dLR=-8.2e+05, dWD=-4.1e+03]\n",
      "[Seed 637451 | Ep 07] Train (L=1.4671 A=46.4%) | Val (L=1.4310 A=47.8%) | Hyper (LR=3.04e-04 WD=7.02e-02) | Time=18.7s\n",
      "[Seed 637451 | Ep 08] Train (L=1.4522 A=47.0%) | Val (L=1.4405 A=48.5%) | Hyper (LR=3.65e-04 WD=7.01e-02) | Time=18.5s [HG UPDATE: dLR=-1.3e+06, dWD=3.4e+03]\n",
      "[Seed 637451 | Ep 09] Train (L=1.4472 A=47.0%) | Val (L=1.4252 A=49.4%) | Hyper (LR=3.65e-04 WD=7.01e-02) | Time=18.6s\n",
      "[Seed 637451 | Ep 10] Train (L=1.4265 A=48.0%) | Val (L=1.3581 A=50.7%) | Hyper (LR=2.92e-04 WD=7.01e-02) | Time=18.7s [HG UPDATE: dLR=3.0e+04, dWD=-8.2e+02]\n",
      "[Seed 637451 | Ep 11] Train (L=1.3872 A=49.3%) | Val (L=1.3873 A=50.4%) | Hyper (LR=2.92e-04 WD=7.01e-02) | Time=18.6s\n",
      "[Seed 637451 | Ep 12] Train (L=1.3744 A=50.0%) | Val (L=1.3378 A=51.0%) | Hyper (LR=2.34e-04 WD=7.02e-02) | Time=18.6s [HG UPDATE: dLR=1.4e+06, dWD=-2.3e+03]\n",
      "[Seed 637451 | Ep 13] Train (L=1.3381 A=51.2%) | Val (L=1.3112 A=52.9%) | Hyper (LR=2.34e-04 WD=7.02e-02) | Time=18.7s\n",
      "[Seed 637451 | Ep 14] Train (L=1.3225 A=51.9%) | Val (L=1.3329 A=51.7%) | Hyper (LR=2.80e-04 WD=7.02e-02) | Time=18.6s [HG UPDATE: dLR=-7.8e+05, dWD=-1.8e+03]\n",
      "[Seed 637451 | Ep 15] Train (L=1.3273 A=51.9%) | Val (L=1.2652 A=54.4%) | Hyper (LR=2.80e-04 WD=7.02e-02) | Time=18.6s\n",
      "[Seed 637451 | Ep 16] Train (L=1.3084 A=52.5%) | Val (L=1.2831 A=53.9%) | Hyper (LR=3.36e-04 WD=7.02e-02) | Time=18.6s [HG UPDATE: dLR=-7.6e+05, dWD=-1.7e+03]\n",
      "[Seed 637451 | Ep 17] Train (L=1.3134 A=52.4%) | Val (L=1.2689 A=54.1%) | Hyper (LR=3.36e-04 WD=7.02e-02) | Time=18.6s\n",
      "[Seed 637451 | Ep 18] Train (L=1.3000 A=52.8%) | Val (L=1.2814 A=53.4%) | Hyper (LR=4.04e-04 WD=7.02e-02) | Time=18.5s [HG UPDATE: dLR=-4.1e+05, dWD=-1.3e+03]\n",
      "[Seed 637451 | Ep 19] Train (L=1.3054 A=52.6%) | Val (L=1.3137 A=52.6%) | Hyper (LR=4.04e-04 WD=7.02e-02) | Time=18.4s\n",
      "[Seed 637451 | Ep 20] Train (L=1.2878 A=53.0%) | Val (L=1.2766 A=54.5%) | Hyper (LR=3.23e-04 WD=7.02e-02) | Time=18.5s [HG UPDATE: dLR=3.1e+05, dWD=-4.3e+02]\n",
      "[Seed 637451 | Ep 21] Train (L=1.2607 A=54.6%) | Val (L=1.2373 A=55.5%) | Hyper (LR=3.23e-04 WD=7.02e-02) | Time=18.4s\n",
      "[Seed 637451 | Ep 22] Train (L=1.2362 A=55.5%) | Val (L=1.2178 A=55.8%) | Hyper (LR=2.58e-04 WD=7.02e-02) | Time=18.5s [HG UPDATE: dLR=2.2e+04, dWD=-1.3e+03]\n",
      "[Seed 637451 | Ep 23] Train (L=1.2074 A=56.3%) | Val (L=1.1909 A=57.5%) | Hyper (LR=2.58e-04 WD=7.02e-02) | Time=18.5s\n",
      "[Seed 637451 | Ep 24] Train (L=1.1916 A=57.1%) | Val (L=1.1649 A=58.2%) | Hyper (LR=2.07e-04 WD=7.02e-02) | Time=18.7s [HG UPDATE: dLR=1.8e+05, dWD=-1.0e+03]\n",
      "[Seed 637451 | Ep 25] Train (L=1.1579 A=58.2%) | Val (L=1.1480 A=59.1%) | Hyper (LR=2.07e-04 WD=7.02e-02) | Time=18.5s\n",
      "[Seed 637451 | Ep 26] Train (L=1.1432 A=58.8%) | Val (L=1.1413 A=59.8%) | Hyper (LR=2.48e-04 WD=7.02e-02) | Time=18.4s [HG UPDATE: dLR=-5.7e+04, dWD=7.5e+02]\n",
      "[Seed 637451 | Ep 27] Train (L=1.1460 A=58.7%) | Val (L=1.1570 A=58.9%) | Hyper (LR=2.48e-04 WD=7.02e-02) | Time=18.6s\n",
      "[Seed 637451 | Ep 28] Train (L=1.1380 A=59.0%) | Val (L=1.1354 A=59.2%) | Hyper (LR=2.98e-04 WD=7.02e-02) | Time=18.6s [HG UPDATE: dLR=-3.9e+04, dWD=-5.8e+02]\n",
      "[Seed 637451 | Ep 29] Train (L=1.1442 A=58.7%) | Val (L=1.1710 A=58.4%) | Hyper (LR=2.98e-04 WD=7.02e-02) | Time=18.6s\n",
      "[Seed 637451 | Ep 30] Train (L=1.1337 A=59.1%) | Val (L=1.1554 A=58.8%) | Hyper (LR=2.38e-04 WD=7.03e-02) | Time=18.5s [HG UPDATE: dLR=5.3e+04, dWD=-5.4e+03]\n",
      "[Seed 637451 | Ep 31] Train (L=1.1003 A=60.3%) | Val (L=1.1009 A=61.2%) | Hyper (LR=2.38e-04 WD=7.03e-02) | Time=18.4s\n",
      "[Seed 637451 | Ep 32] Train (L=1.0870 A=61.1%) | Val (L=1.0996 A=61.0%) | Hyper (LR=2.86e-04 WD=7.03e-02) | Time=18.6s [HG UPDATE: dLR=-4.6e+05, dWD=-1.9e+03]\n",
      "[Seed 637451 | Ep 33] Train (L=1.0920 A=60.6%) | Val (L=1.1082 A=60.6%) | Hyper (LR=2.86e-04 WD=7.03e-02) | Time=18.6s\n",
      "[Seed 637451 | Ep 34] Train (L=1.0762 A=61.5%) | Val (L=1.1082 A=61.0%) | Hyper (LR=2.29e-04 WD=7.03e-02) | Time=18.7s [HG UPDATE: dLR=4.0e+05, dWD=3.8e+03]\n",
      "[Seed 637451 | Ep 35] Train (L=1.0443 A=62.3%) | Val (L=1.0845 A=61.9%) | Hyper (LR=2.29e-04 WD=7.03e-02) | Time=18.5s\n",
      "[Seed 637451 | Ep 36] Train (L=1.0329 A=62.8%) | Val (L=1.0653 A=62.6%) | Hyper (LR=1.83e-04 WD=7.03e-02) | Time=18.6s [HG UPDATE: dLR=1.1e+06, dWD=2.5e+02]\n",
      "[Seed 637451 | Ep 37] Train (L=0.9979 A=64.2%) | Val (L=1.0312 A=63.5%) | Hyper (LR=1.83e-04 WD=7.03e-02) | Time=18.5s\n",
      "[Seed 637451 | Ep 38] Train (L=0.9847 A=64.5%) | Val (L=1.0292 A=64.0%) | Hyper (LR=2.19e-04 WD=7.02e-02) | Time=18.3s [HG UPDATE: dLR=-4.2e+05, dWD=2.1e+03]\n",
      "[Seed 637451 | Ep 39] Train (L=0.9888 A=64.4%) | Val (L=1.0309 A=64.3%) | Hyper (LR=2.19e-04 WD=7.02e-02) | Time=18.8s\n",
      "[Seed 637451 | Ep 40] Train (L=0.9854 A=64.5%) | Val (L=1.0807 A=61.9%) | Hyper (LR=2.63e-04 WD=7.02e-02) | Time=18.7s [HG UPDATE: dLR=-2.0e+05, dWD=1.9e+03]\n",
      "[Seed 637451 | Ep 41] Train (L=0.9921 A=64.3%) | Val (L=1.0583 A=62.9%) | Hyper (LR=2.63e-04 WD=7.02e-02) | Time=18.7s\n",
      "[Seed 637451 | Ep 42] Train (L=0.9830 A=64.9%) | Val (L=1.0211 A=64.2%) | Hyper (LR=3.16e-04 WD=7.02e-02) | Time=18.5s [HG UPDATE: dLR=-2.2e+05, dWD=3.5e+02]\n",
      "[Seed 637451 | Ep 43] Train (L=0.9956 A=64.3%) | Val (L=1.0555 A=63.4%) | Hyper (LR=3.16e-04 WD=7.02e-02) | Time=18.7s\n",
      "[Seed 637451 | Ep 44] Train (L=0.9902 A=64.6%) | Val (L=1.0372 A=64.1%) | Hyper (LR=2.53e-04 WD=7.03e-02) | Time=18.7s [HG UPDATE: dLR=7.6e+05, dWD=-5.7e+03]\n",
      "[Seed 637451 | Ep 45] Train (L=0.9457 A=66.1%) | Val (L=1.0363 A=64.2%) | Hyper (LR=2.53e-04 WD=7.03e-02) | Time=18.3s\n",
      "[Seed 637451 | Ep 46] Train (L=0.9369 A=66.3%) | Val (L=0.9825 A=65.4%) | Hyper (LR=3.03e-04 WD=7.03e-02) | Time=18.3s [HG UPDATE: dLR=-8.2e+05, dWD=-2.7e+03]\n",
      "[Seed 637451 | Ep 47] Train (L=0.9494 A=66.2%) | Val (L=1.0087 A=64.6%) | Hyper (LR=3.03e-04 WD=7.03e-02) | Time=18.5s\n",
      "[Seed 637451 | Ep 48] Train (L=0.9356 A=66.6%) | Val (L=1.0080 A=64.7%) | Hyper (LR=2.43e-04 WD=7.03e-02) | Time=18.6s [HG UPDATE: dLR=5.7e+05, dWD=-2.9e+03]\n",
      "[Seed 637451 | Ep 49] Train (L=0.9025 A=67.5%) | Val (L=0.9912 A=65.6%) | Hyper (LR=2.43e-04 WD=7.03e-02) | Time=18.6s\n",
      "[Seed 637451 | Ep 50] Train (L=0.8864 A=68.2%) | Val (L=0.9954 A=66.1%) | Hyper (LR=1.94e-04 WD=7.03e-02) | Time=18.6s [HG UPDATE: dLR=4.3e+05, dWD=3.5e+03]\n",
      "[Seed 637451 | Ep 51] Train (L=0.8495 A=69.4%) | Val (L=0.9658 A=67.1%) | Hyper (LR=1.94e-04 WD=7.03e-02) | Time=18.7s\n",
      "[Seed 637451 | Ep 52] Train (L=0.8306 A=70.2%) | Val (L=0.9472 A=67.2%) | Hyper (LR=2.33e-04 WD=7.03e-02) | Time=18.9s [HG UPDATE: dLR=-9.2e+05, dWD=-3.7e+03]\n",
      "[Seed 637451 | Ep 53] Train (L=0.8473 A=69.7%) | Val (L=0.9428 A=67.2%) | Hyper (LR=2.33e-04 WD=7.03e-02) | Time=18.4s\n",
      "[Seed 637451 | Ep 54] Train (L=0.8317 A=70.1%) | Val (L=0.9634 A=66.5%) | Hyper (LR=2.80e-04 WD=7.03e-02) | Time=18.6s [HG UPDATE: dLR=-4.5e+05, dWD=1.1e+03]\n",
      "[Seed 637451 | Ep 55] Train (L=0.8491 A=69.5%) | Val (L=0.9870 A=66.0%) | Hyper (LR=2.80e-04 WD=7.03e-02) | Time=18.6s\n",
      "[Seed 637451 | Ep 56] Train (L=0.8356 A=70.1%) | Val (L=0.9587 A=66.7%) | Hyper (LR=3.35e-04 WD=7.03e-02) | Time=18.7s [HG UPDATE: dLR=-4.6e+05, dWD=6.8e+02]\n",
      "[Seed 637451 | Ep 57] Train (L=0.8567 A=69.4%) | Val (L=0.9482 A=67.0%) | Hyper (LR=3.35e-04 WD=7.03e-02) | Time=18.6s\n",
      "[Seed 637451 | Ep 58] Train (L=0.8376 A=69.9%) | Val (L=0.9749 A=66.4%) | Hyper (LR=4.02e-04 WD=7.03e-02) | Time=18.8s [HG UPDATE: dLR=-5.1e+04, dWD=3.0e+03]\n",
      "[Seed 637451 | Ep 59] Train (L=0.8645 A=68.9%) | Val (L=0.9787 A=66.5%) | Hyper (LR=4.02e-04 WD=7.03e-02) | Time=18.6s\n",
      "[Seed 637451 | Ep 60] Train (L=0.8557 A=69.4%) | Val (L=0.9299 A=67.4%) | Hyper (LR=3.22e-04 WD=7.03e-02) | Time=18.8s [HG UPDATE: dLR=2.9e+05, dWD=-7.5e+02]\n",
      "[Seed 637451 | Ep 61] Train (L=0.8085 A=71.0%) | Val (L=0.9400 A=67.3%) | Hyper (LR=3.22e-04 WD=7.03e-02) | Time=18.6s\n",
      "[Seed 637451 | Ep 62] Train (L=0.7905 A=71.6%) | Val (L=0.9089 A=68.7%) | Hyper (LR=3.86e-04 WD=7.03e-02) | Time=18.6s [HG UPDATE: dLR=-1.2e+05, dWD=-2.7e+01]\n",
      "[Seed 637451 | Ep 63] Train (L=0.8131 A=70.8%) | Val (L=0.9322 A=67.0%) | Hyper (LR=3.86e-04 WD=7.03e-02) | Time=18.7s\n",
      "[Seed 637451 | Ep 64] Train (L=0.8065 A=71.1%) | Val (L=0.9414 A=67.5%) | Hyper (LR=3.09e-04 WD=7.03e-02) | Time=18.9s [HG UPDATE: dLR=3.9e+05, dWD=-2.9e+03]\n",
      "[Seed 637451 | Ep 65] Train (L=0.7608 A=72.8%) | Val (L=0.8983 A=69.5%) | Hyper (LR=3.09e-04 WD=7.03e-02) | Time=18.7s\n",
      "[Seed 637451 | Ep 66] Train (L=0.7461 A=73.1%) | Val (L=0.8859 A=69.3%) | Hyper (LR=3.71e-04 WD=7.03e-02) | Time=18.5s [HG UPDATE: dLR=-1.3e+05, dWD=7.1e+02]\n",
      "[Seed 637451 | Ep 67] Train (L=0.7664 A=72.5%) | Val (L=0.9228 A=68.4%) | Hyper (LR=3.71e-04 WD=7.03e-02) | Time=18.5s\n",
      "[Seed 637451 | Ep 68] Train (L=0.7580 A=72.7%) | Val (L=0.9227 A=69.0%) | Hyper (LR=2.97e-04 WD=7.03e-02) | Time=18.7s [HG UPDATE: dLR=1.7e+05, dWD=2.2e+03]\n",
      "[Seed 637451 | Ep 69] Train (L=0.7072 A=74.7%) | Val (L=0.9146 A=68.8%) | Hyper (LR=2.97e-04 WD=7.03e-02) | Time=18.8s\n",
      "[Seed 637451 | Ep 70] Train (L=0.6909 A=75.3%) | Val (L=0.8840 A=69.6%) | Hyper (LR=2.37e-04 WD=7.03e-02) | Time=18.5s [HG UPDATE: dLR=1.6e+05, dWD=-3.5e+03]\n",
      "[Seed 637451 | Ep 71] Train (L=0.6435 A=76.9%) | Val (L=0.8902 A=70.0%) | Hyper (LR=2.37e-04 WD=7.03e-02) | Time=18.6s\n",
      "[Seed 637451 | Ep 72] Train (L=0.6356 A=77.1%) | Val (L=0.9037 A=70.3%) | Hyper (LR=2.85e-04 WD=7.03e-02) | Time=18.5s [HG UPDATE: dLR=-2.6e+05, dWD=1.1e+03]\n",
      "[Seed 637451 | Ep 73] Train (L=0.6434 A=76.7%) | Val (L=0.8817 A=70.6%) | Hyper (LR=2.85e-04 WD=7.03e-02) | Time=18.7s\n",
      "[Seed 637451 | Ep 74] Train (L=0.6320 A=77.4%) | Val (L=0.8939 A=70.1%) | Hyper (LR=3.42e-04 WD=7.03e-02) | Time=18.7s [HG UPDATE: dLR=-5.7e+05, dWD=-5.9e+02]\n",
      "[Seed 637451 | Ep 75] Train (L=0.6548 A=76.5%) | Val (L=0.9100 A=69.2%) | Hyper (LR=3.42e-04 WD=7.03e-02) | Time=18.5s\n",
      "[Seed 637451 | Ep 76] Train (L=0.6566 A=76.5%) | Val (L=0.9091 A=69.7%) | Hyper (LR=2.73e-04 WD=7.03e-02) | Time=18.7s [HG UPDATE: dLR=2.7e+04, dWD=2.7e+03]\n",
      "[Seed 637451 | Ep 77] Train (L=0.6118 A=78.0%) | Val (L=0.8909 A=70.7%) | Hyper (LR=2.73e-04 WD=7.03e-02) | Time=18.7s\n",
      "[Seed 637451 | Ep 78] Train (L=0.5937 A=78.7%) | Val (L=0.9065 A=70.4%) | Hyper (LR=3.28e-04 WD=7.03e-02) | Time=18.7s [HG UPDATE: dLR=-2.7e+05, dWD=-8.9e+02]\n",
      "[Seed 637451 | Ep 79] Train (L=0.6148 A=77.9%) | Val (L=0.8896 A=70.2%) | Hyper (LR=3.28e-04 WD=7.03e-02) | Time=18.5s\n",
      "[Seed 637451 | Ep 80] Train (L=0.6140 A=77.9%) | Val (L=0.8863 A=70.6%) | Hyper (LR=2.63e-04 WD=7.03e-02) | Time=18.6s [HG UPDATE: dLR=2.1e+04, dWD=-1.0e+03]\n",
      "[Seed 637451 | Ep 81] Train (L=0.5553 A=80.0%) | Val (L=0.8898 A=70.9%) | Hyper (LR=2.63e-04 WD=7.03e-02) | Time=18.6s\n",
      "[Seed 637451 | Ep 82] Train (L=0.5456 A=80.3%) | Val (L=0.9044 A=70.6%) | Hyper (LR=3.15e-04 WD=7.03e-02) | Time=18.7s [HG UPDATE: dLR=-3.8e+05, dWD=-7.3e+02]\n",
      "[Seed 637451 | Ep 83] Train (L=0.5650 A=79.6%) | Val (L=0.9291 A=70.3%) | Hyper (LR=3.15e-04 WD=7.03e-02) | Time=18.6s\n",
      "[Seed 637451 | Ep 84] Train (L=0.5555 A=80.0%) | Val (L=0.9531 A=69.5%) | Hyper (LR=2.52e-04 WD=7.03e-02) | Time=18.5s [HG UPDATE: dLR=3.4e+05, dWD=9.8e+02]\n",
      "[Seed 637451 | Ep 85] Train (L=0.5167 A=81.3%) | Val (L=0.9096 A=70.8%) | Hyper (LR=2.52e-04 WD=7.03e-02) | Time=18.8s\n",
      "[Seed 637451 | Ep 86] Train (L=0.5081 A=81.5%) | Val (L=0.8976 A=71.3%) | Hyper (LR=2.02e-04 WD=7.03e-02) | Time=18.5s [HG UPDATE: dLR=4.8e+05, dWD=-3.0e+03]\n",
      "[Seed 637451 | Ep 87] Train (L=0.4588 A=83.3%) | Val (L=0.9157 A=71.2%) | Hyper (LR=2.02e-04 WD=7.03e-02) | Time=18.4s\n",
      "[Seed 637451 | Ep 88] Train (L=0.4483 A=83.9%) | Val (L=0.9262 A=71.0%) | Hyper (LR=1.61e-04 WD=7.03e-02) | Time=18.5s [HG UPDATE: dLR=2.1e+05, dWD=5.0e+02]\n",
      "[Seed 637451 | Ep 89] Train (L=0.4168 A=84.9%) | Val (L=0.9500 A=71.8%) | Hyper (LR=1.61e-04 WD=7.03e-02) | Time=18.6s\n",
      "--> Completed Seed 637451. Saved checkpoint to 'hypergradient_results.csv'\n",
      "\n",
      "================================================================================\n",
      "STARTING SEED: 207796\n",
      "================================================================================\n",
      "[Seed 207796 | Ep 00] Train (L=2.0036 A=23.7%) | Val (L=1.7524 A=35.2%) | Hyper (LR=3.96e-04 WD=6.99e-02) | Time=18.6s [HG UPDATE: dLR=-2.1e+06, dWD=5.9e+03]\n",
      "[Seed 207796 | Ep 01] Train (L=1.7655 A=34.1%) | Val (L=1.7041 A=38.0%) | Hyper (LR=3.96e-04 WD=6.99e-02) | Time=18.4s\n",
      "[Seed 207796 | Ep 02] Train (L=1.6549 A=39.0%) | Val (L=1.6339 A=41.2%) | Hyper (LR=3.17e-04 WD=7.00e-02) | Time=18.5s [HG UPDATE: dLR=3.1e+05, dWD=-5.8e+03]\n",
      "[Seed 207796 | Ep 03] Train (L=1.5859 A=41.6%) | Val (L=1.5492 A=43.2%) | Hyper (LR=3.17e-04 WD=7.00e-02) | Time=18.6s\n",
      "[Seed 207796 | Ep 04] Train (L=1.5459 A=43.0%) | Val (L=1.5298 A=44.6%) | Hyper (LR=2.53e-04 WD=7.00e-02) | Time=18.5s [HG UPDATE: dLR=2.2e+05, dWD=3.4e+03]\n",
      "[Seed 207796 | Ep 05] Train (L=1.5094 A=44.7%) | Val (L=1.4530 A=46.7%) | Hyper (LR=2.53e-04 WD=7.00e-02) | Time=18.5s\n",
      "[Seed 207796 | Ep 06] Train (L=1.4842 A=45.2%) | Val (L=1.4596 A=47.2%) | Hyper (LR=2.03e-04 WD=7.00e-02) | Time=18.6s [HG UPDATE: dLR=4.5e+05, dWD=-4.5e+03]\n",
      "[Seed 207796 | Ep 07] Train (L=1.4554 A=46.7%) | Val (L=1.4436 A=47.6%) | Hyper (LR=2.03e-04 WD=7.00e-02) | Time=18.6s\n",
      "[Seed 207796 | Ep 08] Train (L=1.4350 A=47.4%) | Val (L=1.4011 A=49.4%) | Hyper (LR=2.43e-04 WD=7.00e-02) | Time=18.6s [HG UPDATE: dLR=-1.5e+06, dWD=1.6e+03]\n",
      "[Seed 207796 | Ep 09] Train (L=1.4348 A=47.5%) | Val (L=1.4397 A=47.5%) | Hyper (LR=2.43e-04 WD=7.00e-02) | Time=18.6s\n",
      "[Seed 207796 | Ep 10] Train (L=1.4130 A=48.2%) | Val (L=1.3856 A=50.1%) | Hyper (LR=1.95e-04 WD=7.00e-02) | Time=18.5s [HG UPDATE: dLR=1.9e+06, dWD=4.1e+02]\n",
      "[Seed 207796 | Ep 11] Train (L=1.3806 A=50.0%) | Val (L=1.3838 A=50.4%) | Hyper (LR=1.95e-04 WD=7.00e-02) | Time=18.6s\n",
      "[Seed 207796 | Ep 12] Train (L=1.3715 A=50.0%) | Val (L=1.3904 A=49.5%) | Hyper (LR=1.56e-04 WD=7.00e-02) | Time=18.5s [HG UPDATE: dLR=4.9e+05, dWD=2.7e+03]\n",
      "[Seed 207796 | Ep 13] Train (L=1.3423 A=51.3%) | Val (L=1.3329 A=52.2%) | Hyper (LR=1.56e-04 WD=7.00e-02) | Time=18.6s\n",
      "[Seed 207796 | Ep 14] Train (L=1.3235 A=51.6%) | Val (L=1.3116 A=52.9%) | Hyper (LR=1.25e-04 WD=6.99e-02) | Time=18.7s [HG UPDATE: dLR=1.7e+06, dWD=5.3e+03]\n",
      "[Seed 207796 | Ep 15] Train (L=1.2982 A=52.9%) | Val (L=1.2979 A=53.7%) | Hyper (LR=1.25e-04 WD=6.99e-02) | Time=18.6s\n",
      "[Seed 207796 | Ep 16] Train (L=1.2848 A=53.3%) | Val (L=1.2671 A=54.0%) | Hyper (LR=1.49e-04 WD=6.99e-02) | Time=18.5s [HG UPDATE: dLR=-5.2e+05, dWD=1.8e+03]\n",
      "[Seed 207796 | Ep 17] Train (L=1.2901 A=53.2%) | Val (L=1.2816 A=54.1%) | Hyper (LR=1.49e-04 WD=6.99e-02) | Time=18.4s\n",
      "[Seed 207796 | Ep 18] Train (L=1.2781 A=53.8%) | Val (L=1.2834 A=54.4%) | Hyper (LR=1.20e-04 WD=6.99e-02) | Time=18.5s [HG UPDATE: dLR=6.7e+05, dWD=-1.7e+03]\n",
      "[Seed 207796 | Ep 19] Train (L=1.2608 A=54.3%) | Val (L=1.2610 A=54.9%) | Hyper (LR=1.20e-04 WD=6.99e-02) | Time=18.4s\n",
      "[Seed 207796 | Ep 20] Train (L=1.2433 A=54.9%) | Val (L=1.2858 A=53.8%) | Hyper (LR=1.44e-04 WD=6.99e-02) | Time=18.5s [HG UPDATE: dLR=-2.5e+05, dWD=9.2e+02]\n",
      "[Seed 207796 | Ep 21] Train (L=1.2480 A=54.7%) | Val (L=1.2400 A=55.6%) | Hyper (LR=1.44e-04 WD=6.99e-02) | Time=18.6s\n",
      "[Seed 207796 | Ep 22] Train (L=1.2406 A=55.1%) | Val (L=1.2513 A=54.1%) | Hyper (LR=1.15e-04 WD=6.99e-02) | Time=18.5s [HG UPDATE: dLR=1.1e+05, dWD=-2.0e+03]\n",
      "[Seed 207796 | Ep 23] Train (L=1.2135 A=56.0%) | Val (L=1.2289 A=56.0%) | Hyper (LR=1.15e-04 WD=6.99e-02) | Time=18.5s\n",
      "[Seed 207796 | Ep 24] Train (L=1.2058 A=56.5%) | Val (L=1.2221 A=56.4%) | Hyper (LR=1.38e-04 WD=6.99e-02) | Time=18.4s [HG UPDATE: dLR=-1.5e+06, dWD=-1.0e+02]\n",
      "[Seed 207796 | Ep 25] Train (L=1.2049 A=56.5%) | Val (L=1.2095 A=56.5%) | Hyper (LR=1.38e-04 WD=6.99e-02) | Time=18.6s\n",
      "[Seed 207796 | Ep 26] Train (L=1.1990 A=56.5%) | Val (L=1.2136 A=56.5%) | Hyper (LR=1.65e-04 WD=6.99e-02) | Time=18.4s [HG UPDATE: dLR=-4.6e+05, dWD=2.6e+02]\n",
      "[Seed 207796 | Ep 27] Train (L=1.2103 A=56.2%) | Val (L=1.2317 A=55.8%) | Hyper (LR=1.65e-04 WD=6.99e-02) | Time=18.7s\n",
      "[Seed 207796 | Ep 28] Train (L=1.1972 A=56.6%) | Val (L=1.2382 A=55.2%) | Hyper (LR=1.98e-04 WD=6.99e-02) | Time=18.7s [HG UPDATE: dLR=-1.3e+06, dWD=1.8e+03]\n",
      "[Seed 207796 | Ep 29] Train (L=1.2050 A=56.3%) | Val (L=1.2536 A=55.0%) | Hyper (LR=1.98e-04 WD=6.99e-02) | Time=18.4s\n",
      "[Seed 207796 | Ep 30] Train (L=1.1948 A=56.9%) | Val (L=1.2203 A=55.7%) | Hyper (LR=2.38e-04 WD=6.99e-02) | Time=18.5s [HG UPDATE: dLR=-1.5e+05, dWD=1.6e+03]\n",
      "[Seed 207796 | Ep 31] Train (L=1.2059 A=56.3%) | Val (L=1.2293 A=55.8%) | Hyper (LR=2.38e-04 WD=6.99e-02) | Time=18.6s\n",
      "[Seed 207796 | Ep 32] Train (L=1.2039 A=56.5%) | Val (L=1.2562 A=55.2%) | Hyper (LR=2.86e-04 WD=6.99e-02) | Time=18.4s [HG UPDATE: dLR=-1.1e+06, dWD=1.3e+03]\n",
      "[Seed 207796 | Ep 33] Train (L=1.2121 A=56.4%) | Val (L=1.2194 A=56.0%) | Hyper (LR=2.86e-04 WD=6.99e-02) | Time=18.5s\n",
      "[Seed 207796 | Ep 34] Train (L=1.1977 A=56.6%) | Val (L=1.2198 A=56.4%) | Hyper (LR=2.29e-04 WD=6.99e-02) | Time=18.7s [HG UPDATE: dLR=2.3e+05, dWD=8.3e+02]\n",
      "[Seed 207796 | Ep 35] Train (L=1.1688 A=57.8%) | Val (L=1.1865 A=57.7%) | Hyper (LR=2.29e-04 WD=6.99e-02) | Time=18.4s\n",
      "[Seed 207796 | Ep 36] Train (L=1.1589 A=58.4%) | Val (L=1.1972 A=57.3%) | Hyper (LR=1.83e-04 WD=6.99e-02) | Time=18.5s [HG UPDATE: dLR=1.3e+06, dWD=-5.1e+03]\n",
      "[Seed 207796 | Ep 37] Train (L=1.1243 A=59.6%) | Val (L=1.1700 A=58.3%) | Hyper (LR=1.83e-04 WD=6.99e-02) | Time=18.4s\n",
      "[Seed 207796 | Ep 38] Train (L=1.1152 A=59.8%) | Val (L=1.1774 A=58.8%) | Hyper (LR=1.46e-04 WD=6.99e-02) | Time=18.6s [HG UPDATE: dLR=4.7e+05, dWD=1.2e+03]\n",
      "[Seed 207796 | Ep 39] Train (L=1.0841 A=61.1%) | Val (L=1.1577 A=58.4%) | Hyper (LR=1.46e-04 WD=6.99e-02) | Time=18.5s\n",
      "[Seed 207796 | Ep 40] Train (L=1.0713 A=61.3%) | Val (L=1.1345 A=59.4%) | Hyper (LR=1.76e-04 WD=6.99e-02) | Time=18.5s [HG UPDATE: dLR=-4.7e+05, dWD=-6.9e+02]\n",
      "[Seed 207796 | Ep 41] Train (L=1.0820 A=60.9%) | Val (L=1.1401 A=60.0%) | Hyper (LR=1.76e-04 WD=6.99e-02) | Time=18.5s\n",
      "[Seed 207796 | Ep 42] Train (L=1.0743 A=61.4%) | Val (L=1.1326 A=59.7%) | Hyper (LR=1.40e-04 WD=6.99e-02) | Time=18.4s [HG UPDATE: dLR=9.0e+04, dWD=1.4e+03]\n",
      "[Seed 207796 | Ep 43] Train (L=1.0421 A=62.5%) | Val (L=1.1334 A=60.1%) | Hyper (LR=1.40e-04 WD=6.99e-02) | Time=18.4s\n",
      "[Seed 207796 | Ep 44] Train (L=1.0307 A=63.1%) | Val (L=1.0959 A=61.8%) | Hyper (LR=1.12e-04 WD=6.99e-02) | Time=18.6s [HG UPDATE: dLR=1.9e+05, dWD=-6.3e+02]\n",
      "[Seed 207796 | Ep 45] Train (L=1.0040 A=63.7%) | Val (L=1.0954 A=61.2%) | Hyper (LR=1.12e-04 WD=6.99e-02) | Time=18.7s\n",
      "[Seed 207796 | Ep 46] Train (L=0.9902 A=64.4%) | Val (L=1.1238 A=60.4%) | Hyper (LR=1.35e-04 WD=7.00e-02) | Time=18.5s [HG UPDATE: dLR=-1.9e+05, dWD=-5.1e+03]\n",
      "[Seed 207796 | Ep 47] Train (L=0.9988 A=64.2%) | Val (L=1.1196 A=60.6%) | Hyper (LR=1.35e-04 WD=7.00e-02) | Time=18.7s\n",
      "[Seed 207796 | Ep 48] Train (L=0.9902 A=64.6%) | Val (L=1.0856 A=61.8%) | Hyper (LR=1.62e-04 WD=6.99e-02) | Time=18.8s [HG UPDATE: dLR=-6.4e+05, dWD=1.7e+03]\n",
      "[Seed 207796 | Ep 49] Train (L=0.9996 A=64.1%) | Val (L=1.1036 A=60.9%) | Hyper (LR=1.62e-04 WD=6.99e-02) | Time=18.5s\n",
      "[Seed 207796 | Ep 50] Train (L=0.9893 A=64.6%) | Val (L=1.0938 A=61.4%) | Hyper (LR=1.94e-04 WD=6.99e-02) | Time=18.5s [HG UPDATE: dLR=-1.1e+05, dWD=6.5e+02]\n",
      "[Seed 207796 | Ep 51] Train (L=1.0017 A=63.8%) | Val (L=1.1080 A=61.3%) | Hyper (LR=1.94e-04 WD=6.99e-02) | Time=18.7s\n",
      "[Seed 207796 | Ep 52] Train (L=0.9991 A=64.0%) | Val (L=1.1069 A=61.1%) | Hyper (LR=2.33e-04 WD=6.99e-02) | Time=18.6s [HG UPDATE: dLR=-3.1e+05, dWD=-1.7e+03]\n",
      "[Seed 207796 | Ep 53] Train (L=1.0114 A=63.4%) | Val (L=1.1114 A=61.0%) | Hyper (LR=2.33e-04 WD=6.99e-02) | Time=18.5s\n",
      "[Seed 207796 | Ep 54] Train (L=1.0010 A=64.0%) | Val (L=1.1413 A=59.8%) | Hyper (LR=2.80e-04 WD=6.99e-02) | Time=18.6s [HG UPDATE: dLR=-1.1e+05, dWD=4.0e+03]\n",
      "[Seed 207796 | Ep 55] Train (L=1.0256 A=63.1%) | Val (L=1.1201 A=60.6%) | Hyper (LR=2.80e-04 WD=6.99e-02) | Time=18.5s\n",
      "[Seed 207796 | Ep 56] Train (L=1.0130 A=63.6%) | Val (L=1.1126 A=61.0%) | Hyper (LR=3.35e-04 WD=6.99e-02) | Time=18.7s [HG UPDATE: dLR=-5.3e+05, dWD=-1.5e+03]\n",
      "[Seed 207796 | Ep 57] Train (L=1.0415 A=62.5%) | Val (L=1.1115 A=60.2%) | Hyper (LR=3.35e-04 WD=6.99e-02) | Time=18.5s\n",
      "[Seed 207796 | Ep 58] Train (L=1.0358 A=62.8%) | Val (L=1.1602 A=59.5%) | Hyper (LR=4.02e-04 WD=6.99e-02) | Time=18.6s [HG UPDATE: dLR=-9.9e+05, dWD=2.1e+03]\n",
      "[Seed 207796 | Ep 59] Train (L=1.0547 A=62.0%) | Val (L=1.1103 A=60.4%) | Hyper (LR=4.02e-04 WD=6.99e-02) | Time=18.5s\n",
      "[Seed 207796 | Ep 60] Train (L=1.0472 A=62.4%) | Val (L=1.1203 A=60.6%) | Hyper (LR=4.83e-04 WD=6.99e-02) | Time=18.3s [HG UPDATE: dLR=-3.6e+05, dWD=-4.2e+03]\n",
      "[Seed 207796 | Ep 61] Train (L=1.0776 A=61.3%) | Val (L=1.1367 A=59.8%) | Hyper (LR=4.83e-04 WD=6.99e-02) | Time=18.4s\n",
      "[Seed 207796 | Ep 62] Train (L=1.0594 A=61.9%) | Val (L=1.1184 A=60.5%) | Hyper (LR=5.80e-04 WD=6.99e-02) | Time=18.6s [HG UPDATE: dLR=-3.7e+05, dWD=4.2e+03]\n",
      "[Seed 207796 | Ep 63] Train (L=1.0918 A=60.7%) | Val (L=1.1603 A=59.0%) | Hyper (LR=5.80e-04 WD=6.99e-02) | Time=18.6s\n",
      "[Seed 207796 | Ep 64] Train (L=1.0803 A=61.5%) | Val (L=1.1226 A=60.6%) | Hyper (LR=4.64e-04 WD=6.99e-02) | Time=18.6s [HG UPDATE: dLR=3.8e+05, dWD=3.7e+03]\n",
      "[Seed 207796 | Ep 65] Train (L=1.0260 A=63.2%) | Val (L=1.0867 A=61.5%) | Hyper (LR=4.64e-04 WD=6.99e-02) | Time=18.6s\n",
      "[Seed 207796 | Ep 66] Train (L=1.0056 A=64.1%) | Val (L=1.0846 A=62.2%) | Hyper (LR=3.71e-04 WD=6.98e-02) | Time=18.5s [HG UPDATE: dLR=1.0e+05, dWD=1.4e+03]\n",
      "[Seed 207796 | Ep 67] Train (L=0.9514 A=66.0%) | Val (L=1.0374 A=63.9%) | Hyper (LR=3.71e-04 WD=6.98e-02) | Time=18.5s\n",
      "[Seed 207796 | Ep 68] Train (L=0.9375 A=66.2%) | Val (L=1.0279 A=63.8%) | Hyper (LR=2.97e-04 WD=6.99e-02) | Time=18.5s [HG UPDATE: dLR=1.1e+06, dWD=-6.7e+03]\n",
      "[Seed 207796 | Ep 69] Train (L=0.8871 A=68.2%) | Val (L=1.0229 A=64.1%) | Hyper (LR=2.97e-04 WD=6.99e-02) | Time=18.6s\n",
      "[Seed 207796 | Ep 70] Train (L=0.8717 A=68.5%) | Val (L=1.0180 A=65.1%) | Hyper (LR=3.56e-04 WD=6.99e-02) | Time=18.4s [HG UPDATE: dLR=-1.3e+05, dWD=1.9e+03]\n",
      "[Seed 207796 | Ep 71] Train (L=0.8866 A=68.2%) | Val (L=1.0218 A=64.3%) | Hyper (LR=3.56e-04 WD=6.99e-02) | Time=18.3s\n",
      "[Seed 207796 | Ep 72] Train (L=0.8671 A=68.7%) | Val (L=1.0162 A=64.9%) | Hyper (LR=4.27e-04 WD=6.99e-02) | Time=18.7s [HG UPDATE: dLR=-8.4e+05, dWD=-1.7e+03]\n",
      "[Seed 207796 | Ep 73] Train (L=0.8879 A=68.3%) | Val (L=1.0447 A=63.9%) | Hyper (LR=4.27e-04 WD=6.99e-02) | Time=18.2s\n",
      "[Seed 207796 | Ep 74] Train (L=0.8832 A=68.3%) | Val (L=1.0265 A=64.9%) | Hyper (LR=5.13e-04 WD=6.99e-02) | Time=18.5s [HG UPDATE: dLR=-4.7e+05, dWD=3.7e+03]\n",
      "[Seed 207796 | Ep 75] Train (L=0.9045 A=67.4%) | Val (L=1.0472 A=63.8%) | Hyper (LR=5.13e-04 WD=6.99e-02) | Time=18.3s\n",
      "[Seed 207796 | Ep 76] Train (L=0.8924 A=67.9%) | Val (L=0.9843 A=65.8%) | Hyper (LR=6.15e-04 WD=6.99e-02) | Time=18.3s [HG UPDATE: dLR=-2.0e+05, dWD=-4.1e+03]\n",
      "[Seed 207796 | Ep 77] Train (L=0.9188 A=67.1%) | Val (L=1.0424 A=63.4%) | Hyper (LR=6.15e-04 WD=6.99e-02) | Time=18.6s\n",
      "[Seed 207796 | Ep 78] Train (L=0.9111 A=67.4%) | Val (L=1.0407 A=64.1%) | Hyper (LR=4.92e-04 WD=6.98e-02) | Time=18.7s [HG UPDATE: dLR=3.2e+05, dWD=7.9e+03]\n",
      "[Seed 207796 | Ep 79] Train (L=0.8533 A=69.3%) | Val (L=0.9788 A=65.8%) | Hyper (LR=4.92e-04 WD=6.98e-02) | Time=18.7s\n",
      "[Seed 207796 | Ep 80] Train (L=0.8308 A=70.3%) | Val (L=0.9631 A=66.6%) | Hyper (LR=5.91e-04 WD=6.99e-02) | Time=18.6s [HG UPDATE: dLR=-2.8e+05, dWD=-2.4e+03]\n",
      "[Seed 207796 | Ep 81] Train (L=0.8559 A=69.4%) | Val (L=0.9873 A=66.0%) | Hyper (LR=5.91e-04 WD=6.99e-02) | Time=18.8s\n",
      "[Seed 207796 | Ep 82] Train (L=0.8506 A=69.6%) | Val (L=1.0035 A=65.2%) | Hyper (LR=4.73e-04 WD=6.98e-02) | Time=18.8s [HG UPDATE: dLR=2.1e+05, dWD=4.0e+03]\n",
      "[Seed 207796 | Ep 83] Train (L=0.7839 A=72.1%) | Val (L=0.9852 A=66.1%) | Hyper (LR=4.73e-04 WD=6.98e-02) | Time=18.6s\n",
      "[Seed 207796 | Ep 84] Train (L=0.7622 A=72.8%) | Val (L=0.9535 A=67.7%) | Hyper (LR=5.67e-04 WD=6.98e-02) | Time=19.2s [HG UPDATE: dLR=-6.0e+05, dWD=-1.1e+02]\n",
      "[Seed 207796 | Ep 85] Train (L=0.7919 A=71.6%) | Val (L=0.9409 A=67.6%) | Hyper (LR=5.67e-04 WD=6.98e-02) | Time=18.5s\n",
      "[Seed 207796 | Ep 86] Train (L=0.7794 A=72.2%) | Val (L=0.9669 A=66.4%) | Hyper (LR=6.81e-04 WD=6.98e-02) | Time=18.6s [HG UPDATE: dLR=-1.7e+04, dWD=-6.4e+02]\n",
      "[Seed 207796 | Ep 87] Train (L=0.8086 A=70.9%) | Val (L=0.9566 A=67.3%) | Hyper (LR=6.81e-04 WD=6.98e-02) | Time=18.6s\n",
      "[Seed 207796 | Ep 88] Train (L=0.8016 A=71.1%) | Val (L=0.9538 A=66.9%) | Hyper (LR=8.17e-04 WD=6.98e-02) | Time=18.6s [HG UPDATE: dLR=-1.7e+05, dWD=3.8e+02]\n",
      "[Seed 207796 | Ep 89] Train (L=0.8363 A=70.1%) | Val (L=0.9778 A=66.3%) | Hyper (LR=8.17e-04 WD=6.98e-02) | Time=18.5s\n",
      "--> Completed Seed 207796. Saved checkpoint to 'hypergradient_results.csv'\n",
      "\n",
      "================================================================================\n",
      "STARTING SEED: 45921\n",
      "================================================================================\n",
      "[Seed 45921 | Ep 00] Train (L=1.9794 A=24.0%) | Val (L=1.7845 A=33.1%) | Hyper (LR=2.64e-04 WD=6.99e-02) | Time=18.6s [HG UPDATE: dLR=1.0e+06, dWD=5.2e+03]\n",
      "[Seed 45921 | Ep 01] Train (L=1.7447 A=34.3%) | Val (L=1.6608 A=38.8%) | Hyper (LR=2.64e-04 WD=6.99e-02) | Time=18.5s\n",
      "[Seed 45921 | Ep 02] Train (L=1.6598 A=38.3%) | Val (L=1.6208 A=40.6%) | Hyper (LR=2.11e-04 WD=6.99e-02) | Time=18.8s [HG UPDATE: dLR=1.7e+06, dWD=2.5e+01]\n",
      "[Seed 45921 | Ep 03] Train (L=1.5850 A=41.6%) | Val (L=1.5256 A=44.0%) | Hyper (LR=2.11e-04 WD=6.99e-02) | Time=18.6s\n",
      "[Seed 45921 | Ep 04] Train (L=1.5440 A=43.3%) | Val (L=1.5166 A=44.9%) | Hyper (LR=2.53e-04 WD=7.00e-02) | Time=18.7s [HG UPDATE: dLR=-1.1e+06, dWD=-2.9e+03]\n",
      "[Seed 45921 | Ep 05] Train (L=1.5228 A=44.0%) | Val (L=1.4991 A=45.4%) | Hyper (LR=2.53e-04 WD=7.00e-02) | Time=18.5s\n",
      "[Seed 45921 | Ep 06] Train (L=1.4903 A=45.5%) | Val (L=1.4975 A=46.0%) | Hyper (LR=2.03e-04 WD=7.00e-02) | Time=18.5s [HG UPDATE: dLR=4.4e+05, dWD=2.2e+02]\n",
      "[Seed 45921 | Ep 07] Train (L=1.4498 A=46.9%) | Val (L=1.3732 A=51.0%) | Hyper (LR=2.03e-04 WD=7.00e-02) | Time=18.6s\n",
      "[Seed 45921 | Ep 08] Train (L=1.4240 A=48.3%) | Val (L=1.3905 A=49.5%) | Hyper (LR=1.62e-04 WD=7.00e-02) | Time=18.7s [HG UPDATE: dLR=1.0e+06, dWD=2.2e+03]\n",
      "[Seed 45921 | Ep 09] Train (L=1.3887 A=49.2%) | Val (L=1.3324 A=51.5%) | Hyper (LR=1.62e-04 WD=7.00e-02) | Time=18.4s\n",
      "[Seed 45921 | Ep 10] Train (L=1.3718 A=50.2%) | Val (L=1.3314 A=51.8%) | Hyper (LR=1.30e-04 WD=7.00e-02) | Time=18.5s [HG UPDATE: dLR=2.2e+06, dWD=-3.4e+03]\n",
      "[Seed 45921 | Ep 11] Train (L=1.3435 A=51.3%) | Val (L=1.3226 A=51.9%) | Hyper (LR=1.30e-04 WD=7.00e-02) | Time=18.5s\n",
      "[Seed 45921 | Ep 12] Train (L=1.3235 A=51.9%) | Val (L=1.2979 A=53.1%) | Hyper (LR=1.56e-04 WD=7.00e-02) | Time=18.5s [HG UPDATE: dLR=-3.1e+05, dWD=2.0e+03]\n",
      "[Seed 45921 | Ep 13] Train (L=1.3230 A=52.0%) | Val (L=1.2774 A=53.6%) | Hyper (LR=1.56e-04 WD=7.00e-02) | Time=18.5s\n",
      "[Seed 45921 | Ep 14] Train (L=1.3105 A=52.5%) | Val (L=1.2672 A=54.1%) | Hyper (LR=1.87e-04 WD=7.00e-02) | Time=18.5s [HG UPDATE: dLR=-9.1e+05, dWD=4.7e+02]\n",
      "[Seed 45921 | Ep 15] Train (L=1.3040 A=52.7%) | Val (L=1.2673 A=53.9%) | Hyper (LR=1.87e-04 WD=7.00e-02) | Time=18.6s\n",
      "[Seed 45921 | Ep 16] Train (L=1.2924 A=53.1%) | Val (L=1.2747 A=54.3%) | Hyper (LR=2.24e-04 WD=6.99e-02) | Time=18.8s [HG UPDATE: dLR=-6.2e+05, dWD=2.6e+03]\n",
      "[Seed 45921 | Ep 17] Train (L=1.2944 A=53.0%) | Val (L=1.2644 A=54.1%) | Hyper (LR=2.24e-04 WD=6.99e-02) | Time=18.5s\n",
      "[Seed 45921 | Ep 18] Train (L=1.2769 A=53.7%) | Val (L=1.2646 A=54.8%) | Hyper (LR=2.69e-04 WD=7.00e-02) | Time=18.5s [HG UPDATE: dLR=-6.0e+05, dWD=-2.0e+03]\n",
      "[Seed 45921 | Ep 19] Train (L=1.2886 A=53.3%) | Val (L=1.2867 A=54.2%) | Hyper (LR=2.69e-04 WD=7.00e-02) | Time=18.5s\n",
      "[Seed 45921 | Ep 20] Train (L=1.2733 A=54.0%) | Val (L=1.3061 A=53.0%) | Hyper (LR=2.15e-04 WD=6.99e-02) | Time=18.8s [HG UPDATE: dLR=1.4e+06, dWD=8.9e+02]\n",
      "[Seed 45921 | Ep 21] Train (L=1.2387 A=54.9%) | Val (L=1.2493 A=55.5%) | Hyper (LR=2.15e-04 WD=6.99e-02) | Time=18.5s\n",
      "[Seed 45921 | Ep 22] Train (L=1.2220 A=55.9%) | Val (L=1.1914 A=57.6%) | Hyper (LR=2.58e-04 WD=7.00e-02) | Time=18.6s [HG UPDATE: dLR=-1.4e+06, dWD=-2.4e+03]\n",
      "[Seed 45921 | Ep 23] Train (L=1.2220 A=56.0%) | Val (L=1.2249 A=55.9%) | Hyper (LR=2.58e-04 WD=7.00e-02) | Time=18.5s\n",
      "[Seed 45921 | Ep 24] Train (L=1.2152 A=56.1%) | Val (L=1.2070 A=57.0%) | Hyper (LR=2.07e-04 WD=7.00e-02) | Time=18.7s [HG UPDATE: dLR=1.1e+06, dWD=-5.1e+03]\n",
      "[Seed 45921 | Ep 25] Train (L=1.1809 A=57.4%) | Val (L=1.1854 A=57.3%) | Hyper (LR=2.07e-04 WD=7.00e-02) | Time=18.6s\n",
      "[Seed 45921 | Ep 26] Train (L=1.1613 A=58.1%) | Val (L=1.2057 A=56.7%) | Hyper (LR=2.17e-04 WD=7.00e-02) | Time=18.7s [HG UPDATE: dLR=-1.1e+03, dWD=1.9e+03]\n",
      "[Seed 45921 | Ep 27] Train (L=1.1535 A=58.3%) | Val (L=1.1558 A=58.6%) | Hyper (LR=2.17e-04 WD=7.00e-02) | Time=18.7s\n",
      "[Seed 45921 | Ep 28] Train (L=1.1444 A=58.8%) | Val (L=1.1688 A=58.0%) | Hyper (LR=1.74e-04 WD=7.00e-02) | Time=18.6s [HG UPDATE: dLR=4.7e+05, dWD=-1.4e+03]\n",
      "[Seed 45921 | Ep 29] Train (L=1.1082 A=59.9%) | Val (L=1.1263 A=59.9%) | Hyper (LR=1.74e-04 WD=7.00e-02) | Time=18.6s\n",
      "[Seed 45921 | Ep 30] Train (L=1.0944 A=60.5%) | Val (L=1.1268 A=59.7%) | Hyper (LR=1.39e-04 WD=7.00e-02) | Time=18.7s [HG UPDATE: dLR=5.3e+05, dWD=-1.4e+02]\n",
      "[Seed 45921 | Ep 31] Train (L=1.0585 A=61.8%) | Val (L=1.0911 A=61.2%) | Hyper (LR=1.39e-04 WD=7.00e-02) | Time=18.5s\n",
      "[Seed 45921 | Ep 32] Train (L=1.0453 A=62.3%) | Val (L=1.0578 A=62.1%) | Hyper (LR=1.67e-04 WD=7.00e-02) | Time=18.7s [HG UPDATE: dLR=-3.7e+05, dWD=1.6e+03]\n",
      "[Seed 45921 | Ep 33] Train (L=1.0492 A=62.3%) | Val (L=1.0717 A=61.5%) | Hyper (LR=1.67e-04 WD=7.00e-02) | Time=18.6s\n",
      "[Seed 45921 | Ep 34] Train (L=1.0357 A=62.7%) | Val (L=1.0480 A=63.1%) | Hyper (LR=1.34e-04 WD=7.00e-02) | Time=18.6s [HG UPDATE: dLR=3.4e+05, dWD=1.2e+03]\n",
      "[Seed 45921 | Ep 35] Train (L=1.0048 A=64.0%) | Val (L=1.0394 A=62.8%) | Hyper (LR=1.34e-04 WD=7.00e-02) | Time=18.6s\n",
      "[Seed 45921 | Ep 36] Train (L=0.9981 A=64.1%) | Val (L=1.0271 A=63.3%) | Hyper (LR=1.60e-04 WD=7.00e-02) | Time=18.5s [HG UPDATE: dLR=-7.2e+05, dWD=8.7e+02]\n",
      "[Seed 45921 | Ep 37] Train (L=0.9943 A=64.4%) | Val (L=1.0365 A=63.2%) | Hyper (LR=1.60e-04 WD=7.00e-02) | Time=18.7s\n",
      "[Seed 45921 | Ep 38] Train (L=0.9856 A=64.5%) | Val (L=1.0511 A=63.0%) | Hyper (LR=1.92e-04 WD=7.00e-02) | Time=18.7s [HG UPDATE: dLR=-3.6e+04, dWD=1.3e+03]\n",
      "[Seed 45921 | Ep 39] Train (L=0.9913 A=64.4%) | Val (L=1.0246 A=63.6%) | Hyper (LR=1.92e-04 WD=7.00e-02) | Time=18.6s\n",
      "[Seed 45921 | Ep 40] Train (L=0.9788 A=65.0%) | Val (L=1.0288 A=63.9%) | Hyper (LR=2.31e-04 WD=7.00e-02) | Time=18.5s [HG UPDATE: dLR=-7.4e+05, dWD=-3.5e+03]\n",
      "[Seed 45921 | Ep 41] Train (L=0.9911 A=64.3%) | Val (L=1.0454 A=62.4%) | Hyper (LR=2.31e-04 WD=7.00e-02) | Time=18.7s\n",
      "[Seed 45921 | Ep 42] Train (L=0.9826 A=64.9%) | Val (L=1.0071 A=65.3%) | Hyper (LR=1.85e-04 WD=7.00e-02) | Time=18.7s [HG UPDATE: dLR=9.7e+05, dWD=-4.3e+02]\n",
      "[Seed 45921 | Ep 43] Train (L=0.9400 A=66.0%) | Val (L=0.9757 A=65.7%) | Hyper (LR=1.85e-04 WD=7.00e-02) | Time=18.6s\n",
      "[Seed 45921 | Ep 44] Train (L=0.9308 A=66.7%) | Val (L=1.0043 A=64.9%) | Hyper (LR=2.22e-04 WD=7.00e-02) | Time=18.5s [HG UPDATE: dLR=-4.2e+05, dWD=5.0e+02]\n",
      "[Seed 45921 | Ep 45] Train (L=0.9373 A=66.3%) | Val (L=1.0121 A=64.2%) | Hyper (LR=2.22e-04 WD=7.00e-02) | Time=18.7s\n",
      "[Seed 45921 | Ep 46] Train (L=0.9279 A=66.6%) | Val (L=0.9887 A=65.4%) | Hyper (LR=1.77e-04 WD=7.00e-02) | Time=18.7s [HG UPDATE: dLR=3.1e+05, dWD=2.7e+03]\n",
      "[Seed 45921 | Ep 47] Train (L=0.8865 A=68.1%) | Val (L=0.9653 A=66.3%) | Hyper (LR=1.77e-04 WD=7.00e-02) | Time=18.6s\n",
      "[Seed 45921 | Ep 48] Train (L=0.8697 A=68.9%) | Val (L=0.9562 A=66.8%) | Hyper (LR=1.42e-04 WD=7.00e-02) | Time=18.7s [HG UPDATE: dLR=2.4e+06, dWD=-1.3e+03]\n",
      "[Seed 45921 | Ep 49] Train (L=0.8448 A=69.7%) | Val (L=0.9283 A=67.8%) | Hyper (LR=1.42e-04 WD=7.00e-02) | Time=18.7s\n",
      "[Seed 45921 | Ep 50] Train (L=0.8251 A=70.3%) | Val (L=0.9476 A=67.5%) | Hyper (LR=1.13e-04 WD=7.00e-02) | Time=18.6s [HG UPDATE: dLR=4.4e+05, dWD=-5.4e+02]\n",
      "[Seed 45921 | Ep 51] Train (L=0.7960 A=71.3%) | Val (L=0.9113 A=68.5%) | Hyper (LR=1.13e-04 WD=7.00e-02) | Time=18.6s\n",
      "[Seed 45921 | Ep 52] Train (L=0.7807 A=71.9%) | Val (L=0.9094 A=68.6%) | Hyper (LR=9.08e-05 WD=7.00e-02) | Time=18.7s [HG UPDATE: dLR=4.0e+05, dWD=-2.1e+03]\n",
      "[Seed 45921 | Ep 53] Train (L=0.7547 A=72.7%) | Val (L=0.8930 A=69.3%) | Hyper (LR=9.08e-05 WD=7.00e-02) | Time=18.7s\n",
      "[Seed 45921 | Ep 54] Train (L=0.7441 A=73.4%) | Val (L=0.8855 A=69.7%) | Hyper (LR=7.26e-05 WD=7.00e-02) | Time=18.7s [HG UPDATE: dLR=4.2e+05, dWD=-7.2e+02]\n",
      "[Seed 45921 | Ep 55] Train (L=0.7183 A=74.3%) | Val (L=0.8992 A=68.9%) | Hyper (LR=7.26e-05 WD=7.00e-02) | Time=18.6s\n",
      "[Seed 45921 | Ep 56] Train (L=0.7101 A=74.8%) | Val (L=0.8931 A=69.5%) | Hyper (LR=8.71e-05 WD=7.00e-02) | Time=18.8s [HG UPDATE: dLR=-1.2e+05, dWD=2.1e+02]\n",
      "[Seed 45921 | Ep 57] Train (L=0.7128 A=74.5%) | Val (L=0.8942 A=69.3%) | Hyper (LR=8.71e-05 WD=7.00e-02) | Time=18.5s\n",
      "[Seed 45921 | Ep 58] Train (L=0.7073 A=74.6%) | Val (L=0.8881 A=69.2%) | Hyper (LR=6.97e-05 WD=7.00e-02) | Time=18.6s [HG UPDATE: dLR=6.7e+05, dWD=-5.7e+02]\n",
      "[Seed 45921 | Ep 59] Train (L=0.6821 A=75.4%) | Val (L=0.8904 A=69.7%) | Hyper (LR=6.97e-05 WD=7.00e-02) | Time=18.6s\n",
      "[Seed 45921 | Ep 60] Train (L=0.6768 A=75.6%) | Val (L=0.8973 A=69.5%) | Hyper (LR=8.36e-05 WD=7.00e-02) | Time=18.5s [HG UPDATE: dLR=-2.1e+06, dWD=2.8e+02]\n",
      "[Seed 45921 | Ep 61] Train (L=0.6779 A=75.6%) | Val (L=0.9074 A=69.5%) | Hyper (LR=8.36e-05 WD=7.00e-02) | Time=18.9s\n",
      "[Seed 45921 | Ep 62] Train (L=0.6732 A=75.9%) | Val (L=0.8997 A=70.0%) | Hyper (LR=6.69e-05 WD=7.00e-02) | Time=18.6s [HG UPDATE: dLR=2.5e+04, dWD=-1.5e+03]\n",
      "[Seed 45921 | Ep 63] Train (L=0.6469 A=76.6%) | Val (L=0.8939 A=70.1%) | Hyper (LR=6.69e-05 WD=7.00e-02) | Time=18.6s\n",
      "[Seed 45921 | Ep 64] Train (L=0.6398 A=77.0%) | Val (L=0.8980 A=70.2%) | Hyper (LR=8.03e-05 WD=7.01e-02) | Time=18.8s [HG UPDATE: dLR=-3.9e+05, dWD=-2.5e+03]\n",
      "[Seed 45921 | Ep 65] Train (L=0.6436 A=76.9%) | Val (L=0.9091 A=69.5%) | Hyper (LR=8.03e-05 WD=7.01e-02) | Time=18.7s\n",
      "[Seed 45921 | Ep 66] Train (L=0.6422 A=76.9%) | Val (L=0.8997 A=69.6%) | Hyper (LR=9.64e-05 WD=7.00e-02) | Time=18.5s [HG UPDATE: dLR=-3.5e+05, dWD=1.8e+03]\n",
      "[Seed 45921 | Ep 67] Train (L=0.6531 A=76.5%) | Val (L=0.8899 A=70.1%) | Hyper (LR=9.64e-05 WD=7.00e-02) | Time=18.5s\n",
      "[Seed 45921 | Ep 68] Train (L=0.6378 A=77.0%) | Val (L=0.9195 A=70.2%) | Hyper (LR=1.16e-04 WD=7.01e-02) | Time=18.6s [HG UPDATE: dLR=-1.8e+06, dWD=-1.5e+03]\n",
      "[Seed 45921 | Ep 69] Train (L=0.6518 A=76.6%) | Val (L=0.9302 A=69.3%) | Hyper (LR=1.16e-04 WD=7.01e-02) | Time=18.7s\n",
      "[Seed 45921 | Ep 70] Train (L=0.6456 A=76.9%) | Val (L=0.9083 A=70.1%) | Hyper (LR=1.39e-04 WD=7.00e-02) | Time=18.7s [HG UPDATE: dLR=-1.5e+05, dWD=1.0e+03]\n",
      "[Seed 45921 | Ep 71] Train (L=0.6616 A=76.4%) | Val (L=0.9221 A=69.8%) | Hyper (LR=1.39e-04 WD=7.00e-02) | Time=18.6s\n",
      "[Seed 45921 | Ep 72] Train (L=0.6510 A=76.5%) | Val (L=0.9255 A=69.7%) | Hyper (LR=1.11e-04 WD=7.00e-02) | Time=18.9s [HG UPDATE: dLR=6.9e+05, dWD=7.0e+02]\n",
      "[Seed 45921 | Ep 73] Train (L=0.6252 A=77.5%) | Val (L=0.9070 A=70.0%) | Hyper (LR=1.11e-04 WD=7.00e-02) | Time=18.5s\n",
      "[Seed 45921 | Ep 74] Train (L=0.6178 A=77.8%) | Val (L=0.9048 A=70.2%) | Hyper (LR=8.88e-05 WD=7.00e-02) | Time=18.6s [HG UPDATE: dLR=7.4e+05, dWD=4.9e+01]\n",
      "[Seed 45921 | Ep 75] Train (L=0.5844 A=79.1%) | Val (L=0.9214 A=70.4%) | Hyper (LR=8.88e-05 WD=7.00e-02) | Time=18.5s\n",
      "[Seed 45921 | Ep 76] Train (L=0.5805 A=79.0%) | Val (L=0.8900 A=70.6%) | Hyper (LR=7.10e-05 WD=7.01e-02) | Time=18.7s [HG UPDATE: dLR=4.6e+05, dWD=-1.1e+03]\n",
      "[Seed 45921 | Ep 77] Train (L=0.5542 A=80.0%) | Val (L=0.8996 A=71.5%) | Hyper (LR=7.10e-05 WD=7.01e-02) | Time=18.5s\n",
      "[Seed 45921 | Ep 78] Train (L=0.5394 A=80.5%) | Val (L=0.9118 A=70.8%) | Hyper (LR=5.68e-05 WD=7.01e-02) | Time=18.7s [HG UPDATE: dLR=5.4e+05, dWD=-1.0e+03]\n",
      "[Seed 45921 | Ep 79] Train (L=0.5239 A=81.2%) | Val (L=0.9215 A=71.4%) | Hyper (LR=5.68e-05 WD=7.01e-02) | Time=18.7s\n",
      "[Seed 45921 | Ep 80] Train (L=0.5172 A=81.4%) | Val (L=0.9252 A=71.1%) | Hyper (LR=6.82e-05 WD=7.00e-02) | Time=18.9s [HG UPDATE: dLR=-1.3e+06, dWD=1.6e+03]\n",
      "[Seed 45921 | Ep 81] Train (L=0.5255 A=81.0%) | Val (L=0.9143 A=70.6%) | Hyper (LR=6.82e-05 WD=7.00e-02) | Time=18.8s\n",
      "[Seed 45921 | Ep 82] Train (L=0.5210 A=81.3%) | Val (L=0.9145 A=71.3%) | Hyper (LR=8.18e-05 WD=7.00e-02) | Time=18.6s [HG UPDATE: dLR=-2.2e+05, dWD=1.1e+03]\n",
      "[Seed 45921 | Ep 83] Train (L=0.5282 A=80.9%) | Val (L=0.9226 A=71.0%) | Hyper (LR=8.18e-05 WD=7.00e-02) | Time=18.6s\n",
      "[Seed 45921 | Ep 84] Train (L=0.5233 A=81.3%) | Val (L=0.9117 A=70.9%) | Hyper (LR=6.55e-05 WD=7.00e-02) | Time=18.5s [HG UPDATE: dLR=8.4e+05, dWD=9.3e+02]\n",
      "[Seed 45921 | Ep 85] Train (L=0.4953 A=82.2%) | Val (L=0.9349 A=71.4%) | Hyper (LR=6.55e-05 WD=7.00e-02) | Time=18.6s\n",
      "[Seed 45921 | Ep 86] Train (L=0.4959 A=82.3%) | Val (L=0.9077 A=72.3%) | Hyper (LR=5.24e-05 WD=7.00e-02) | Time=18.5s [HG UPDATE: dLR=1.5e+06, dWD=-4.9e+01]\n",
      "[Seed 45921 | Ep 87] Train (L=0.4720 A=83.0%) | Val (L=0.9321 A=71.5%) | Hyper (LR=5.24e-05 WD=7.00e-02) | Time=18.7s\n",
      "[Seed 45921 | Ep 88] Train (L=0.4617 A=83.3%) | Val (L=0.9305 A=71.8%) | Hyper (LR=6.29e-05 WD=7.00e-02) | Time=18.5s [HG UPDATE: dLR=-1.3e+06, dWD=-9.3e+02]\n",
      "[Seed 45921 | Ep 89] Train (L=0.4725 A=83.0%) | Val (L=0.9351 A=71.5%) | Hyper (LR=6.29e-05 WD=7.00e-02) | Time=18.6s\n",
      "--> Completed Seed 45921. Saved checkpoint to 'hypergradient_results.csv'\n",
      "\n",
      "================================================================================\n",
      "ALL SEEDS COMPLETED SUCCESSFULLY.\n",
      "Final data saved to 'hypergradient_results.csv'\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import pandas as pd  # Required for saving results\n",
    "\n",
    "# ==========================================\n",
    "# 1. SETUP & CONFIGURATION\n",
    "# ==========================================\n",
    "# The specific seeds requested by your partner\n",
    "SEEDS = [38042, 217401, 637451, 207796, 45921]\n",
    "\n",
    "# Training duration matching your paper (90 epochs)\n",
    "EPOCHS = 90\n",
    "HYPER_INTERVAL = 2  # How often HGD updates (epochs)\n",
    "\n",
    "# Master list to store every single data point for CSV\n",
    "full_data_log = []\n",
    "\n",
    "# ==========================================\n",
    "# 2. HELPER FUNCTIONS\n",
    "# ==========================================\n",
    "def set_seed(seed):\n",
    "    \"\"\"Sets all random seeds for deterministic reproducibility.\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_metrics(model, loader):\n",
    "    \"\"\"Calculates Loss and Accuracy on a given loader.\"\"\"\n",
    "    model.eval()\n",
    "    total_loss, total, correct = 0.0, 0, 0\n",
    "    for x, y in loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        logits = model(x)\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "        total_loss += loss.item() * x.size(0)\n",
    "        total += x.size(0)\n",
    "        \n",
    "        # Calculate Accuracy\n",
    "        _, preds = logits.max(1)\n",
    "        correct += preds.eq(y).sum().item()\n",
    "        \n",
    "    return total_loss / total, correct / total\n",
    "\n",
    "# ==========================================\n",
    "# 3. MAIN EXPERIMENT LOOP\n",
    "# ==========================================\n",
    "print(f\"Starting Multi-Seed Experiment.\")\n",
    "print(f\"Seeds: {SEEDS}\")\n",
    "print(f\"Config: {EPOCHS} Epochs per seed | ViT-Small (Patch=8, Dim=192)\")\n",
    "print(f\"Estimated duration: ~3.5 to 4 hours\") \n",
    "\n",
    "for seed in SEEDS:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"STARTING SEED: {seed}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # 1. Reset Environment\n",
    "    set_seed(seed)\n",
    "    \n",
    "    # 2. Initialize Model (Partner's Config: Small/Fast)\n",
    "    model = ViTSmall(\n",
    "        image_size=32,      \n",
    "        patch_size=8,       # 16 tokens (Fast)\n",
    "        embed_dim=192,      # 192 dim (Lightweight)\n",
    "        depth=12,           \n",
    "        num_heads=3,        \n",
    "        mlp_ratio=4.0,      \n",
    "        dropout=0.1         \n",
    "    ).to(device)\n",
    "\n",
    "    # 3. Initialize HyperGradient Wrapper\n",
    "    hg = HyperGradient(\n",
    "        model,\n",
    "        train_loader,\n",
    "        val_loader,\n",
    "        lr=3.3e-4,      # Baseline start\n",
    "        wd=0.07,        # Baseline start\n",
    "        hyper_lr=1e-8,  # Safe step size for hyper-updates\n",
    "    )\n",
    "\n",
    "    # 4. Initialize Optimizer\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=hg.lr,\n",
    "        weight_decay=hg.wd\n",
    "    )\n",
    "\n",
    "    # 5. Training Loop for this Seed\n",
    "    for epoch in range(EPOCHS):\n",
    "        t0 = time.time()\n",
    "        model.train()\n",
    "\n",
    "        running_loss = 0.0\n",
    "        running_correct = 0\n",
    "        seen = 0\n",
    "\n",
    "        # --- Batch Loop ---\n",
    "        for batch_idx, (x, y) in enumerate(train_loader):\n",
    "            x, y = x.to(device), y.to(device)\n",
    "\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            logits = model(x)\n",
    "            loss = F.cross_entropy(logits, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Track Train Stats\n",
    "            running_loss += loss.item() * x.size(0)\n",
    "            _, preds = logits.max(1)\n",
    "            running_correct += preds.eq(y).sum().item()\n",
    "            seen += x.size(0)\n",
    "\n",
    "        # Calculate Epoch Stats\n",
    "        epoch_train_loss = running_loss / seen\n",
    "        epoch_train_acc = running_correct / seen\n",
    "        epoch_time = time.time() - t0\n",
    "\n",
    "        # --- Hypergradient Step ---\n",
    "        hg_update_msg = \"\"\n",
    "        if epoch % HYPER_INTERVAL == 0:\n",
    "            # Run the probe and update LR/WD\n",
    "            grads = hg.step()\n",
    "            \n",
    "            # Apply new values to optimizer\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] = hg.lr\n",
    "                param_group['weight_decay'] = hg.wd\n",
    "            \n",
    "            hg_update_msg = f\" [HG UPDATE: dLR={grads['lr']:.1e}, dWD={grads['wd']:.1e}]\"\n",
    "\n",
    "        # --- Validation Step ---\n",
    "        val_loss, val_acc = evaluate_metrics(model, val_loader)\n",
    "\n",
    "        # --- Logging Data ---\n",
    "        # 1. Print to Console (The \"Training Log\")\n",
    "        print(\n",
    "            f\"[Seed {seed} | Ep {epoch:02d}] \"\n",
    "            f\"Train (L={epoch_train_loss:.4f} A={epoch_train_acc:.1%}) | \"\n",
    "            f\"Val (L={val_loss:.4f} A={val_acc:.1%}) | \"\n",
    "            f\"Hyper (LR={hg.lr:.2e} WD={hg.wd:.2e}) | \"\n",
    "            f\"Time={epoch_time:.1f}s{hg_update_msg}\"\n",
    "        )\n",
    "\n",
    "        # 2. Save to Master List\n",
    "        log_entry = {\n",
    "            'Seed': seed,\n",
    "            'Epoch': epoch,\n",
    "            'Train_Loss': epoch_train_loss,\n",
    "            'Train_Acc': epoch_train_acc,\n",
    "            'Val_Loss': val_loss,\n",
    "            'Val_Acc': val_acc,\n",
    "            'LR': hg.lr,\n",
    "            'WD': hg.wd,\n",
    "            'Time_Sec': epoch_time\n",
    "        }\n",
    "        full_data_log.append(log_entry)\n",
    "\n",
    "    # --- End of Seed Safety Save ---\n",
    "    # Save after every seed so data is safe if Colab crashes\n",
    "    current_df = pd.DataFrame(full_data_log)\n",
    "    current_df.to_csv('hypergradient_results.csv', index=False)\n",
    "    print(f\"--> Completed Seed {seed}. Saved checkpoint to 'hypergradient_results.csv'\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ALL SEEDS COMPLETED SUCCESSFULLY.\")\n",
    "print(\"Final data saved to 'hypergradient_results.csv'\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "de588c82",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-04T12:13:37.459918Z",
     "iopub.status.busy": "2025-12-04T12:13:37.459269Z",
     "iopub.status.idle": "2025-12-04T12:13:37.741427Z",
     "shell.execute_reply": "2025-12-04T12:13:37.740339Z"
    },
    "papermill": {
     "duration": 0.3031,
     "end_time": "2025-12-04T12:13:37.742663",
     "exception": true,
     "start_time": "2025-12-04T12:13:37.439563",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'all_results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_20/1538347321.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mseed\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mSEEDS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;31m# Extract data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0ms_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mall_results\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0ms_lr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mall_results\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'all_results' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1200x700 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# 1. Setup the Plot\n",
    "fig, ax1 = plt.subplots(figsize=(12, 7))\n",
    "ax2 = ax1.twinx() # Right axis for Learning Rate\n",
    "\n",
    "# Colors\n",
    "color_loss = 'tab:red'\n",
    "color_lr = 'tab:blue'\n",
    "\n",
    "# 2. Iterate through every seed to plot individual \"faint\" lines\n",
    "# This proves to your partner that every seed worked, not just the average\n",
    "for seed in SEEDS:\n",
    "    # Extract data\n",
    "    s_val = all_results[seed]['val_loss']\n",
    "    s_lr = all_results[seed]['lr']\n",
    "    epochs = range(len(s_val))\n",
    "    \n",
    "    # Plot faint lines (alpha=0.2)\n",
    "    ax1.plot(epochs, s_val, color=color_loss, alpha=0.15, linewidth=1)\n",
    "    ax2.step(epochs, s_lr, where='post', color=color_lr, alpha=0.15, linewidth=1, linestyle='--')\n",
    "\n",
    "# 3. Calculate and Plot the MEAN (Bold Lines)\n",
    "val_losses_matrix = np.array([all_results[s]['val_loss'] for s in SEEDS])\n",
    "lr_matrix = np.array([all_results[s]['lr'] for s in SEEDS])\n",
    "\n",
    "mean_val = np.mean(val_losses_matrix, axis=0)\n",
    "mean_lr = np.mean(lr_matrix, axis=0)\n",
    "\n",
    "# Plot Mean Loss (Thick Red Line)\n",
    "ax1.plot(epochs, mean_val, color=color_loss, linewidth=2.5, label=f'Mean Val Loss (N={len(SEEDS)})')\n",
    "\n",
    "# Plot Mean LR (Thick Blue Dashed Line)\n",
    "# Note: We use the mean LR, though ideally they should be identical if deterministic\n",
    "ax2.step(epochs, mean_lr, where='post', color=color_lr, linewidth=2.5, linestyle='--', label='Mean Adaptive LR')\n",
    "\n",
    "# 4. Styling\n",
    "ax1.set_xlabel('Epoch', fontsize=12)\n",
    "ax1.set_ylabel('Validation Loss', color=color_loss, fontsize=12)\n",
    "ax2.set_ylabel('Learning Rate', color=color_lr, fontsize=12)\n",
    "ax1.tick_params(axis='y', labelcolor=color_loss)\n",
    "ax2.tick_params(axis='y', labelcolor=color_lr)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Combined Legend\n",
    "lines1, labels1 = ax1.get_legend_handles_labels()\n",
    "lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "ax1.legend(lines1 + lines2, labels1 + labels2, loc='upper center', bbox_to_anchor=(0.5, 1.1), ncol=2)\n",
    "\n",
    "plt.title(f'Hypergradient Dynamics: Robustness Across {len(SEEDS)} Random Seeds', fontsize=14, pad=50)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save\n",
    "plt.savefig('hgd_robustness_spaghetti.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f902236",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 8911589,
     "sourceId": 13979389,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 10136.310565,
   "end_time": "2025-12-04T12:13:39.383252",
   "environment_variables": {},
   "exception": true,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-12-04T09:24:43.072687",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
