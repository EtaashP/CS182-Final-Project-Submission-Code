{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2fd6efff",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-12-05T07:57:24.102298Z",
     "iopub.status.busy": "2025-12-05T07:57:24.102033Z",
     "iopub.status.idle": "2025-12-05T07:57:25.645038Z",
     "shell.execute_reply": "2025-12-05T07:57:25.644191Z"
    },
    "papermill": {
     "duration": 1.548504,
     "end_time": "2025-12-05T07:57:25.646346",
     "exception": false,
     "start_time": "2025-12-05T07:57:24.097842",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/cifar-10-dataset/cifar-10-batches-py/data_batch_1\n",
      "/kaggle/input/cifar-10-dataset/cifar-10-batches-py/data_batch_2\n",
      "/kaggle/input/cifar-10-dataset/cifar-10-batches-py/batches.meta\n",
      "/kaggle/input/cifar-10-dataset/cifar-10-batches-py/test_batch\n",
      "/kaggle/input/cifar-10-dataset/cifar-10-batches-py/data_batch_3\n",
      "/kaggle/input/cifar-10-dataset/cifar-10-batches-py/data_batch_5\n",
      "/kaggle/input/cifar-10-dataset/cifar-10-batches-py/data_batch_4\n",
      "/kaggle/input/cifar-10-dataset/cifar-10-batches-py/readme.html\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4efecb2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-05T07:57:25.652884Z",
     "iopub.status.busy": "2025-12-05T07:57:25.652501Z",
     "iopub.status.idle": "2025-12-05T07:57:34.743997Z",
     "shell.execute_reply": "2025-12-05T07:57:34.743080Z"
    },
    "papermill": {
     "duration": 9.096299,
     "end_time": "2025-12-05T07:57:34.745325",
     "exception": false,
     "start_time": "2025-12-05T07:57:25.649026",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5a9b4bb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-05T07:57:34.751336Z",
     "iopub.status.busy": "2025-12-05T07:57:34.750936Z",
     "iopub.status.idle": "2025-12-05T07:57:38.418451Z",
     "shell.execute_reply": "2025-12-05T07:57:38.417816Z"
    },
    "papermill": {
     "duration": 3.672099,
     "end_time": "2025-12-05T07:57:38.419890",
     "exception": false,
     "start_time": "2025-12-05T07:57:34.747791",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "DATA_ROOT = \"/kaggle/input/cifar-10-dataset\"\n",
    "\n",
    "transform_train = T.Compose([\n",
    "    T.RandomCrop(32, padding=4),\n",
    "    T.RandomHorizontalFlip(),\n",
    "    T.ToTensor(),\n",
    "])\n",
    "\n",
    "transform_val = T.Compose([\n",
    "    T.ToTensor(),\n",
    "])\n",
    "\n",
    "train_dataset = torchvision.datasets.CIFAR10(\n",
    "    root=DATA_ROOT,\n",
    "    train=True,\n",
    "    download=False,   # IMPORTANT: Kaggle already has the data\n",
    "    transform=transform_train,\n",
    ")\n",
    "\n",
    "val_dataset = torchvision.datasets.CIFAR10(\n",
    "    root=DATA_ROOT,\n",
    "    train=False,\n",
    "    download=False,\n",
    "    transform=transform_val,\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=128,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=256,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    pin_memory=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f616d231",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-05T07:57:38.425630Z",
     "iopub.status.busy": "2025-12-05T07:57:38.425356Z",
     "iopub.status.idle": "2025-12-05T07:57:38.430754Z",
     "shell.execute_reply": "2025-12-05T07:57:38.430141Z"
    },
    "papermill": {
     "duration": 0.00946,
     "end_time": "2025-12-05T07:57:38.431861",
     "exception": false,
     "start_time": "2025-12-05T07:57:38.422401",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, dim, num_heads, mlp_ratio=4.0, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(dim)\n",
    "        self.attn = nn.MultiheadAttention(\n",
    "            embed_dim=dim,\n",
    "            num_heads=num_heads,\n",
    "            dropout=dropout,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.norm2 = nn.LayerNorm(dim)\n",
    "\n",
    "        hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, dim),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        attn_out, _ = self.attn(self.norm1(x), self.norm1(x), self.norm1(x), need_weights=False)\n",
    "        x = x + attn_out\n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7cd45542",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-05T07:57:38.437394Z",
     "iopub.status.busy": "2025-12-05T07:57:38.436777Z",
     "iopub.status.idle": "2025-12-05T07:57:38.440462Z",
     "shell.execute_reply": "2025-12-05T07:57:38.439930Z"
    },
    "papermill": {
     "duration": 0.007575,
     "end_time": "2025-12-05T07:57:38.441562",
     "exception": false,
     "start_time": "2025-12-05T07:57:38.433987",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000 10000\n"
     ]
    }
   ],
   "source": [
    "#sanity check, should output 50000 10000\n",
    "print(len(train_dataset), len(val_dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8e0ca7b1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-05T07:57:38.446929Z",
     "iopub.status.busy": "2025-12-05T07:57:38.446512Z",
     "iopub.status.idle": "2025-12-05T07:57:38.453504Z",
     "shell.execute_reply": "2025-12-05T07:57:38.452995Z"
    },
    "papermill": {
     "duration": 0.010813,
     "end_time": "2025-12-05T07:57:38.454530",
     "exception": false,
     "start_time": "2025-12-05T07:57:38.443717",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ViTSmall(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        image_size=32,\n",
    "        patch_size=4,\n",
    "        in_channels=3,\n",
    "        num_classes=10,\n",
    "        embed_dim=384,\n",
    "        depth=12,\n",
    "        num_heads=6,\n",
    "        mlp_ratio=4.0,\n",
    "        dropout=0.1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        num_patches = (image_size // patch_size) ** 2\n",
    "\n",
    "        self.patch_embed = nn.Conv2d(\n",
    "            in_channels, embed_dim,\n",
    "            kernel_size=patch_size,\n",
    "            stride=patch_size\n",
    "        )\n",
    "\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        self.pos_embed = nn.Parameter(\n",
    "            torch.zeros(1, num_patches + 1, embed_dim)\n",
    "        )\n",
    "        self.pos_drop = nn.Dropout(dropout)\n",
    "\n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerBlock(\n",
    "                embed_dim, num_heads, mlp_ratio, dropout\n",
    "            )\n",
    "            for _ in range(depth)\n",
    "        ])\n",
    "\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        self.head = nn.Linear(embed_dim, num_classes)\n",
    "\n",
    "        nn.init.trunc_normal_(self.pos_embed, std=0.02)\n",
    "        nn.init.trunc_normal_(self.cls_token, std=0.02)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B = x.size(0)\n",
    "        x = self.patch_embed(x)\n",
    "        x = x.flatten(2).transpose(1, 2)\n",
    "\n",
    "        cls = self.cls_token.expand(B, -1, -1)\n",
    "        x = torch.cat((cls, x), dim=1)\n",
    "        x = self.pos_drop(x + self.pos_embed)\n",
    "\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x)\n",
    "\n",
    "        x = self.norm(x)\n",
    "        return self.head(x[:, 0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e576450d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-05T07:57:38.459955Z",
     "iopub.status.busy": "2025-12-05T07:57:38.459784Z",
     "iopub.status.idle": "2025-12-05T07:57:38.465133Z",
     "shell.execute_reply": "2025-12-05T07:57:38.464443Z"
    },
    "papermill": {
     "duration": 0.009363,
     "end_time": "2025-12-05T07:57:38.466117",
     "exception": false,
     "start_time": "2025-12-05T07:57:38.456754",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    total_loss, total = 0.0, 0\n",
    "    for x, y in loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        logits = model(x)\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "        total_loss += loss.item() * x.size(0)\n",
    "        total += x.size(0)\n",
    "    return total_loss / total\n",
    "\n",
    "\n",
    "def train_n_batches(model, optimizer, loader, n_batches):\n",
    "    model.train()\n",
    "    it = iter(loader)\n",
    "    for _ in range(n_batches):\n",
    "        try:\n",
    "            x, y = next(it)\n",
    "        except StopIteration:\n",
    "            it = iter(loader)\n",
    "            x, y = next(it)\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        loss = F.cross_entropy(model(x), y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b3b16fe1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-05T07:57:38.471636Z",
     "iopub.status.busy": "2025-12-05T07:57:38.471097Z",
     "iopub.status.idle": "2025-12-05T07:57:38.480378Z",
     "shell.execute_reply": "2025-12-05T07:57:38.479875Z"
    },
    "papermill": {
     "duration": 0.013158,
     "end_time": "2025-12-05T07:57:38.481412",
     "exception": false,
     "start_time": "2025-12-05T07:57:38.468254",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class HyperGradient:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model,\n",
    "        train_loader,\n",
    "        val_loader,\n",
    "        lr=3.3e-4,\n",
    "        wd=0.07,\n",
    "        delta=1e-4,\n",
    "        hyper_lr=1e-6,  # CHANGED: Reduced default from 1e-2 to 1e-6\n",
    "        probe_batches=5\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "\n",
    "        self.lr = lr\n",
    "        self.wd = wd\n",
    "        self.delta = delta\n",
    "        self.hyper_lr = hyper_lr\n",
    "        self.probe_batches = probe_batches\n",
    "\n",
    "    def _make_opt(self, model, lr, wd):\n",
    "        return torch.optim.AdamW(\n",
    "            model.parameters(), lr=lr, weight_decay=wd\n",
    "        )\n",
    "\n",
    "    # NEW: Helper to evaluate faster\n",
    "    @torch.no_grad()\n",
    "    def evaluate_subset(self, model, loader, limit_batches=20):\n",
    "        model.eval()\n",
    "        total_loss, total = 0.0, 0\n",
    "        for i, (x, y) in enumerate(loader):\n",
    "            if i >= limit_batches: break\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            loss = F.cross_entropy(model(x), y)\n",
    "            total_loss += loss.item() * x.size(0)\n",
    "            total += x.size(0)\n",
    "        return total_loss / total\n",
    "\n",
    "    def step(self):\n",
    "        grads = {}\n",
    "\n",
    "        for name in [\"lr\", \"wd\"]:\n",
    "            base = getattr(self, name)\n",
    "            # Ensure epsilon isn't too tiny\n",
    "            eps = self.delta * max(base, 1e-8)\n",
    "\n",
    "            losses = {}\n",
    "            for sign in [+1, -1]:\n",
    "                val = max(base + sign * eps, 1e-8)\n",
    "                clone = copy.deepcopy(self.model)\n",
    "                opt = self._make_opt(\n",
    "                    clone,\n",
    "                    lr=val if name == \"lr\" else self.lr,\n",
    "                    wd=val if name == \"wd\" else self.wd,\n",
    "                )\n",
    "\n",
    "                train_n_batches(\n",
    "                    clone, opt, self.train_loader, self.probe_batches\n",
    "                )\n",
    "                # CHANGED: Use fast subset evaluation\n",
    "                losses[sign] = self.evaluate_subset(clone, self.val_loader, limit_batches=20)\n",
    "\n",
    "            grads[name] = (losses[+1] - losses[-1]) / (2 * eps)\n",
    "\n",
    "        # CHANGED: Robust Update Logic with Clipping\n",
    "        # 1. Update LR with safety clip\n",
    "        lr_grad = grads[\"lr\"]\n",
    "        lr_update = self.hyper_lr * lr_grad\n",
    "        # Cap change to max 20% of current value to prevent explosion\n",
    "        max_lr_change = 0.20 * self.lr\n",
    "        if abs(lr_update) > max_lr_change:\n",
    "            lr_update = max_lr_change if lr_update > 0 else -max_lr_change\n",
    "        \n",
    "        self.lr -= lr_update\n",
    "\n",
    "        # 2. Update WD with safety clip\n",
    "        wd_grad = grads[\"wd\"]\n",
    "        wd_update = self.hyper_lr * wd_grad\n",
    "        max_wd_change = 0.20 * self.wd\n",
    "        if abs(wd_update) > max_wd_change:\n",
    "            wd_update = max_wd_change if wd_update > 0 else -max_wd_change\n",
    "            \n",
    "        self.wd -= wd_update\n",
    "\n",
    "        # 3. Final Clamps\n",
    "        self.lr = float(torch.clamp(torch.tensor(self.lr), 1e-6, 1e-2))\n",
    "        self.wd = float(torch.clamp(torch.tensor(self.wd), 1e-6, 1.0))\n",
    "\n",
    "        return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "367cb2f2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-05T07:57:38.486780Z",
     "iopub.status.busy": "2025-12-05T07:57:38.486572Z",
     "iopub.status.idle": "2025-12-05T11:22:45.829197Z",
     "shell.execute_reply": "2025-12-05T11:22:45.828384Z"
    },
    "papermill": {
     "duration": 12307.351177,
     "end_time": "2025-12-05T11:22:45.834793",
     "exception": false,
     "start_time": "2025-12-05T07:57:38.483616",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Ablation Experiment.\n",
      "Comparing HyperLRs: [0.0, 1e-10, 1e-08, 1e-06]\n",
      "\n",
      "######################################################################\n",
      "PROCESSING ABLATION GROUP: hyper_lr = 0.0\n",
      "######################################################################\n",
      "--> Running Seed 38042...\n",
      "   Ep 0 | Val Acc: 30.4% | LR: 3.30e-04\n",
      "   Ep 10 | Val Acc: 49.9% | LR: 3.30e-04\n",
      "   Ep 20 | Val Acc: 54.6% | LR: 3.30e-04\n",
      "   Ep 30 | Val Acc: 59.1% | LR: 3.30e-04\n",
      "   Ep 40 | Val Acc: 63.5% | LR: 3.30e-04\n",
      "   Ep 50 | Val Acc: 68.0% | LR: 3.30e-04\n",
      "   Ep 60 | Val Acc: 69.4% | LR: 3.30e-04\n",
      "--> Running Seed 217401...\n",
      "   Ep 0 | Val Acc: 36.6% | LR: 3.30e-04\n",
      "   Ep 10 | Val Acc: 50.6% | LR: 3.30e-04\n",
      "   Ep 20 | Val Acc: 55.3% | LR: 3.30e-04\n",
      "   Ep 30 | Val Acc: 60.0% | LR: 3.30e-04\n",
      "   Ep 40 | Val Acc: 64.4% | LR: 3.30e-04\n",
      "   Ep 50 | Val Acc: 67.7% | LR: 3.30e-04\n",
      "   Ep 60 | Val Acc: 69.0% | LR: 3.30e-04\n",
      "\n",
      "######################################################################\n",
      "PROCESSING ABLATION GROUP: hyper_lr = 1e-10\n",
      "######################################################################\n",
      "--> Running Seed 38042...\n",
      "   Ep 0 | Val Acc: 30.4% | LR: 2.64e-04\n",
      "   Ep 10 | Val Acc: 50.1% | LR: 1.42e-04\n",
      "   Ep 20 | Val Acc: 54.8% | LR: 1.52e-04\n",
      "   Ep 30 | Val Acc: 55.6% | LR: 1.68e-04\n",
      "   Ep 40 | Val Acc: 59.6% | LR: 2.31e-04\n",
      "   Ep 50 | Val Acc: 63.7% | LR: 4.95e-04\n",
      "   Ep 60 | Val Acc: 68.5% | LR: 3.47e-04\n",
      "--> Running Seed 217401...\n",
      "   Ep 0 | Val Acc: 36.6% | LR: 3.71e-04\n",
      "   Ep 10 | Val Acc: 50.1% | LR: 4.55e-04\n",
      "   Ep 20 | Val Acc: 53.8% | LR: 4.47e-04\n",
      "   Ep 30 | Val Acc: 59.6% | LR: 4.45e-04\n",
      "   Ep 40 | Val Acc: 64.9% | LR: 3.41e-04\n",
      "   Ep 50 | Val Acc: 67.6% | LR: 3.53e-04\n",
      "   Ep 60 | Val Acc: 69.2% | LR: 3.63e-04\n",
      "\n",
      "######################################################################\n",
      "PROCESSING ABLATION GROUP: hyper_lr = 1e-08\n",
      "######################################################################\n",
      "--> Running Seed 38042...\n",
      "   Ep 0 | Val Acc: 30.4% | LR: 2.64e-04\n",
      "   Ep 10 | Val Acc: 49.8% | LR: 1.30e-04\n",
      "   Ep 20 | Val Acc: 53.8% | LR: 9.57e-05\n",
      "   Ep 30 | Val Acc: 56.1% | LR: 1.06e-04\n",
      "   Ep 40 | Val Acc: 57.7% | LR: 1.17e-04\n",
      "   Ep 50 | Val Acc: 61.5% | LR: 2.91e-04\n",
      "   Ep 60 | Val Acc: 67.0% | LR: 9.54e-05\n",
      "--> Running Seed 217401...\n",
      "   Ep 0 | Val Acc: 36.6% | LR: 3.96e-04\n",
      "   Ep 10 | Val Acc: 50.3% | LR: 4.38e-04\n",
      "   Ep 20 | Val Acc: 52.6% | LR: 7.26e-04\n",
      "   Ep 30 | Val Acc: 58.8% | LR: 5.36e-04\n",
      "   Ep 40 | Val Acc: 65.2% | LR: 5.28e-04\n",
      "   Ep 50 | Val Acc: 68.2% | LR: 5.84e-04\n",
      "   Ep 60 | Val Acc: 71.0% | LR: 4.31e-04\n",
      "\n",
      "######################################################################\n",
      "PROCESSING ABLATION GROUP: hyper_lr = 1e-06\n",
      "######################################################################\n",
      "--> Running Seed 38042...\n",
      "   Ep 0 | Val Acc: 30.4% | LR: 2.64e-04\n",
      "   Ep 10 | Val Acc: 50.2% | LR: 1.30e-04\n",
      "   Ep 20 | Val Acc: 53.9% | LR: 1.44e-04\n",
      "   Ep 30 | Val Acc: 54.7% | LR: 2.38e-04\n",
      "   Ep 40 | Val Acc: 60.2% | LR: 2.63e-04\n",
      "   Ep 50 | Val Acc: 64.7% | LR: 4.37e-04\n",
      "   Ep 60 | Val Acc: 68.5% | LR: 2.15e-04\n",
      "--> Running Seed 217401...\n",
      "   Ep 0 | Val Acc: 36.6% | LR: 3.96e-04\n",
      "   Ep 10 | Val Acc: 50.4% | LR: 4.38e-04\n",
      "   Ep 20 | Val Acc: 52.6% | LR: 7.26e-04\n",
      "   Ep 30 | Val Acc: 59.2% | LR: 5.36e-04\n",
      "   Ep 40 | Val Acc: 66.7% | LR: 3.95e-04\n",
      "   Ep 50 | Val Acc: 68.3% | LR: 2.91e-04\n",
      "   Ep 60 | Val Acc: 70.1% | LR: 2.15e-04\n",
      "Ablation Complete. Data saved to ablation_results.csv\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# ==========================================\n",
    "# 1. SETUP & CONFIGURATION\n",
    "# ==========================================\n",
    "# We use fewer seeds per ablation to keep runtime reasonable (~1.5 hours total)\n",
    "# If you have 9 hours of Kaggle time, you can add more seeds.\n",
    "SEEDS = [38042, 217401] \n",
    "ABLATION_VALUES = [0.0, 1e-10, 1e-8, 1e-6] # The specific values to test\n",
    "\n",
    "EPOCHS = 70 # Reduced from 90 to 70 for faster ablation feedback\n",
    "HYPER_INTERVAL = 2  \n",
    "\n",
    "full_data_log = []\n",
    "\n",
    "# ==========================================\n",
    "# 2. HELPER FUNCTIONS\n",
    "# ==========================================\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_metrics(model, loader):\n",
    "    model.eval()\n",
    "    total_loss, total, correct = 0.0, 0, 0\n",
    "    for x, y in loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        logits = model(x)\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "        total_loss += loss.item() * x.size(0)\n",
    "        _, preds = logits.max(1)\n",
    "        correct += preds.eq(y).sum().item()\n",
    "        total += x.size(0)\n",
    "    return total_loss / total, correct / total\n",
    "\n",
    "# ==========================================\n",
    "# 3. MAIN ABLATION LOOP\n",
    "# ==========================================\n",
    "print(f\"Starting Ablation Experiment.\")\n",
    "print(f\"Comparing HyperLRs: {ABLATION_VALUES}\")\n",
    "\n",
    "for h_lr in ABLATION_VALUES:\n",
    "    print(f\"\\n{'#'*70}\")\n",
    "    print(f\"PROCESSING ABLATION GROUP: hyper_lr = {h_lr}\")\n",
    "    print(f\"{'#'*70}\")\n",
    "    \n",
    "    for seed in SEEDS:\n",
    "        print(f\"--> Running Seed {seed}...\")\n",
    "        set_seed(seed)\n",
    "        \n",
    "        # Init Model\n",
    "        model = ViTSmall(\n",
    "            image_size=32, patch_size=8, embed_dim=192,\n",
    "            depth=12, num_heads=3, mlp_ratio=4.0, dropout=0.1\n",
    "        ).to(device)\n",
    "\n",
    "        # Init HyperGradient with CURRENT ABLATION VALUE\n",
    "        hg = HyperGradient(\n",
    "            model, train_loader, val_loader,\n",
    "            lr=3.3e-4, wd=0.07,\n",
    "            hyper_lr=h_lr  # <--- CRITICAL CHANGE\n",
    "        )\n",
    "\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=hg.lr, weight_decay=hg.wd)\n",
    "\n",
    "        for epoch in range(EPOCHS):\n",
    "            t0 = time.time()\n",
    "            model.train()\n",
    "            \n",
    "            # Batch Loop\n",
    "            running_loss, seen = 0.0, 0\n",
    "            for x, y in train_loader:\n",
    "                x, y = x.to(device), y.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                loss = F.cross_entropy(model(x), y)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                running_loss += loss.item() * x.size(0)\n",
    "                seen += x.size(0)\n",
    "            \n",
    "            # Hyper Step\n",
    "            if epoch % HYPER_INTERVAL == 0:\n",
    "                hg.step()\n",
    "                for pg in optimizer.param_groups:\n",
    "                    pg['lr'] = hg.lr\n",
    "                    pg['weight_decay'] = hg.wd\n",
    "\n",
    "            # Validation\n",
    "            val_loss, val_acc = evaluate_metrics(model, val_loader)\n",
    "            \n",
    "            # Log\n",
    "            full_data_log.append({\n",
    "                'HyperLR_Group': str(h_lr), # Convert to string for better plotting labels\n",
    "                'Seed': seed,\n",
    "                'Epoch': epoch,\n",
    "                'Train_Loss': running_loss / seen,\n",
    "                'Val_Acc': val_acc,\n",
    "                'LR': hg.lr,\n",
    "                'WD': hg.wd\n",
    "            })\n",
    "            \n",
    "            if epoch % 10 == 0:\n",
    "                print(f\"   Ep {epoch} | Val Acc: {val_acc:.1%} | LR: {hg.lr:.2e}\")\n",
    "\n",
    "# Save Final Results\n",
    "df = pd.DataFrame(full_data_log)\n",
    "df.to_csv(\"ablation_results.csv\", index=False)\n",
    "print(\"Ablation Complete. Data saved to ablation_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "40ccd1f5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-05T11:22:45.845279Z",
     "iopub.status.busy": "2025-12-05T11:22:45.845037Z",
     "iopub.status.idle": "2025-12-05T11:22:47.912861Z",
     "shell.execute_reply": "2025-12-05T11:22:47.911713Z"
    },
    "papermill": {
     "duration": 2.074713,
     "end_time": "2025-12-05T11:22:47.914103",
     "exception": true,
     "start_time": "2025-12-05T11:22:45.839390",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The palette dictionary is missing keys: {0.0, 1e-08, 1e-10, 1e-06}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_20/4231612620.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m# Plot 1: Validation Accuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m sns.lineplot(\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Epoch\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Val_Acc\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"HyperLR_Group\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mpalette\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpalette\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlinewidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/seaborn/relational.py\u001b[0m in \u001b[0;36mlineplot\u001b[0;34m(data, x, y, hue, size, style, units, palette, hue_order, hue_norm, sizes, size_order, size_norm, dashes, markers, style_order, estimator, errorbar, n_boot, seed, orient, sort, err_style, err_kws, legend, ci, ax, **kwargs)\u001b[0m\n\u001b[1;32m    623\u001b[0m     )\n\u001b[1;32m    624\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 625\u001b[0;31m     \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_hue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpalette\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpalette\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhue_order\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhue_norm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    626\u001b[0m     \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msizes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msizes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msize_order\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msize_norm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m     \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_style\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmarkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmarkers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdashes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdashes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstyle_order\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/seaborn/_oldcore.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(cls, plotter, *args, **kwargs)\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0;31m# This method is assigned the __init__ docstring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0mmethod_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"_{cls.__name__[:-7].lower()}_map\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0msetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mplotter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mplotter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mplotter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/seaborn/_oldcore.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, plotter, palette, order, norm)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m                 \u001b[0mcmap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnorm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 148\u001b[0;31m                 levels, lookup_table = self.categorical_mapping(\n\u001b[0m\u001b[1;32m    149\u001b[0m                     \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpalette\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m                 )\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/seaborn/_oldcore.py\u001b[0m in \u001b[0;36mcategorical_mapping\u001b[0;34m(self, data, palette, order)\u001b[0m\n\u001b[1;32m    225\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m                 \u001b[0merr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"The palette dictionary is missing keys: {}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 227\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m             \u001b[0mlookup_table\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpalette\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: The palette dictionary is missing keys: {0.0, 1e-08, 1e-10, 1e-06}"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 2000x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "# Load the data we just saved\n",
    "df = pd.read_csv(\"ablation_results.csv\")\n",
    "\n",
    "# Setup layout\n",
    "fig, axes = plt.subplots(1, 2, figsize=(20, 6))\n",
    "\n",
    "# Define colors for clarity\n",
    "palette = {\n",
    "    '0.0': \"gray\",      # Baseline\n",
    "    '1e-10': \"green\",   # Smooth\n",
    "    '1e-08': \"blue\",    # Moderate\n",
    "    '1e-06': \"red\"      # Chaotic\n",
    "}\n",
    "\n",
    "# Plot 1: Validation Accuracy\n",
    "sns.lineplot(\n",
    "    data=df, x=\"Epoch\", y=\"Val_Acc\", hue=\"HyperLR_Group\", \n",
    "    palette=palette, ax=axes[0], linewidth=2\n",
    ")\n",
    "axes[0].set_title(\"Validation Accuracy by Hyper-LR\")\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Learning Rate Dynamics\n",
    "sns.lineplot(\n",
    "    data=df, x=\"Epoch\", y=\"LR\", hue=\"HyperLR_Group\", \n",
    "    palette=palette, ax=axes[1], linewidth=2\n",
    ")\n",
    "axes[1].set_title(\"Learning Rate Adaptation\")\n",
    "axes[1].set_yscale(\"log\") # Log scale to see the magnitude differences\n",
    "axes[1].grid(True, alpha=0.3, which=\"both\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"ablation_chart.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e40aa8",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 8911589,
     "sourceId": 13979389,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 12330.076724,
   "end_time": "2025-12-05T11:22:50.293126",
   "environment_variables": {},
   "exception": true,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-12-05T07:57:20.216402",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
