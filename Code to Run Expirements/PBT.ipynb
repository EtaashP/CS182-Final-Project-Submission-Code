{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1764849308990,
     "user": {
      "displayName": "Etaash Tripathi Patel",
      "userId": "11287900455267433447"
     },
     "user_tz": 480
    },
    "id": "qycmNj_FevBv",
    "outputId": "f5f11fd1-2b23-4a36-e12c-e245485c9e1b"
   },
   "outputs": [
    {
     "data": {
      "application/javascript": "\nfunction ClickConnect(){\n    console.log(\"Clicking reconnect button to prevent idle timeout\");\n    document.querySelector(\"colab-toolbar-button#connect\").click()\n}\nsetInterval(ClickConnect, 1200000);\n",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# KEEP COLAB SESSION ALIVE\n",
    "# -----------------------------\n",
    "import IPython\n",
    "from google.colab import output\n",
    "\n",
    "# JavaScript to auto-click \"connect\" button every 60s to avoid idle disconnect\n",
    "js = \"\"\"\n",
    "function ClickConnect(){\n",
    "    console.log(\"Clicking reconnect button to prevent idle timeout\");\n",
    "    document.querySelector(\"colab-toolbar-button#connect\").click()\n",
    "}\n",
    "setInterval(ClickConnect, 1200000);\n",
    "\"\"\"\n",
    "display(IPython.display.Javascript(js))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This file is used to run both the base PBT and WD ablations. Manually adjust the function calls for run_pbt_expirement_vit and then run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7QKfWDDf8fmS"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import random\n",
    "import copy\n",
    "from typing import List, Dict, Any, Tuple, Optional\n",
    "import matplotlib.pyplot as plt\n",
    "from dataclasses import dataclass, field\n",
    "import time\n",
    "import math\n",
    "from functools import wraps\n",
    "import os\n",
    "from google.colab import drive\n",
    "import io\n",
    "import contextlib\n",
    "import rich"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7iSrHCLnjXu-"
   },
   "outputs": [],
   "source": [
    "\n",
    "# -----------------------------\n",
    "# ViT Architecture (from grid_vit_cifar10.py)\n",
    "# -----------------------------\n",
    "\n",
    "class DropPath(nn.Module):\n",
    "    def __init__(self, drop_prob: float = 0.0):\n",
    "        super().__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.drop_prob == 0.0 or not self.training:\n",
    "            return x\n",
    "        keep_prob = 1.0 - self.drop_prob\n",
    "        shape = (x.shape[0],) + (1,) * (x.ndim - 1)\n",
    "        random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)\n",
    "        random_tensor.floor_()\n",
    "        return x.div(keep_prob) * random_tensor\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, dim, mlp_ratio=4, drop=0.0):\n",
    "        super().__init__()\n",
    "        hidden = int(dim * mlp_ratio)\n",
    "        self.fc1 = nn.Linear(dim, hidden)\n",
    "        self.act = nn.GELU()\n",
    "        self.fc2 = nn.Linear(hidden, dim)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        return x\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, num_heads=6, qkv_bias=True, attn_drop=0.0, proj_drop=0.0):\n",
    "        super().__init__()\n",
    "        assert dim % num_heads == 0\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = dim // num_heads\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, N, C = x.shape\n",
    "        qkv = self.qkv(x)  # B, N, 3C\n",
    "        qkv = qkv.reshape(B, N, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]  # each: B, heads, N, head_dim\n",
    "\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        attn = self.attn_drop(attn)\n",
    "\n",
    "        out = attn @ v  # B, heads, N, head_dim\n",
    "        out = out.transpose(1, 2).reshape(B, N, C)\n",
    "        out = self.proj(out)\n",
    "        out = self.proj_drop(out)\n",
    "        return out\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, dim, num_heads, mlp_ratio, drop=0.0, attn_drop=0.0, drop_path=0.0):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(dim)\n",
    "        self.attn = Attention(dim, num_heads=num_heads, attn_drop=attn_drop, proj_drop=drop)\n",
    "        self.drop_path = DropPath(drop_path)\n",
    "        self.norm2 = nn.LayerNorm(dim)\n",
    "        self.mlp = MLP(dim, mlp_ratio=mlp_ratio, drop=drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.drop_path(self.attn(self.norm1(x)))\n",
    "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
    "        return x\n",
    "\n",
    "class PatchEmbed(nn.Module):\n",
    "    def __init__(self, img_size=32, patch_size=4, in_chans=3, embed_dim=384):\n",
    "        super().__init__()\n",
    "        assert img_size % patch_size == 0\n",
    "        self.grid = img_size // patch_size  # 8\n",
    "        self.num_patches = self.grid * self.grid  # 64\n",
    "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.proj(x)  # B, C, 8, 8\n",
    "        x = x.flatten(2).transpose(1, 2)  # B, 64, C\n",
    "        return x\n",
    "\n",
    "class ViTSmallCIFAR(nn.Module):\n",
    "    def __init__(self, num_classes=10, img_size=32, patch_size=4,\n",
    "                 embed_dim=384, depth=12, num_heads=6, mlp_ratio=4.0,\n",
    "                 drop_rate=0.0, attn_drop_rate=0.0, drop_path_rate=0.0,\n",
    "                 cls_norm=True):\n",
    "        super().__init__()\n",
    "        self.patch_embed = PatchEmbed(img_size, patch_size, 3, embed_dim)\n",
    "        num_patches = self.patch_embed.num_patches\n",
    "\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim))\n",
    "        self.pos_drop = nn.Dropout(p=drop_rate)\n",
    "\n",
    "        # stochastic depth decay rule\n",
    "        dpr = torch.linspace(0, drop_path_rate, steps=depth).tolist()\n",
    "\n",
    "        self.blocks = nn.ModuleList([\n",
    "            Block(embed_dim, num_heads, mlp_ratio,\n",
    "                  drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i])\n",
    "            for i in range(depth)\n",
    "        ])\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        self.cls_norm = cls_norm\n",
    "        self.head = nn.Linear(embed_dim, num_classes)\n",
    "\n",
    "        nn.init.trunc_normal_(self.pos_embed, std=0.02)\n",
    "        nn.init.trunc_normal_(self.cls_token, std=0.02)\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.trunc_normal_(m.weight, std=0.02)\n",
    "            if m.bias is not None:\n",
    "                nn.init.zeros_(m.bias)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.ones_(m.weight)\n",
    "            nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B = x.shape[0]\n",
    "        x = self.patch_embed(x)  # B, 64, C\n",
    "        cls = self.cls_token.expand(B, -1, -1)\n",
    "        x = torch.cat([cls, x], dim=1)  # B, 65, C\n",
    "        x = x + self.pos_embed\n",
    "        x = self.pos_drop(x)\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x)\n",
    "        if self.cls_norm:\n",
    "            x = self.norm(x)\n",
    "        cls_tok = x[:, 0]\n",
    "        logits = self.head(cls_tok)\n",
    "        return logits\n",
    "\n",
    "# ---------------------------\n",
    "# Warmup Cosine LR Scheduler\n",
    "# ---------------------------\n",
    "\n",
    "class WarmupCosineLR(optim.lr_scheduler._LRScheduler):\n",
    "    def __init__(self, optimizer, total_steps, warmup_steps=0, min_lr=1e-5, last_epoch=-1):\n",
    "        self.total_steps = total_steps\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.min_lr = min_lr\n",
    "        super().__init__(optimizer, last_epoch)\n",
    "\n",
    "    def get_lr(self):\n",
    "        step = self.last_epoch + 1\n",
    "        lrs = []\n",
    "        for base_lr in self.base_lrs:\n",
    "            if step < self.warmup_steps:\n",
    "                lr = base_lr * float(step) / float(max(1, self.warmup_steps))\n",
    "            else:\n",
    "                progress = (step - self.warmup_steps) / float(max(1, self.total_steps - self.warmup_steps))\n",
    "                cosine = 0.5 * (1.0 + math.cos(math.pi * progress))\n",
    "                lr = self.min_lr + (base_lr - self.min_lr) * cosine\n",
    "            lrs.append(lr)\n",
    "        return lrs\n",
    "\n",
    "# ---------------------------\n",
    "# Timer Class\n",
    "# ---------------------------\n",
    "\n",
    "class PBTTimer:\n",
    "    \"\"\"Timer class for tracking PBT experiment timing\"\"\"\n",
    "    def __init__(self):\n",
    "        self.metrics = {\n",
    "            'total_start': None,\n",
    "            'total_end': None,\n",
    "            'epoch_times': [],\n",
    "            'member_times': [],  # [epoch][member]\n",
    "            'update_times': [],\n",
    "            'member_details': {},  # {member_id: [times]}\n",
    "            'batch_size_history': []  # [epoch][member_batch_size]\n",
    "        }\n",
    "\n",
    "    def start_total(self):\n",
    "        \"\"\"Start overall timer\"\"\"\n",
    "        self.metrics['total_start'] = time.time()\n",
    "\n",
    "    def end_total(self):\n",
    "        \"\"\"End overall timer\"\"\"\n",
    "        self.metrics['total_end'] = time.time()\n",
    "\n",
    "    def get_total_time(self):\n",
    "        \"\"\"Get total runtime\"\"\"\n",
    "        if self.metrics['total_start'] and self.metrics['total_end']:\n",
    "            return self.metrics['total_end'] - self.metrics['total_start']\n",
    "        return 0\n",
    "\n",
    "    def start_epoch(self):\n",
    "        \"\"\"Start timing an epoch\"\"\"\n",
    "        self.current_epoch_start = time.time()\n",
    "        self.current_member_times = []\n",
    "        self.current_batch_sizes = []\n",
    "\n",
    "    def end_epoch(self):\n",
    "        \"\"\"End timing an epoch\"\"\"\n",
    "        if hasattr(self, 'current_epoch_start'):\n",
    "            epoch_time = time.time() - self.current_epoch_start\n",
    "            self.metrics['epoch_times'].append(epoch_time)\n",
    "            self.metrics['member_times'].append(self.current_member_times)\n",
    "            self.metrics['batch_size_history'].append(self.current_batch_sizes)\n",
    "\n",
    "    def time_member(self, member_id: int, batch_size: int = None):\n",
    "        \"\"\"Context manager for timing member training\"\"\"\n",
    "        class MemberTimer:\n",
    "            def __init__(self, timer, member_id, batch_size):\n",
    "                self.timer = timer\n",
    "                self.member_id = member_id\n",
    "                self.batch_size = batch_size\n",
    "\n",
    "            def __enter__(self):\n",
    "                self.start = time.time()\n",
    "                return self\n",
    "\n",
    "            def __exit__(self, *args):\n",
    "                elapsed = time.time() - self.start\n",
    "                self.timer.current_member_times.append(elapsed)\n",
    "                self.timer.current_batch_sizes.append(self.batch_size)\n",
    "\n",
    "                # Track detailed member times\n",
    "                if self.member_id not in self.timer.metrics['member_details']:\n",
    "                    self.timer.metrics['member_details'][self.member_id] = []\n",
    "                self.timer.metrics['member_details'][self.member_id].append(elapsed)\n",
    "\n",
    "        return MemberTimer(self, member_id, batch_size)\n",
    "\n",
    "    def time_update(self):\n",
    "        \"\"\"Context manager for timing population updates\"\"\"\n",
    "        class UpdateTimer:\n",
    "            def __init__(self, timer):\n",
    "                self.timer = timer\n",
    "\n",
    "            def __enter__(self):\n",
    "                self.start = time.time()\n",
    "                return self\n",
    "\n",
    "            def __exit__(self, *args):\n",
    "                elapsed = time.time() - self.start\n",
    "                self.timer.metrics['update_times'].append(elapsed)\n",
    "\n",
    "        return UpdateTimer(self)\n",
    "\n",
    "    def print_report(self):\n",
    "        \"\"\"Print detailed timing report\"\"\"\n",
    "        total = self.get_total_time()\n",
    "        epoch_times = self.metrics['epoch_times']\n",
    "        member_times = self.metrics['member_times']\n",
    "        update_times = self.metrics['update_times']\n",
    "        batch_history = self.metrics['batch_size_history']\n",
    "\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"{'PBT TIMING REPORT':^70}\")\n",
    "        print(f\"{'='*70}\")\n",
    "\n",
    "        # Overall statistics\n",
    "        print(f\"\\n{'Overall Statistics':<40} {'Value':<30}\")\n",
    "        print(f\"{'-'*70}\")\n",
    "        print(f\"{'Total runtime':<40} {total:.2f}s ({total/60:.2f} min)\")\n",
    "        print(f\"{'Number of epochs':<40} {len(epoch_times)}\")\n",
    "        print(f\"{'Number of population updates':<40} {len([t for t in update_times if t > 0])}\")\n",
    "\n",
    "        # Epoch timing\n",
    "        avg_epoch = np.mean(epoch_times)\n",
    "        std_epoch = np.std(epoch_times)\n",
    "        print(f\"\\n{'Epoch Timing':<40} {'Value':<30}\")\n",
    "        print(f\"{'-'*70}\")\n",
    "        print(f\"{'Average epoch time':<40} {avg_epoch:.2f}s\")\n",
    "        print(f\"{'Std epoch time':<40} {std_epoch:.2f}s\")\n",
    "        print(f\"{'Min epoch time':<40} {min(epoch_times):.2f}s (epoch {np.argmin(epoch_times)+1})\")\n",
    "        print(f\"{'Max epoch time':<40} {max(epoch_times):.2f}s (epoch {np.argmax(epoch_times)+1})\")\n",
    "\n",
    "        # Member training timing\n",
    "        all_member_times = [t for epoch_times in member_times for t in epoch_times]\n",
    "        avg_member_time = np.mean(all_member_times)\n",
    "        total_training_time = sum(all_member_times)\n",
    "\n",
    "        print(f\"\\n{'Member Training Timing':<40} {'Value':<30}\")\n",
    "        print(f\"{'-'*70}\")\n",
    "        print(f\"{'Average member time per epoch':<40} {avg_member_time:.2f}s\")\n",
    "        print(f\"{'Total training time':<40} {total_training_time:.2f}s\")\n",
    "        print(f\"{'Training efficiency':<40} {(total_training_time/total*100):.1f}%\")\n",
    "\n",
    "        # Population update timing\n",
    "        non_zero_updates = [t for t in update_times if t > 0]\n",
    "        if non_zero_updates:\n",
    "            avg_update = np.mean(non_zero_updates)\n",
    "            total_update = sum(update_times)\n",
    "\n",
    "            print(f\"\\n{'Population Update Timing':<40} {'Value':<30}\")\n",
    "            print(f\"{'-'*70}\")\n",
    "            print(f\"{'Average update time':<40} {avg_update:.2f}s\")\n",
    "            print(f\"{'Total update time':<40} {total_update:.2f}s\")\n",
    "            print(f\"{'Update efficiency':<40} {(total_update/total*100):.1f}%\")\n",
    "\n",
    "        # Batch size correlation\n",
    "        if batch_history and len(batch_history[0]) > 0:\n",
    "            batch_times = []\n",
    "            for epoch_idx, batch_sizes in enumerate(batch_history):\n",
    "                for member_idx, bs in enumerate(batch_sizes):\n",
    "                    batch_times.append((bs, member_times[epoch_idx][member_idx]))\n",
    "\n",
    "            # Group by batch size\n",
    "            batch_stats = {}\n",
    "            for bs, t in batch_times:\n",
    "                if bs not in batch_stats:\n",
    "                    batch_stats[bs] = []\n",
    "                batch_stats[bs].append(t)\n",
    "\n",
    "            print(f\"\\n{'Batch Size Analysis':<40} {'Avg Time':<15} {'Samples':<15}\")\n",
    "            print(f\"{'='*70}\")\n",
    "            for bs in sorted(batch_stats.keys()):\n",
    "                avg_time = np.mean(batch_stats[bs])\n",
    "                samples = len(batch_stats[bs])\n",
    "                print(f\"{f'Batch size {bs}':<40} {avg_time:.2f}s{'':<5} {samples:<15}\")\n",
    "\n",
    "        # Per-member statistics\n",
    "        print(f\"\\n{'Per-Member Statistics':<40} {'Avg Time':<15} {'Total Time':<15}\")\n",
    "        print(f\"{'='*70}\")\n",
    "        for member_id in sorted(self.metrics['member_details'].keys()):\n",
    "            times = self.metrics['member_details'][member_id]\n",
    "            avg_time = np.mean(times)\n",
    "            total_time = sum(times)\n",
    "            print(f\"{f'Member {member_id}':<40} {avg_time:.2f}s{'':<5} {total_time:.2f}s\")\n",
    "\n",
    "        print(f\"\\n{'='*70}\")\n",
    "\n",
    "# ---------------------------\n",
    "# PBT Configuration\n",
    "# ---------------------------\n",
    "\n",
    "@dataclass\n",
    "class PBTConfig:\n",
    "    # Population settings\n",
    "    population_size: int = 5  # Smaller for memory constraints\n",
    "    exploit_interval: int = 5  # Exploit every 5 epochs\n",
    "    truncation_factor: float = 0.25  # top 25% replace bottom 25%\n",
    "\n",
    "    # ViT-specific hyperparameter search spaces\n",
    "    lr_bounds: Tuple[float, float] = (1e-4, 1e-3)\n",
    "    weight_decay_bounds: Tuple[float, float] = (0.02, 0.15)\n",
    "    drop_path_bounds: Tuple[float, float] = (0.0, 0.2)\n",
    "    warmup_epochs_bounds: Tuple[int, int] = (5, 5)\n",
    "    batch_size_bounds: Tuple[int, int] = (256, 256)  # Batch size search space\n",
    "\n",
    "    # Training settings\n",
    "    epochs: int = 70  # Shorter run for demonstration\n",
    "    min_lr: float = 1e-6\n",
    "\n",
    "    # Model architecture (fixed for ViT-Small)\n",
    "    embed_dim: int = 192\n",
    "    depth: int = 12\n",
    "    num_heads: int = 3\n",
    "    mlp_ratio: float = 4.0\n",
    "    patch_size: int = 8\n",
    "    img_size: int = 32\n",
    "    drive_base_path: str = '/content/drive/MyDrive/Colab_Checkpoints'\n",
    "    seed: int = None\n",
    "# ---------------------------\n",
    "# PBT Member\n",
    "# ---------------------------\n",
    "\n",
    "class PBTMember:\n",
    "    def __init__(self, model_class, model_args, hyperparams: Dict[str, float], member_id: int,\n",
    "                 total_epochs: int, min_lr: float = 1e-6):\n",
    "        self.member_id = member_id\n",
    "        self.hyperparams = hyperparams\n",
    "        self.total_epochs = total_epochs\n",
    "        self.batch_size = hyperparams['batch_size']  # Get batch size from hyperparams\n",
    "        self.min_lr = min_lr\n",
    "\n",
    "        # NEW: Track LR history\n",
    "        self.lr_history = []  # Store LR at each epoch\n",
    "        self.current_lr = hyperparams['lr']  # Track current LR\n",
    "\n",
    "        # Create ViT model with specific hyperparameters\n",
    "        model_args['drop_path_rate'] = hyperparams['drop_path']\n",
    "        self.model = model_class(**model_args)\n",
    "\n",
    "        # AdamW optimizer for ViT\n",
    "        self.optimizer = optim.AdamW(\n",
    "            self.model.parameters(),\n",
    "            lr=hyperparams['lr'],\n",
    "            weight_decay=hyperparams['weight_decay'],\n",
    "            betas=(0.9, 0.999),\n",
    "            eps=1e-8\n",
    "        )\n",
    "\n",
    "        # Scheduler will be setup when dataloader is available\n",
    "        self.scheduler = None\n",
    "\n",
    "        # Tracking\n",
    "        self.performance_history = []\n",
    "        self.current_performance = 0.0\n",
    "        self.steps = 0\n",
    "        self.epoch = 0\n",
    "\n",
    "\n",
    "\n",
    "    def setup_scheduler(self, train_loader_length: int, warmup_epochs: int):\n",
    "        \"\"\"Setup scheduler with correct total steps based on batch size\"\"\"\n",
    "        total_steps = self.total_epochs * train_loader_length\n",
    "        warmup_steps = warmup_epochs * train_loader_length\n",
    "        self.scheduler = WarmupCosineLR(\n",
    "            self.optimizer,\n",
    "            total_steps=total_steps,\n",
    "            warmup_steps=warmup_steps,\n",
    "            min_lr=self.min_lr\n",
    "        )\n",
    "\n",
    "    def update_hyperparams(self, new_hyperparams: Dict[str, float]):\n",
    "        \"\"\"Update hyperparameters\"\"\"\n",
    "        old_hyperparams = copy.deepcopy(self.hyperparams)\n",
    "        self.hyperparams = new_hyperparams\n",
    "        self.batch_size = new_hyperparams['batch_size']  # Update batch size\n",
    "        self.current_lr = new_hyperparams['lr'] # Update current LR\n",
    "\n",
    "        # Log LR change\n",
    "        for key, val in old_hyperparams.items():\n",
    "          if old_hyperparams[key] != new_hyperparams[key]:\n",
    "            print(f\"Member {self.member_id}: {key} changed from {val} to {new_hyperparams[key]}\")\n",
    "\n",
    "\n",
    "        # Update optimizer\n",
    "        self.optimizer = optim.AdamW(\n",
    "            self.model.parameters(),\n",
    "            lr=new_hyperparams['lr'],\n",
    "            weight_decay=new_hyperparams['weight_decay'],\n",
    "            betas=(0.9, 0.999),\n",
    "            eps=1e-8\n",
    "        )\n",
    "\n",
    "        # Reset scheduler (will be setup with new dataloader)\n",
    "        self.scheduler = None\n",
    "\n",
    "\n",
    "    def record_lr(self, epoch_lr: float = None):\n",
    "      if epoch_lr is not None:\n",
    "          self.lr_history.append(epoch_lr)\n",
    "      else:\n",
    "          # Get current LR from optimizer\n",
    "          current_lr = self.optimizer.param_groups[0]['lr']\n",
    "          self.lr_history.append(current_lr)\n",
    "          self.current_lr = current_lr\n",
    "\n",
    "    def update_weights(self, new_weights):\n",
    "        self.model.load_state_dict(new_weights)\n",
    "\n",
    "    def save_checkpoint(self):\n",
    "        \"\"\"Save model and optimizer state\"\"\"\n",
    "        return {\n",
    "            'model_state': copy.deepcopy(self.model.state_dict()),\n",
    "            'optimizer_state': copy.deepcopy(self.optimizer.state_dict()),\n",
    "            'scheduler_state': copy.deepcopy(self.scheduler.state_dict()) if self.scheduler else None,\n",
    "            'hyperparams': copy.deepcopy(self.hyperparams),\n",
    "            'performance': self.current_performance,\n",
    "            'steps': self.steps,\n",
    "            'epoch': self.epoch\n",
    "        }\n",
    "\n",
    "    def load_checkpoint(self, checkpoint):\n",
    "        \"\"\"Load model, optimizer, and scheduler state\"\"\"\n",
    "        self.model.load_state_dict(checkpoint['model_state'])\n",
    "        self.optimizer.load_state_dict(checkpoint['optimizer_state'])\n",
    "        if checkpoint['scheduler_state']:\n",
    "            # Need to initialize scheduler before loading state_dict if it's not None\n",
    "            # This assumes `setup_scheduler` would have been called before loading if it's not the first epoch\n",
    "            if self.scheduler is None: # Placeholder if scheduler wasn't setup yet (e.g., loading into a fresh member obj)\n",
    "                # This case is tricky, requires knowing train_loader_length etc.\n",
    "                # For PBT, usually scheduler is reset/re-setup after hyperparam changes or init\n",
    "                # For now, rely on `setup_scheduler` being called after `load_checkpoint` if needed.\n",
    "                pass # Or raise an error/warning\n",
    "            else:\n",
    "                self.scheduler.load_state_dict(checkpoint['scheduler_state'])\n",
    "\n",
    "        self.hyperparams = checkpoint['hyperparams']\n",
    "        self.batch_size = checkpoint['hyperparams']['batch_size']\n",
    "        self.current_performance = checkpoint['performance']\n",
    "        self.steps = checkpoint['steps']\n",
    "        self.epoch = checkpoint['epoch']\n",
    "\n",
    "# ---------------------------\n",
    "# Google Drive Checkpointing Helpers\n",
    "# ---------------------------\n",
    "\n",
    "def mount_drive_if_not_mounted():\n",
    "    \"\"\"Mounts Google Drive if it's not already mounted.\"\"\"\n",
    "    if not os.path.exists('/content/drive'):\n",
    "        print(\"Google Drive not mounted. Mounting now...\")\n",
    "        drive.mount('/content/drive')\n",
    "    else:\n",
    "        print(\"Google Drive already mounted.\")\n",
    "\n",
    "def save_member_checkpoint_to_drive(member: PBTMember, drive_base_path: str, seed):\n",
    "    \"\"\"\n",
    "    Saves a member's checkpoint to Google Drive with specified filename format, overwriting if exists.\n",
    "\n",
    "    Args:\n",
    "        member (PBTMember): The PBT member object.\n",
    "        drive_base_path (str): The base path in Google Drive (e.g., '/content/drive/MyDrive/Colab_Checkpoints').\n",
    "    \"\"\"\n",
    "    # Ensure Drive is mounted\n",
    "    mount_drive_if_not_mounted()\n",
    "\n",
    "    # Construct the target directory and ensure it exists\n",
    "    # Filename format: model_{member_id}_seed_{member_id}.pt (member_id serves as 'seed' here)\n",
    "    checkpoint_dir = os.path.join(drive_base_path, \"pbt_checkpoints\")\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "    checkpoint_filename = os.path.join(checkpoint_dir, f\"model_{member.member_id}_seed_{seed}.pt\")\n",
    "\n",
    "    checkpoint_data = {\n",
    "        'model_state': member.model.state_dict(),\n",
    "        'optimizer_state': member.optimizer.state_dict(),\n",
    "        'scheduler_state': member.scheduler.state_dict() if member.scheduler else None,\n",
    "        'hyperparams': member.hyperparams,\n",
    "        'performance': member.current_performance,\n",
    "        'steps': member.steps,\n",
    "        'epoch': member.epoch\n",
    "    }\n",
    "\n",
    "    torch.save(checkpoint_data, checkpoint_filename)\n",
    "    print(f\"Member {member.member_id}: Checkpoint saved to {checkpoint_filename}\")\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Population Based Training\n",
    "# ---------------------------\n",
    "\n",
    "class PopulationBasedTraining:\n",
    "    def __init__(self, config: PBTConfig, model_class, model_args):\n",
    "        self.config = config\n",
    "        self.model_class = model_class\n",
    "        self.model_args = model_args\n",
    "        self.population: List[PBTMember] = []\n",
    "        self.best_performance = 0.0\n",
    "        self.best_member = None\n",
    "\n",
    "    def initialize_population(self):\n",
    "        \"\"\"Initialize population with random hyperparameters including batch size\"\"\"\n",
    "        for i in range(self.config.population_size):\n",
    "            hyperparams = {\n",
    "                'lr': self._sample_log_uniform(*self.config.lr_bounds),\n",
    "                'weight_decay': random.uniform(*self.config.weight_decay_bounds),\n",
    "                'drop_path': random.uniform(*self.config.drop_path_bounds),\n",
    "                'warmup_epochs': random.randint(*self.config.warmup_epochs_bounds),\n",
    "                'batch_size': self._sample_batch_size()\n",
    "            }\n",
    "\n",
    "            member = PBTMember(\n",
    "                model_class=self.model_class,\n",
    "                model_args=self.model_args,\n",
    "                hyperparams=hyperparams,\n",
    "                member_id=i,\n",
    "                total_epochs=self.config.epochs,\n",
    "                min_lr=self.config.min_lr\n",
    "            )\n",
    "            self.population.append(member)\n",
    "\n",
    "    def _sample_log_uniform(self, low, high):\n",
    "        \"\"\"Sample from log-uniform distribution\"\"\"\n",
    "        return np.exp(random.uniform(np.log(low), np.log(high)))\n",
    "\n",
    "    def _sample_batch_size(self):\n",
    "        \"\"\"Sample batch size from discrete options within bounds\"\"\"\n",
    "        batch_options = [64, 128, 256, 512]\n",
    "        valid_options = [bs for bs in batch_options\n",
    "                        if self.config.batch_size_bounds[0] <= bs <= self.config.batch_size_bounds[1]]\n",
    "        return random.choice(valid_options)\n",
    "\n",
    "    def _perturb_hyperparam(self, value, bounds, is_log=True):\n",
    "        \"\"\"Perturb hyperparameter with random noise\"\"\"\n",
    "        if is_log:\n",
    "            low, high = np.log(bounds[0]), np.log(bounds[1])\n",
    "            current = np.log(value)\n",
    "        else:\n",
    "            low, high = bounds\n",
    "            current = value\n",
    "\n",
    "        # Random walk with reflection at boundaries\n",
    "        perturbed = current + random.uniform(-0.2, 0.2) * (high - low)\n",
    "        perturbed = max(min(perturbed, high), low)  # Clip to bounds\n",
    "\n",
    "        return np.exp(perturbed) if is_log else perturbed\n",
    "\n",
    "    def _perturb_batch_size(self, current_batch_size):\n",
    "        \"\"\"Perturb batch size within discrete options\"\"\"\n",
    "        batch_options = [64, 128, 256, 512]\n",
    "        # Filter options within bounds\n",
    "        valid_options = [bs for bs in batch_options\n",
    "                        if self.config.batch_size_bounds[0] <= bs <= self.config.batch_size_bounds[1]]\n",
    "\n",
    "        if current_batch_size not in valid_options:\n",
    "            return random.choice(valid_options)\n",
    "\n",
    "        current_idx = valid_options.index(current_batch_size)\n",
    "        # Move up or down by 1 step (or stay same)\n",
    "        new_idx = current_idx + random.choice([-1, 0, 1])\n",
    "        new_idx = max(0, min(new_idx, len(valid_options) - 1))\n",
    "\n",
    "        return valid_options[new_idx]\n",
    "\n",
    "    def exploit_and_explore(self):\n",
    "        \"\"\"PBT core algorithm for ViT\"\"\"\n",
    "        # Rank population by performance\n",
    "        ranked_members = sorted(self.population,\n",
    "                              key=lambda x: x.current_performance,\n",
    "                              reverse=True)\n",
    "\n",
    "        num_truncate = int(self.config.truncation_factor * self.config.population_size)\n",
    "        top_members = ranked_members[:num_truncate]\n",
    "        bottom_members = ranked_members[-num_truncate:]\n",
    "\n",
    "        print(f\"\\n=== PBT Exploit & Explore ===\")\n",
    "        print(f\"Top performers: {[m.current_performance for m in top_members]}\")\n",
    "        print(f\"Bottom performers: {[m.current_performance for m in bottom_members]}\")\n",
    "\n",
    "        # Exploit: Copy from top to bottom\n",
    "        for bottom_member in bottom_members:\n",
    "            # Randomly select a top member to copy from\n",
    "            donor = random.choice(top_members)\n",
    "            #checkpoint = donor.save_checkpoint()\n",
    "            #start_t = time.time()\n",
    "            #bottom_member.load_checkpoint(checkpoint)\n",
    "            better_weights = donor.model.state_dict()\n",
    "            bottom_member.update_weights(better_weights)\n",
    "            #print(f'load checkpoint in {time.time() - start_t} seconds')\n",
    "\n",
    "            # Explore: Perturb hyperparameters\n",
    "            new_hyperparams = {\n",
    "                'lr': self._perturb_hyperparam(\n",
    "                    bottom_member.hyperparams['lr'],\n",
    "                    self.config.lr_bounds,\n",
    "                    is_log=True\n",
    "                ),\n",
    "                'weight_decay': self._perturb_hyperparam(\n",
    "                    bottom_member.hyperparams['weight_decay'],\n",
    "                    self.config.weight_decay_bounds,\n",
    "                    is_log=False\n",
    "                ),\n",
    "                'drop_path': self._perturb_hyperparam(\n",
    "                    bottom_member.hyperparams['drop_path'],\n",
    "                    self.config.drop_path_bounds,\n",
    "                    is_log=False\n",
    "                ),\n",
    "                'warmup_epochs': bottom_member.hyperparams['warmup_epochs'], #keeping warmup epochs the same\n",
    "                'batch_size': self._perturb_batch_size(bottom_member.hyperparams['batch_size'])\n",
    "            }\n",
    "\n",
    "            bottom_member.update_hyperparams(new_hyperparams)\n",
    "\n",
    "            print(f\"Member {bottom_member.member_id} copied from {donor.member_id}\")\n",
    "            print(f\"  LR={new_hyperparams['lr']:.2e}, WD={new_hyperparams['weight_decay']:.3f}, \"\n",
    "                  f\"DropPath={new_hyperparams['drop_path']:.3f}, \"\n",
    "                  f\"Warmup={new_hyperparams['warmup_epochs']} epochs, \"\n",
    "                  f\"Batch={new_hyperparams['batch_size']}\")\n",
    "\n",
    "    def train_member_one_epoch(self, member: PBTMember, train_loader, device, scaler):\n",
    "        \"\"\"Train one ViT member for one epoch with mixed precision\"\"\"\n",
    "        start = time.time()\n",
    "        member.model.train()\n",
    "        member.model.to(device)\n",
    "\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        total_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        # Store LR at the start of epoch\n",
    "        start_lr = member.optimizer.param_groups[0]['lr']\n",
    "\n",
    "\n",
    "\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "\n",
    "            member.optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "            # Mixed precision training\n",
    "            with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n",
    "                output = member.model(data)\n",
    "                loss = criterion(output, target)\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(member.optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            # Update scheduler if available (changes LR!)\n",
    "            if member.scheduler:\n",
    "                member.scheduler.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "            total += target.size(0)\n",
    "            member.steps += 1\n",
    "\n",
    "        accuracy = 100. * correct / total\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "\n",
    "        # Record LR at the end of epoch\n",
    "        end_lr = member.optimizer.param_groups[0]['lr']\n",
    "        member.lr_history.append(end_lr) # Store in history\n",
    "        member.current_lr = end_lr # Update current LR\n",
    "\n",
    "        # Track performance\n",
    "        member.current_performance = accuracy\n",
    "        member.performance_history.append(accuracy)\n",
    "        member.epoch += 1\n",
    "\n",
    "        # Print LR change if it occurred\n",
    "        if abs(start_lr - end_lr) > 1e-10: # If LR changed during epoch\n",
    "          print(f\" LR changed during epoch: {start_lr:.2e} -> {end_lr:.2e}\")\n",
    "        print(f'total runtime to train this model was {time.time() - start} seconds')\n",
    "        print(f'Hyperparameteres for model {member.member_id} at epoch {member.epoch}')\n",
    "        for key, val in member.hyperparams.items():\n",
    "          print(f'{key}: {val} \\n')\n",
    "        print(f'_______ end of hyperparam printing _____')\n",
    "        return avg_loss, accuracy\n",
    "\n",
    "    def evaluate_member(self, member: PBTMember, test_loader, device):\n",
    "        \"\"\"Evaluate ViT member on test set\"\"\"\n",
    "        member.model.eval()\n",
    "        member.model.to(device)\n",
    "\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for data, target in test_loader:\n",
    "                data, target = data.to(device), target.to(device)\n",
    "                output = member.model(data)\n",
    "                pred = output.argmax(dim=1, keepdim=True)\n",
    "                correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "                total += target.size(0)\n",
    "\n",
    "        accuracy = 100. * correct / total\n",
    "        member.current_performance = accuracy\n",
    "        return accuracy\n",
    "\n",
    "    def get_population_stats(self):\n",
    "        \"\"\"Get statistics about current population\"\"\"\n",
    "        performances = [m.current_performance for m in self.population]\n",
    "        lrs = [m.hyperparams['lr'] for m in self.population]\n",
    "        wds = [m.hyperparams['weight_decay'] for m in self.population]\n",
    "        drop_paths = [m.hyperparams['drop_path'] for m in self.population]\n",
    "        warmups = [m.hyperparams['warmup_epochs'] for m in self.population]\n",
    "        batch_sizes = [m.hyperparams['batch_size'] for m in self.population]\n",
    "\n",
    "        return {\n",
    "            'mean_performance': np.mean(performances),\n",
    "            'std_performance': np.std(performances),\n",
    "            'best_performance': np.max(performances),\n",
    "            'mean_lr': np.mean(lrs),\n",
    "            'mean_weight_decay': np.mean(wds),\n",
    "            'mean_drop_path': np.mean(drop_paths),\n",
    "            'mean_warmup': np.mean(warmups),\n",
    "            'mean_batch_size': np.mean(batch_sizes),\n",
    "            'batch_sizes': batch_sizes\n",
    "        }\n",
    "\n",
    "# ---------------------------\n",
    "# Data Loading\n",
    "# ---------------------------\n",
    "\n",
    "def build_dataloaders(batch_size: int, num_workers: int = 4):\n",
    "    \"\"\"Build CIFAR-10 dataloaders\"\"\"\n",
    "    train_tf = transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=(0.4914, 0.4822, 0.4465),\n",
    "                           std=(0.2470, 0.2435, 0.2616)),\n",
    "    ])\n",
    "    test_tf = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=(0.4914, 0.4822, 0.4465),\n",
    "                           std=(0.2470, 0.2435, 0.2616)),\n",
    "    ])\n",
    "\n",
    "    train_ds = torchvision.datasets.CIFAR10(root=\"./data\", train=True, transform=train_tf, download=True)\n",
    "    test_ds = torchvision.datasets.CIFAR10(root=\"./data\", train=False, transform=test_tf, download=True)\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True,\n",
    "                            num_workers=num_workers, pin_memory=True)\n",
    "    test_loader = DataLoader(test_ds, batch_size=512, shuffle=False,\n",
    "                           num_workers=num_workers, pin_memory=True)\n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Btno5DLAjl6f"
   },
   "outputs": [],
   "source": [
    "\n",
    "# ---------------------------\n",
    "# Main Experiment\n",
    "# ---------------------------\n",
    "\n",
    "def run_pbt_vit_experiment(seed):\n",
    "    \"\"\"Run PBT for ViT-Small on CIFAR-10 with comprehensive timing\"\"\"\n",
    "    # Configuration\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Initialize timer\n",
    "    timer = PBTTimer()\n",
    "    timer.start_total()\n",
    "\n",
    "    # PBT Configuration for ViT\n",
    "    pbt_config = PBTConfig(\n",
    "        population_size=5,\n",
    "        exploit_interval=5,\n",
    "        truncation_factor=0.25,\n",
    "        #EPOCH CONFIGURATION\n",
    "        epochs=70,  # Even shorter for quick testing        REDUCED THE EPOCHS TO 2 FROM ORIGINAL\n",
    "        batch_size_bounds=(256, 256),\n",
    "        seed = seed\n",
    "        )\n",
    "\n",
    "    random.seed(pbt_config.seed)\n",
    "\n",
    "    # Mount Google Drive once at the beginning of the experiment\n",
    "    mount_drive_if_not_mounted()\n",
    "\n",
    "    # Initialize PBT for ViT\n",
    "    pbt = PopulationBasedTraining(\n",
    "        config=pbt_config,\n",
    "        model_class=ViTSmallCIFAR,\n",
    "        model_args={\n",
    "            'num_classes': 10,            # CHANGED THIS FROM 5 TO 10 TO MATCH CIFAR-10\n",
    "            'img_size': pbt_config.img_size,\n",
    "            'patch_size': pbt_config.patch_size,\n",
    "            'embed_dim': pbt_config.embed_dim,\n",
    "            'depth': pbt_config.depth,\n",
    "            'num_heads': pbt_config.num_heads,\n",
    "            'mlp_ratio': pbt_config.mlp_ratio,\n",
    "            'drop_rate': 0.0,\n",
    "            'attn_drop_rate': 0.0,\n",
    "            'cls_norm': True\n",
    "        }\n",
    "    )\n",
    "    pbt.initialize_population()\n",
    "\n",
    "    # Create gradient scaler for mixed precision\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=torch.cuda.is_available())\n",
    "\n",
    "    # Training loop\n",
    "    history = []\n",
    "    print(\"Starting PBT Training for ViT-Small...\")\n",
    "    print(f\"Population size: {pbt_config.population_size}\")\n",
    "    print(f\"Total epochs: {pbt_config.epochs}\")\n",
    "    print(f\"Exploit interval: {pbt_config.exploit_interval} epochs\")\n",
    "    print(f\"Checkpoints will be saved to: {pbt_config.drive_base_path}\")\n",
    "\n",
    "    for epoch in range(pbt_config.epochs):\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Epoch {epoch + 1}/{pbt_config.epochs}\")\n",
    "        print(f\"{'='*60}\")\n",
    "\n",
    "        # Start timing this epoch\n",
    "        timer.start_epoch()\n",
    "\n",
    "        # Train all members\n",
    "        for i, member in enumerate(pbt.population):\n",
    "            print(f\"\\n--- Training Member {i} (Batch size: {member.batch_size}) ---\")\n",
    "\n",
    "            # Time member training\n",
    "            with timer.time_member(member.member_id, member.batch_size):\n",
    "                start_t = time.time()\n",
    "                # Create dataloader for this member's batch size\n",
    "                train_loader, test_loader = build_dataloaders(\n",
    "                    batch_size=member.batch_size,\n",
    "                    num_workers=2\n",
    "                )\n",
    "\n",
    "                # Setup scheduler if needed\n",
    "                if member.scheduler is None:\n",
    "                    member.setup_scheduler(\n",
    "                        train_loader_length=len(train_loader),\n",
    "                        warmup_epochs=member.hyperparams['warmup_epochs']\n",
    "                    )\n",
    "                print(f'built data in {time.time() - start_t} seconds')\n",
    "\n",
    "                # Train member\n",
    "                loss, train_acc = pbt.train_member_one_epoch(member, train_loader, device, scaler)\n",
    "\n",
    "                # Evaluate this member\n",
    "                start_t = time.time()\n",
    "                test_acc = pbt.evaluate_member(member, test_loader, device)\n",
    "                print(f'evaluation in {time.time() - start_t} seconds')\n",
    "\n",
    "                print(f\"  Loss: {loss:.4f}\")\n",
    "                print(f\"  Train Accuracy: {train_acc:.2f}%\")\n",
    "                print(f\"  Test Accuracy: {test_acc:.2f}%\")\n",
    "\n",
    "                # Save checkpoint to Google Drive after each epoch for each member\n",
    "                save_member_checkpoint_to_drive(member, pbt_config.drive_base_path, pbt_config.seed)\n",
    "\n",
    "        # PBT step: exploit and explore\n",
    "        should_update = (epoch + 1) % pbt_config.exploit_interval == 0\n",
    "        if should_update:\n",
    "            print(f\"\\n--- Population Update (Epoch {epoch + 1}) ---\")\n",
    "            with timer.time_update():\n",
    "                pbt.exploit_and_explore()\n",
    "        else:\n",
    "            # Record 0 update time for non-update epochs\n",
    "            timer.metrics['update_times'].append(0.0)\n",
    "\n",
    "        # End timing this epoch\n",
    "        timer.end_epoch()\n",
    "\n",
    "        # Record statistics\n",
    "        stats = pbt.get_population_stats()\n",
    "        history.append(stats)\n",
    "\n",
    "        # Print epoch summary\n",
    "        epoch_time = timer.metrics['epoch_times'][-1]\n",
    "        avg_member_time = np.mean(timer.metrics['member_times'][-1])\n",
    "\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Epoch {epoch + 1} Summary:\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"Time: {epoch_time:.2f}s (Avg member: {avg_member_time:.2f}s)\")\n",
    "        print(f\"Population Mean Accuracy: {stats['mean_performance']:.2f}%\")\n",
    "        print(f\"Best Member Accuracy: {stats['best_performance']:.2f}%\")\n",
    "        print(f\"Mean Batch Size: {stats['mean_batch_size']:.0f}\")\n",
    "        print(f\"Mean Learning Rate: {stats['mean_lr']:.2e}\")\n",
    "        print(f\"Mean Weight Decay: {stats['mean_weight_decay']:.3f}\")\n",
    "\n",
    "    # End overall timer\n",
    "    timer.end_total()\n",
    "\n",
    "    # Final results\n",
    "    best_member = max(pbt.population, key=lambda x: x.current_performance)\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"{'PBT TRAINING COMPLETE':^70}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Best member: {best_member.member_id}\")\n",
    "    print(f\"Best accuracy: {best_member.current_performance:.2f}%\")\n",
    "    print(f\"\\nBest hyperparameters:\")\n",
    "    print(f\"  Learning Rate: {best_member.hyperparams['lr']:.2e}\")\n",
    "    print(f\"  Weight Decay: {best_member.hyperparams['weight_decay']:.3f}\")\n",
    "    print(f\"  Drop Path Rate: {best_member.hyperparams['drop_path']:.3f}\")\n",
    "    print(f\"  Warmup Epochs: {best_member.hyperparams['warmup_epochs']}\")\n",
    "    print(f\"  Batch Size: {best_member.hyperparams['batch_size']}\")\n",
    "\n",
    "    # Print timing report\n",
    "    timer.print_report()\n",
    "\n",
    "    # Plot results\n",
    "    plot_pbt_results_vit(history, pbt.population, timer)\n",
    "\n",
    "    return pbt, history, timer\n",
    "\n",
    "# ---------------------------\n",
    "# Visualization\n",
    "# ---------------------------\n",
    "\n",
    "def plot_pbt_results_vit(history, population, timer):\n",
    "    \"\"\"Plot PBT training results with timing\"\"\"\n",
    "    fig, axes = plt.subplots(3, 3, figsize=(18, 15))\n",
    "\n",
    "    # Flatten axes for easier indexing\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    # Plot 1: Population performance\n",
    "    epochs = range(1, len(history) + 1)\n",
    "    mean_perf = [h['mean_performance'] for h in history]\n",
    "    best_perf = [h['best_performance'] for h in history]\n",
    "\n",
    "    axes[0].plot(epochs, mean_perf, 'b-', label='Mean Population', linewidth=2)\n",
    "    axes[0].plot(epochs, best_perf, 'r-', label='Best Population', linewidth=2)\n",
    "    axes[0].set_xlabel('Epoch', fontsize=12)\n",
    "    axes[0].set_ylabel('Accuracy (%)', fontsize=12)\n",
    "    axes[0].set_title('ViT Population Performance', fontsize=14, fontweight='bold')\n",
    "    axes[0].legend(fontsize=11)\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "    # Plot 2: Individual member performance\n",
    "    for member in population:\n",
    "        axes[1].plot(range(1, len(member.performance_history) + 1),\n",
    "                    member.performance_history,\n",
    "                    linewidth=1.5,\n",
    "                    alpha=0.7,\n",
    "                    label=f'M{member.member_id} (BS={member.batch_size})')\n",
    "    axes[1].set_xlabel('Epoch', fontsize=12)\n",
    "    axes[1].set_ylabel('Accuracy (%)', fontsize=12)\n",
    "    axes[1].set_title('Individual ViT Member Performance', fontsize=14, fontweight='bold')\n",
    "    axes[1].legend(fontsize=10, ncol=2)\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "    # Plot 3: Learning rate evolution\n",
    "    axes[2].clear()\n",
    "\n",
    "    for i, member in enumerate(population):\n",
    "        if len(member.lr_history) > 0:\n",
    "          # Plot actual recorded LR history\n",
    "          epochs_to_plot = range(1, len(member.lr_history) + 1)\n",
    "          axes[2].semilogy(epochs_to_plot, member.lr_history,\n",
    "                           linewidth=1.5, alpha=0.7,\n",
    "                           label=f'Member {i} (BS={member.batch_size})',\n",
    "                           marker='o', markersize=3)\n",
    "        else:\n",
    "          # Fallback: plot constant LR (shouldn't happen with tracking) THIS IS IN CASE THIS TRACKING FAILS\n",
    "          epochs = range(1, len(member.performance_history) + 1)\n",
    "          lrs = [member.hyperparams['lr']] * len(epochs)  # Simplified - LR changes per epoch\n",
    "          axes[2].semilogy(epochs, lrs, linewidth=1.5, alpha=0.7, label=f'Member {i} (no history)')\n",
    "    axes[2].set_xlabel('Epoch', fontsize=12)\n",
    "    axes[2].set_ylabel('Learning Rate', fontsize=12)\n",
    "    axes[2].set_title('Actual Learning Rate Evolution', fontsize=14, fontweight='bold')\n",
    "    axes[2].legend(fontsize=10, ncol=2)\n",
    "    axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "\n",
    "\n",
    "    # Add annotation for LR changes due to PBT\n",
    "    pbt_update_epochs = []\n",
    "    for epoch_idx, update_time in enumerate(timer.metrics['update_times']):\n",
    "        if update_time > 0:  # PBT update occurred\n",
    "            pbt_update_epochs.append(epoch_idx + 1)\n",
    "\n",
    "    for update_epoch in pbt_update_epochs:\n",
    "        axes[2].axvline(x=update_epoch, color='red', alpha=0.3, linestyle='--', linewidth=1)\n",
    "\n",
    "    if pbt_update_epochs:\n",
    "        axes[2].text(0.02, 0.98, 'Red dashes: PBT updates',\n",
    "                    transform=axes[2].transAxes, fontsize=9,\n",
    "                    verticalalignment='top',\n",
    "                    bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Plot 4: Batch size evolution\n",
    "    batch_history = timer.metrics['batch_size_history']\n",
    "    for i in range(len(population)):\n",
    "        batches = [epoch_batches[i] for epoch_batches in batch_history]\n",
    "        axes[3].plot(epochs, batches, linewidth=2, marker='o', markersize=4, label=f'Member {i}')\n",
    "    axes[3].set_xlabel('Epoch', fontsize=12)\n",
    "    axes[3].set_ylabel('Batch Size', fontsize=12)\n",
    "    axes[3].set_title('Batch Size Evolution', fontsize=14, fontweight='bold')\n",
    "    axes[3].set_yticks([64, 128, 256])\n",
    "    axes[3].legend(fontsize=10, ncol=2)\n",
    "    axes[3].grid(True, alpha=0.3)\n",
    "\n",
    "    # Plot 5: Epoch timing\n",
    "    epoch_times = timer.metrics['epoch_times']\n",
    "    axes[4].plot(epochs, epoch_times, 'g-', linewidth=2, marker='s', markersize=5)\n",
    "    axes[4].set_xlabel('Epoch', fontsize=12)\n",
    "    axes[4].set_ylabel('Time (seconds)', fontsize=12)\n",
    "    axes[4].set_title('Epoch Duration', fontsize=14, fontweight='bold')\n",
    "    axes[4].grid(True, alpha=0.3)\n",
    "    axes[4].fill_between(epochs, 0, epoch_times, alpha=0.3, color='green')\n",
    "\n",
    "    # Plot 6: Member training time distribution\n",
    "    member_avg_times = []\n",
    "    for member_id in sorted(timer.metrics['member_details'].keys()):\n",
    "        avg_time = np.mean(timer.metrics['member_details'][member_id])\n",
    "        member_avg_times.append(avg_time)\n",
    "\n",
    "    bars = axes[5].bar(range(len(member_avg_times)), member_avg_times, color='skyblue')\n",
    "    axes[5].set_xlabel('Member ID', fontsize=12)\n",
    "    axes[5].set_ylabel('Average Training Time (seconds)', fontsize=12)\n",
    "    axes[5].set_title('Average Training Time per Member', fontsize=14, fontweight='bold')\n",
    "    axes[5].set_xticks(range(len(member_avg_times)))\n",
    "    axes[5].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "    # Add value labels on bars\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        axes[5].text(bar.get_x() + bar.get_width()/2., height,\n",
    "                    f'{height:.1f}s', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "    # Plot 7: Batch size vs Training time\n",
    "    batch_times = []\n",
    "    for epoch_idx, batch_sizes in enumerate(timer.metrics['batch_size_history']):\n",
    "        for member_idx, bs in enumerate(batch_sizes):\n",
    "            batch_times.append((bs, timer.metrics['member_times'][epoch_idx][member_idx]))\n",
    "\n",
    "    # Group by batch size\n",
    "    batch_stats = {}\n",
    "    for bs, t in batch_times:\n",
    "        if bs not in batch_stats:\n",
    "            batch_stats[bs] = []\n",
    "        batch_stats[bs].append(t)\n",
    "\n",
    "    batch_sizes_list = sorted(batch_stats.keys())\n",
    "    avg_times = [np.mean(batch_stats[bs]) for bs in batch_sizes_list]\n",
    "\n",
    "    bars2 = axes[6].bar(range(len(batch_sizes_list)), avg_times, color='lightcoral')\n",
    "    axes[6].set_xlabel('Batch Size', fontsize=12)\n",
    "    axes[6].set_ylabel('Average Training Time (seconds)', fontsize=12)\n",
    "    axes[6].set_title('Batch Size vs Training Time', fontsize=14, fontweight='bold')\n",
    "    axes[6].set_xticks(range(len(batch_sizes_list)))\n",
    "    axes[6].set_xticklabels([str(bs) for bs in batch_sizes_list])\n",
    "    axes[6].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "    # Add value labels on bars\n",
    "    for bar in bars2:\n",
    "        height = bar.get_height()\n",
    "        axes[6].text(bar.get_x() + bar.get_width()/2., height,\n",
    "                    f'{height:.1f}s', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "    # Plot 8: Runtime breakdown\n",
    "    labels = ['Member Training', 'Population Updates', 'Other']\n",
    "    training_total = sum([sum(t) for t in timer.metrics['member_times']])\n",
    "    update_total = sum(timer.metrics['update_times'])\n",
    "    other_total = timer.get_total_time() - training_total - update_total\n",
    "\n",
    "    sizes = [training_total, update_total, other_total]\n",
    "    colors = ['#ff9999', '#66b3ff', '#99ff99']\n",
    "\n",
    "    axes[7].pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%',\n",
    "                startangle=90, textprops={'fontsize': 10})\n",
    "    axes[7].set_title('Runtime Breakdown', fontsize=14, fontweight='bold')\n",
    "\n",
    "    # Plot 9: Final hyperparameter distribution\n",
    "    final_lrs = [m.hyperparams['lr'] for m in population]\n",
    "    final_batches = [m.hyperparams['batch_size'] for m in population]\n",
    "    final_perf = [m.current_performance for m in population]\n",
    "\n",
    "    scatter = axes[8].scatter(final_lrs, final_batches,\n",
    "                             c=final_perf, cmap='viridis', s=200, alpha=0.8, edgecolors='k')\n",
    "    axes[8].set_xscale('log')\n",
    "    axes[8].set_xlabel('Learning Rate', fontsize=12)\n",
    "    axes[8].set_ylabel('Batch Size', fontsize=12)\n",
    "    axes[8].set_title('Final Hyperparameters vs Performance', fontsize=14, fontweight='bold')\n",
    "    axes[8].set_yticks([64, 128, 256])\n",
    "    plt.colorbar(scatter, ax=axes[8], label='Accuracy (%)')\n",
    "    axes[8].grid(True, alpha=0.3)\n",
    "\n",
    "    plt.suptitle('PBT for ViT-Small on CIFAR-10 with Timing Analysis',\n",
    "                fontsize=16, fontweight='bold', y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('pbt_vit_timing_results.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "    # Also create a separate timing visualization\n",
    "    plot_timing_details(timer)\n",
    "\n",
    "def plot_timing_details(timer):\n",
    "    \"\"\"Create detailed timing visualization\"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "    # Plot 1: Cumulative time over epochs\n",
    "    cumulative_time = np.cumsum(timer.metrics['epoch_times'])\n",
    "    epochs = range(1, len(cumulative_time) + 1)\n",
    "\n",
    "    axes[0, 0].plot(epochs, cumulative_time, 'b-', linewidth=2)\n",
    "    axes[0, 0].fill_between(epochs, 0, cumulative_time, alpha=0.3)\n",
    "    axes[0, 0].set_xlabel('Epoch', fontsize=12)\n",
    "    axes[0, 0].set_ylabel('Cumulative Time (seconds)', fontsize=12)\n",
    "    axes[0, 0].set_title('Cumulative Runtime', fontsize=14, fontweight='bold')\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "    # Plot 2: Member time progression\n",
    "    for member_id in sorted(timer.metrics['member_details'].keys()):\n",
    "        times = timer.metrics['member_details'][member_id]\n",
    "        axes[0, 1].plot(range(1, len(times) + 1), times, linewidth=1.5,\n",
    "                       label=f'Member {member_id}')\n",
    "    axes[0, 1].set_xlabel('Epoch', fontsize=12)\n",
    "    axes[0, 1].set_ylabel('Training Time (seconds)', fontsize=12)\n",
    "    axes[0, 1].set_title('Member Training Time Progression', fontsize=14, fontweight='bold')\n",
    "    axes[0, 1].legend(fontsize=10, ncol=2)\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "    # Plot 3: Batch size vs time scatter\n",
    "    all_batch_times = []\n",
    "    for epoch_idx, batch_sizes in enumerate(timer.metrics['batch_size_history']):\n",
    "        for member_idx, bs in enumerate(batch_sizes):\n",
    "            all_batch_times.append((bs, timer.metrics['member_times'][epoch_idx][member_idx]))\n",
    "\n",
    "    batch_sizes_scatter = [bt[0] for bt in all_batch_times]\n",
    "    times_scatter = [bt[1] for bt in all_batch_times]\n",
    "\n",
    "    scatter = axes[1, 0].scatter(batch_sizes_scatter, times_scatter,\n",
    "                                alpha=0.6, s=50, c='purple')\n",
    "    axes[1, 0].set_xlabel('Batch Size', fontsize=12)\n",
    "    axes[1, 0].set_ylabel('Training Time (seconds)', fontsize=12)\n",
    "    axes[1, 0].set_title('Batch Size vs Training Time (All Samples)', fontsize=14, fontweight='bold')\n",
    "    axes[1, 0].set_xticks([64, 128, 256])\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "    # Plot 4: Update time when it occurs\n",
    "    update_epochs = [i+1 for i, t in enumerate(timer.metrics['update_times']) if t > 0]\n",
    "    update_times_nonzero = [t for t in timer.metrics['update_times'] if t > 0]\n",
    "\n",
    "    if update_times_nonzero:\n",
    "        bars = axes[1, 1].bar(update_epochs, update_times_nonzero, color='orange', alpha=0.7)\n",
    "        axes[1, 1].set_xlabel('Epoch', fontsize=12)\n",
    "        axes[1, 1].set_ylabel('Update Time (seconds)', fontsize=12)\n",
    "        axes[1, 1].set_title('Population Update Times', fontsize=14, fontweight='bold')\n",
    "        axes[1, 1].set_xticks(update_epochs)\n",
    "        axes[1, 1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "        # Add value labels\n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            axes[1, 1].text(bar.get_x() + bar.get_width()/2., height,\n",
    "                           f'{height:.2f}s', ha='center', va='bottom', fontsize=9)\n",
    "    else:\n",
    "        axes[1, 1].text(0.5, 0.5, 'No population updates occurred',\n",
    "                       ha='center', va='center', fontsize=12, transform=axes[1, 1].transAxes)\n",
    "        axes[1, 1].set_title('Population Update Times', fontsize=14, fontweight='bold')\n",
    "\n",
    "    plt.suptitle('Detailed Timing Analysis', fontsize=16, fontweight='bold', y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('pbt_detailed_timing.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "def plot_detailed_lr_evolution(population, timer, history):\n",
    "    \"\"\"Create detailed LR evolution plot showing all changes\"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(14, 10))\n",
    "\n",
    "    # Plot 1: Individual member LR evolution\n",
    "    colors = plt.cm.Set3(np.linspace(0, 1, len(population)))\n",
    "\n",
    "    for i, member in enumerate(population):\n",
    "        if len(member.lr_history) > 0:\n",
    "            epochs = range(1, len(member.lr_history) + 1)\n",
    "\n",
    "            # Plot line\n",
    "            ax1.semilogy(epochs, member.lr_history,\n",
    "                        color=colors[i], linewidth=2,\n",
    "                        label=f'M{i} (BS={member.batch_size})',\n",
    "                        alpha=0.8)\n",
    "\n",
    "            # Mark PBT update points (where LR might jump)\n",
    "            for epoch_idx, lr in enumerate(member.lr_history):\n",
    "                if epoch_idx > 0 and member.lr_history[epoch_idx-1] != lr:\n",
    "                    # Check if this was a PBT update epoch\n",
    "                    if (epoch_idx + 1) in [ep+1 for ep, t in enumerate(timer.metrics['update_times']) if t > 0]:\n",
    "                        ax1.plot(epoch_idx + 1, lr, 'r*', markersize=10, markeredgewidth=1, markeredgecolor='black')\n",
    "                    else:\n",
    "                        ax1.plot(epoch_idx + 1, lr, 'ko', markersize=5, alpha=0.5)\n",
    "\n",
    "    ax1.set_xlabel('Epoch', fontsize=12)\n",
    "    ax1.set_ylabel('Learning Rate', fontsize=12)\n",
    "    ax1.set_title('Detailed Learning Rate Evolution with Changes Marked',\n",
    "                 fontsize=14, fontweight='bold')\n",
    "    ax1.legend(fontsize=10, ncol=2, loc='upper right')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "\n",
    "    # Add explanation\n",
    "    ax1.text(0.02, 0.02, 'Red stars: PBT updates\\nBlack dots: Cosine annealing',\n",
    "            transform=ax1.transAxes, fontsize=10,\n",
    "            bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "\n",
    "    # Plot 2: LR distribution over time (box plot per epoch)\n",
    "    max_epochs = max([len(m.lr_history) for m in population if len(m.lr_history) > 0])\n",
    "\n",
    "    if max_epochs > 0:\n",
    "        # Collect LRs per epoch\n",
    "        lrs_per_epoch = []\n",
    "        for epoch in range(max_epochs):\n",
    "            epoch_lrs = []\n",
    "            for member in population:\n",
    "                if len(member.lr_history) > epoch:\n",
    "                    epoch_lrs.append(member.lr_history[epoch])\n",
    "            lrs_per_epoch.append(epoch_lrs)\n",
    "\n",
    "        # Create box plot\n",
    "        positions = range(1, max_epochs + 1)\n",
    "        box_data = [np.log10(np.array(epoch_lrs)) for epoch_lrs in lrs_per_epoch]\n",
    "\n",
    "        bp = ax2.boxplot(box_data, positions=positions, widths=0.6,\n",
    "                        patch_artist=True, showfliers=False)\n",
    "\n",
    "        # Color boxes\n",
    "        for patch in bp['boxes']:\n",
    "            patch.set_facecolor('lightblue')\n",
    "            patch.set_alpha(0.7)\n",
    "\n",
    "        ax2.set_xlabel('Epoch', fontsize=12)\n",
    "        ax2.set_ylabel('log10(Learning Rate)', fontsize=12)\n",
    "        ax2.set_title('Population Learning Rate Distribution Over Time',\n",
    "                     fontsize=14, fontweight='bold')\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "\n",
    "        # Mark PBT update epochs\n",
    "        for update_epoch in [ep+1 for ep, t in enumerate(timer.metrics['update_times']) if t > 0]:\n",
    "            if update_epoch <= max_epochs:\n",
    "                ax2.axvline(x=update_epoch, color='red', alpha=0.5,\n",
    "                           linestyle='--', linewidth=1)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('pbt_lr_evolution_detailed.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 87,
     "status": "ok",
     "timestamp": 1764849314631,
     "user": {
      "displayName": "Etaash Tripathi Patel",
      "userId": "11287900455267433447"
     },
     "user_tz": 480
    },
    "id": "b0wjNLc7xkrc",
    "outputId": "3eeb2e0c-fa61-494c-fd95-5e58c7a39979"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defined save_text_to_pdf function.\n"
     ]
    }
   ],
   "source": [
    "from reportlab.pdfgen import canvas\n",
    "from reportlab.lib.pagesizes import A4\n",
    "import sys\n",
    "import io\n",
    "import contextlib # Import contextlib\n",
    "\n",
    "def save_text_to_pdf(text_content: str, file_path: str):\n",
    "    \"\"\"\n",
    "    Saves given text content to a PDF file using ReportLab.\n",
    "\n",
    "    Args:\n",
    "        text_content (str): The text content to write to the PDF.\n",
    "        file_path (str): The path where the PDF file will be saved.\n",
    "    \"\"\"\n",
    "    # Create a PDF document\n",
    "    c = canvas.Canvas(file_path, pagesize=A4)\n",
    "    width, height = A4\n",
    "\n",
    "    # Set font\n",
    "    font_name = 'Helvetica'\n",
    "    font_size = 10\n",
    "    line_height = font_size + 2 # Add some padding\n",
    "    c.setFont(font_name, font_size)\n",
    "\n",
    "    # Margins\n",
    "    left_margin = 50\n",
    "    top_margin = height - 50\n",
    "    current_y = top_margin\n",
    "\n",
    "    # Split content into lines and draw\n",
    "    lines = text_content.split('\\n')\n",
    "    for line in lines:\n",
    "        if current_y < 50: # Check if new page is needed\n",
    "            c.showPage()\n",
    "            c.setFont(font_name, font_size)\n",
    "            current_y = top_margin\n",
    "\n",
    "        c.drawString(left_margin, current_y, line)\n",
    "        current_y -= line_height\n",
    "\n",
    "    # Save the PDF\n",
    "    c.save()\n",
    "    print(f\"PDF successfully saved to: {file_path}\")\n",
    "\n",
    "print(\"Defined save_text_to_pdf function.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bc74cafc"
   },
   "outputs": [],
   "source": [
    "def one_seed_run(seed):\n",
    "    # Initialize a StringIO buffer to capture all standard output and error\n",
    "    output_buffer = io.StringIO()\n",
    "\n",
    "    # Redirect sys.stdout and sys.stderr to the buffer\n",
    "    with contextlib.redirect_stdout(output_buffer), contextlib.redirect_stderr(output_buffer):\n",
    "        print(\"Starting PBT ViT Experiment with Timing...\")\n",
    "        print(\"Note: This will train ViT-Small models on CIFAR-10\")\n",
    "        print(\"Each member may have different batch sizes\\n\")\n",
    "\n",
    "        # Define a seed for reproducibility and output file naming\n",
    "        experiment_seed = seed\n",
    "        output_filename_pdf = f\"pbt_experiment_output_seed_{experiment_seed}.pdf\"\n",
    "\n",
    "        # Mount Google Drive to ensure the output can be saved\n",
    "        mount_drive_if_not_mounted()\n",
    "\n",
    "        pbt, history, timer = run_pbt_vit_experiment(experiment_seed)\n",
    "\n",
    "        # These final messages are part of the experiment log, so they should be in the PDF\n",
    "        print(f\"\\nExperiment completed successfully!\")\n",
    "        print(f\"Results saved to: pbt_vit_timing_results.png\")\n",
    "        print(f\"Detailed timing saved to: pbt_detailed_timing.png\")\n",
    "\n",
    "    # After the experiment, the original stdout/stderr are restored.\n",
    "\n",
    "    # Define the output file path in Google Drive\n",
    "    pbt_config_instance = PBTConfig(seed=experiment_seed)\n",
    "    output_dir = os.path.join(pbt_config_instance.drive_base_path, \"pbt_checkpoints\")\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    output_filepath_pdf = os.path.join(output_dir, output_filename_pdf)\n",
    "\n",
    "    # Save the captured output to a PDF file\n",
    "    save_text_to_pdf(output_buffer.getvalue(), output_filepath_pdf)\n",
    "\n",
    "    # This message should appear in Colab's standard output, not in the saved PDF file.\n",
    "    print(f\"Terminal output saved to: {output_filepath_pdf}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "output_embedded_package_id": "1RcEKhrvrT6F4SKuLe48iN8lPcD7XChRK"
    },
    "executionInfo": {
     "elapsed": 17245366,
     "status": "ok",
     "timestamp": 1764904904878,
     "user": {
      "displayName": "Etaash Tripathi Patel",
      "userId": "11287900455267433447"
     },
     "user_tz": 480
    },
    "id": "wTIRPLsxxt0D",
    "outputId": "5c990c72-f70a-47f9-8888-1c6c995ff346"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Output hidden; open in https://colab.research.google.com to view."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "  for seed in [38042, 217401, 637451, 207796, 45921]:\n",
    "    one_seed_run(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "302ac6cd"
   },
   "source": [
    "## Final Task\n",
    "\n",
    "### Subtask:\n",
    "Confirm that the changes have been applied and explain how to view the generated PDF files.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f93d44d8"
   },
   "source": [
    "## Summary:\n",
    "\n",
    "### Q&A\n",
    "*   **Confirm that the changes have been applied and explain how to view the generated PDF files.**\n",
    "    Yes, the changes have been successfully applied. The `one_seed_run` function has been modified to capture standard output and errors, and then save this captured text into a PDF file in Google Drive using the newly defined `save_text_to_pdf` function. To view the generated PDF files, you would navigate to your mounted Google Drive, specifically within the `pbt_checkpoints` directory (e.g., `/{your_drive_mount_path}/pbt_checkpoints/`) which will contain files named like `pbt_experiment_output_seed_X.pdf`, where `X` is the seed used for the run.\n",
    "\n",
    "### Data Analysis Key Findings\n",
    "*   The `reportlab` library, specifically version `reportlab-4.4.5`, was successfully installed to enable PDF generation.\n",
    "*   A helper function `save_text_to_pdf` was defined, capable of taking text content and a file path, then generating a multi-page PDF document with `Helvetica` font size 10, including logic for automatic page breaks when content exceeds page height.\n",
    "*   The `one_seed_run` function was successfully refactored to replace `rich.Console` HTML output with PDF output. It now uses `contextlib.redirect_stdout` and `contextlib.redirect_stderr` to capture all console output into an in-memory buffer.\n",
    "*   The captured output is then passed to the `save_text_to_pdf` function, saving the experiment logs as a PDF file within a dynamically created `pbt_checkpoints` directory in Google Drive.\n",
    "\n",
    "### Insights or Next Steps\n",
    "*   The implemented solution provides a robust way to persist detailed experiment logs in an easily shareable and readable PDF format directly within Google Drive, which is beneficial for documentation and reproducibility.\n",
    "*   The next step is to execute the `one_seed_run` function with various seeds to generate actual PDF files and visually confirm their content and accessibility within Google Drive.\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMXVoB3Lz20ZR0QQFMLtULV",
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
