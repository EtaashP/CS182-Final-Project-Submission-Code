{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d87d53b5",
   "metadata": {},
   "source": [
    "# Convert PBT WD-ablation PDF log â†’ machine-readable CSV + Excel\n",
    "\n",
    "This notebook parses a **PBT training log saved as a PDF** and exports:\n",
    "- `results/pbt_wd_ablation_parsed.csv`\n",
    "- `results/pbt_wd_ablation_parsed.xlsx`\n",
    "\n",
    "Parsing rules (conservative / robust to noisy PDF text):\n",
    "- **LR schedule** is parsed *only* from lines like `LR changed during epoch: a -> b`.\n",
    "- **Initial hyperparameters** per member are parsed from the 5-line block under  \n",
    "  `Hyperparameteres for model k at epoch 1`.\n",
    "- **Hyperparameters are updated only** by the PBT update lines like  \n",
    "  `Member k: weight_decay changed from ... to ...` under `--- Population Update (Epoch u) ---`.\n",
    "- Updates are applied starting at epoch `u+1`, and hyperparameters are **backfilled** between updates.\n",
    "\n",
    "> Note: This PDF prints `Hyperparameteres for model ...` at every epoch, but we intentionally **do not rely**\n",
    "> on those later prints (they can be noisy in PDF text). We reconstruct the piecewise-constant trajectory from\n",
    "> the explicit update lines + initial settings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fec7df60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: ../Raw Outputs/PBT/Full Logs/pbt_wd_ablation_experiment_output_seed_38042.pdf\n",
      "CSV: ../Structured Outputs/PBT/pbt_wd_ablation_parsed.csv\n",
      "XLSX: ../Structured Outputs/PBT/pbt_wd_ablation_parsed.xlsx\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# =============================================================================\n",
    "# CONFIGURATION\n",
    "# =============================================================================\n",
    "\n",
    "# Global variable: relative directory where outputs will be written.\n",
    "from pathlib import Path\n",
    "\n",
    "# =============================================================================\n",
    "# CONFIGURATION\n",
    "# =============================================================================\n",
    "\n",
    "# Global variable: relative directory where outputs will be written.\n",
    "CSV_REL_DIR = \"../Structured Outputs/PBT/\"\n",
    "\n",
    "'''# Input PDFs (seed is inferred from filename substring like 'seed_38042')\n",
    "COMMON_PATH = Path(\"../Raw Outputs/PBT/Full Logs/\")\n",
    "INPUT_LOG_PATHS = [\n",
    "    COMMON_PATH / \"pbt_batchsize_ablation_experiment_output_seed_38042.pdf\",\n",
    "    COMMON_PATH / \"pbt_batchsize_ablation_experiment_output_seed_217401.pdf\",\n",
    "]\n",
    "\n",
    "# Output filenames (written inside CSV_REL_DIR)\n",
    "OUTPUT_CSV_NAME = \"pbt_batchsize_pdfs_parsed.csv\"\n",
    "OUTPUT_XLSX_NAME = \"pbt_batchsize_pdfs_parsed.xlsx\"\n",
    "\n",
    "OUTPUT_DIR = Path(CSV_REL_DIR)\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "OUTPUT_CSV_PATH = OUTPUT_DIR / OUTPUT_CSV_NAME\n",
    "OUTPUT_XLSX_PATH = OUTPUT_DIR / OUTPUT_XLSX_NAME\n",
    "\n",
    "print(\"Inputs:\", INPUT_LOG_PATHS)\n",
    "print(\"CSV:\", OUTPUT_CSV_PATH)\n",
    "print(\"XLSX:\", OUTPUT_XLSX_PATH)\n",
    "\n",
    "CSV_REL_DIR = \"results\"'''\n",
    "\n",
    "# Input PDF (seed inferred from filename like 'seed_38042')\n",
    "INPUT_PDF_PATH = Path(\"../Raw Outputs/PBT/Full Logs/pbt_wd_ablation_experiment_output_seed_38042.pdf\")\n",
    "\n",
    "# Outputs (written inside CSV_REL_DIR)\n",
    "OUTPUT_CSV_NAME = \"pbt_wd_ablation_parsed.csv\"\n",
    "OUTPUT_XLSX_NAME = \"pbt_wd_ablation_parsed.xlsx\"\n",
    "\n",
    "OUTPUT_DIR = Path(CSV_REL_DIR)\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "OUTPUT_CSV_PATH = OUTPUT_DIR / OUTPUT_CSV_NAME\n",
    "OUTPUT_XLSX_PATH = OUTPUT_DIR / OUTPUT_XLSX_NAME\n",
    "\n",
    "print(\"Input:\", INPUT_PDF_PATH)\n",
    "print(\"CSV:\", OUTPUT_CSV_PATH)\n",
    "print(\"XLSX:\", OUTPUT_XLSX_PATH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e917ab1d",
   "metadata": {},
   "source": [
    "## 1) PDF text extraction (PyMuPDF preferred, pdfplumber fallback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a8922f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "def extract_pdf_text(path: Path) -> str:\n",
    "    \"\"\"Extract text from a PDF using PyMuPDF if available, else pdfplumber.\"\"\"\n",
    "    try:\n",
    "        import fitz  # PyMuPDF\n",
    "        doc = fitz.open(str(path))\n",
    "        parts = [page.get_text(\"text\") for page in doc]\n",
    "        doc.close()\n",
    "        return \"\\n\".join(parts)\n",
    "    except Exception:\n",
    "        import pdfplumber\n",
    "        parts = []\n",
    "        with pdfplumber.open(str(path)) as pdf:\n",
    "            for page in pdf.pages:\n",
    "                parts.append(page.extract_text() or \"\")\n",
    "        return \"\\n\".join(parts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb0b3cd",
   "metadata": {},
   "source": [
    "## 2) Regex patterns (conservative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c3a745f",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED_RE = re.compile(r\"seed[_=](\\d+)\", re.IGNORECASE)\n",
    "\n",
    "EPOCH_HDR = re.compile(r\"\\bEpoch\\s+(?P<epoch>\\d+)\\s*/\\s*(?P<epoch_total>\\d+)\\b\")\n",
    "TRAIN_MEMBER = re.compile(r\"---\\s*Training\\s+Member\\s+(?P<member>\\d+)\\s*\\(Batch\\s+size:\\s*(?P<bs>\\d+)\\)\\s*---\")\n",
    "\n",
    "# LR schedule ONLY from this pattern:\n",
    "LR_CHANGED = re.compile(r\"LR\\s+changed\\s+during\\s+epoch:\\s*(?P<start>[-+0-9.eE]+)\\s*->\\s*(?P<end>[-+0-9.eE]+)\")\n",
    "\n",
    "LOSS = re.compile(r\"Loss:\\s*(?P<loss>[-+0-9.eE]+)\")\n",
    "TRAIN_ACC = re.compile(r\"Train\\s+Accuracy:\\s*(?P<acc>[-+0-9.]+)\\s*%\")\n",
    "TEST_ACC = re.compile(r\"Test\\s+Accuracy:\\s*(?P<acc>[-+0-9.]+)\\s*%\")\n",
    "\n",
    "BUILT = re.compile(r\"built\\s+data\\s+in\\s+(?P<t>[-+0-9.eE]+)\\s+seconds\")\n",
    "TRAIN_TIME = re.compile(r\"total\\s+runtime\\s+to\\s+train\\s+this\\s+model\\s+was\\s+(?P<t>[-+0-9.eE]+)\\s+seconds\")\n",
    "EVAL_TIME = re.compile(r\"evaluation\\s+in\\s+(?P<t>[-+0-9.eE]+)\\s+seconds\")\n",
    "\n",
    "TOTAL_EPOCHS = re.compile(r\"Total\\s+epochs:\\s*(?P<t>\\d+)\")\n",
    "EXPLOIT_INTERVAL = re.compile(r\"Exploit\\s+interval:\\s*(?P<t>\\d+)\\s+epochs\")\n",
    "\n",
    "HYPER_HDR = re.compile(r\"Hyperparameteres\\s+for\\s+model\\s+(?P<member>\\d+)\\s+at\\s+epoch\\s+(?P<epoch>\\d+)\", re.IGNORECASE)\n",
    "HP_LINE = re.compile(r\"^(?P<k>lr|weight_decay|drop_path|warmup_epochs|batch_size)\\s*:\\s*(?P<v>[-+0-9.eE]+)\\s*$\", re.IGNORECASE)\n",
    "\n",
    "POP_UPDATE = re.compile(r\"---\\s*Population\\s+Update\\s+\\(Epoch\\s+(?P<epoch>\\d+)\\)\\s*---\")\n",
    "CHANGE_LINE = re.compile(\n",
    "    r\"Member\\s+(?P<member>\\d+):\\s*(?P<param>lr|weight_decay|drop_path|batch_size)\\s+changed\\s+from\\s+(?P<old>[-+0-9.eE]+)\\s+to\\s+(?P<new>[-+0-9.eE]+)\",\n",
    "    re.IGNORECASE,\n",
    ")\n",
    "COPIED_LINE = re.compile(r\"Member\\s+(?P<member>\\d+)\\s+copied\\s+from\\s+(?P<src>\\d+)\", re.IGNORECASE)\n",
    "\n",
    "POST_LINE = re.compile(\n",
    "    r\"LR=(?P<lr>[-+0-9.eE]+),\\s*WD=(?P<wd>[-+0-9.eE]+),\\s*DropPath=(?P<dp>[-+0-9.eE]+),\\s*Warmup=(?P<warm>\\d+)\\s*epochs,\\s*Batch=(?P<bs>\\d+)\",\n",
    "    re.IGNORECASE,\n",
    ")\n",
    "\n",
    "SUMMARY_HDR = re.compile(r\"Epoch\\s+(?P<epoch>\\d+)\\s+Summary:\", re.IGNORECASE)\n",
    "SUMMARY_TIME = re.compile(r\"Time:\\s*(?P<time>[-+0-9.]+)s\\s*\\(Avg\\s+member:\\s*(?P<avg>[-+0-9.]+)s\\)\")\n",
    "POP_MEAN_ACC = re.compile(r\"Population\\s+Mean\\s+Accuracy:\\s*(?P<acc>[-+0-9.]+)\\s*%\")\n",
    "BEST_MEMBER_ACC = re.compile(r\"Best\\s+Member\\s+Accuracy:\\s*(?P<acc>[-+0-9.]+)\\s*%\")\n",
    "MEAN_BS = re.compile(r\"Mean\\s+Batch\\s+Size:\\s*(?P<bs>\\d+)\")\n",
    "MEAN_LR = re.compile(r\"Mean\\s+Learning\\s+Rate:\\s*(?P<lr>[-+0-9.eE]+)\")\n",
    "MEAN_WD = re.compile(r\"Mean\\s+Weight\\s+Decay:\\s*(?P<wd>[-+0-9.eE]+)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6bc5d76",
   "metadata": {},
   "source": [
    "## 3) Parse the PDF into tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "308fbdd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seed</th>\n",
       "      <th>epoch</th>\n",
       "      <th>epoch_total</th>\n",
       "      <th>member</th>\n",
       "      <th>train_batch_size</th>\n",
       "      <th>data_build_s</th>\n",
       "      <th>lr_sched_start</th>\n",
       "      <th>lr_sched_end</th>\n",
       "      <th>train_time_s</th>\n",
       "      <th>eval_time_s</th>\n",
       "      <th>loss</th>\n",
       "      <th>train_acc_pct</th>\n",
       "      <th>test_acc_pct</th>\n",
       "      <th>pbt_lr</th>\n",
       "      <th>pbt_weight_decay</th>\n",
       "      <th>pbt_drop_path</th>\n",
       "      <th>pbt_warmup_epochs</th>\n",
       "      <th>pbt_batch_size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>38042</td>\n",
       "      <td>1</td>\n",
       "      <td>70</td>\n",
       "      <td>0</td>\n",
       "      <td>256</td>\n",
       "      <td>7.242754</td>\n",
       "      <td>1.930000e-07</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>25.363722</td>\n",
       "      <td>2.166985</td>\n",
       "      <td>2.0628</td>\n",
       "      <td>22.78</td>\n",
       "      <td>30.97</td>\n",
       "      <td>0.000189</td>\n",
       "      <td>0.292952</td>\n",
       "      <td>0.307589</td>\n",
       "      <td>5</td>\n",
       "      <td>256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>38042</td>\n",
       "      <td>1</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>256</td>\n",
       "      <td>1.598430</td>\n",
       "      <td>2.660000e-07</td>\n",
       "      <td>0.000053</td>\n",
       "      <td>24.825141</td>\n",
       "      <td>3.064337</td>\n",
       "      <td>2.0400</td>\n",
       "      <td>23.80</td>\n",
       "      <td>31.20</td>\n",
       "      <td>0.000261</td>\n",
       "      <td>0.435120</td>\n",
       "      <td>0.298885</td>\n",
       "      <td>5</td>\n",
       "      <td>256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38042</td>\n",
       "      <td>1</td>\n",
       "      <td>70</td>\n",
       "      <td>2</td>\n",
       "      <td>256</td>\n",
       "      <td>1.541124</td>\n",
       "      <td>4.750000e-07</td>\n",
       "      <td>0.000094</td>\n",
       "      <td>25.769480</td>\n",
       "      <td>2.314755</td>\n",
       "      <td>2.0106</td>\n",
       "      <td>25.24</td>\n",
       "      <td>31.66</td>\n",
       "      <td>0.000466</td>\n",
       "      <td>0.083222</td>\n",
       "      <td>0.217043</td>\n",
       "      <td>5</td>\n",
       "      <td>256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>38042</td>\n",
       "      <td>1</td>\n",
       "      <td>70</td>\n",
       "      <td>3</td>\n",
       "      <td>256</td>\n",
       "      <td>1.719979</td>\n",
       "      <td>3.220000e-07</td>\n",
       "      <td>0.000063</td>\n",
       "      <td>25.652009</td>\n",
       "      <td>3.868828</td>\n",
       "      <td>2.0363</td>\n",
       "      <td>23.82</td>\n",
       "      <td>29.31</td>\n",
       "      <td>0.000316</td>\n",
       "      <td>0.339831</td>\n",
       "      <td>0.206325</td>\n",
       "      <td>5</td>\n",
       "      <td>256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>38042</td>\n",
       "      <td>1</td>\n",
       "      <td>70</td>\n",
       "      <td>4</td>\n",
       "      <td>256</td>\n",
       "      <td>1.585088</td>\n",
       "      <td>3.640000e-07</td>\n",
       "      <td>0.000072</td>\n",
       "      <td>26.570324</td>\n",
       "      <td>2.736207</td>\n",
       "      <td>2.0207</td>\n",
       "      <td>24.57</td>\n",
       "      <td>30.69</td>\n",
       "      <td>0.000357</td>\n",
       "      <td>0.433634</td>\n",
       "      <td>0.254329</td>\n",
       "      <td>5</td>\n",
       "      <td>256</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    seed  epoch  epoch_total  member  train_batch_size  data_build_s  \\\n",
       "0  38042      1           70       0               256      7.242754   \n",
       "1  38042      1           70       1               256      1.598430   \n",
       "2  38042      1           70       2               256      1.541124   \n",
       "3  38042      1           70       3               256      1.719979   \n",
       "4  38042      1           70       4               256      1.585088   \n",
       "\n",
       "   lr_sched_start  lr_sched_end  train_time_s  eval_time_s    loss  \\\n",
       "0    1.930000e-07      0.000038     25.363722     2.166985  2.0628   \n",
       "1    2.660000e-07      0.000053     24.825141     3.064337  2.0400   \n",
       "2    4.750000e-07      0.000094     25.769480     2.314755  2.0106   \n",
       "3    3.220000e-07      0.000063     25.652009     3.868828  2.0363   \n",
       "4    3.640000e-07      0.000072     26.570324     2.736207  2.0207   \n",
       "\n",
       "   train_acc_pct  test_acc_pct    pbt_lr  pbt_weight_decay  pbt_drop_path  \\\n",
       "0          22.78         30.97  0.000189          0.292952       0.307589   \n",
       "1          23.80         31.20  0.000261          0.435120       0.298885   \n",
       "2          25.24         31.66  0.000466          0.083222       0.217043   \n",
       "3          23.82         29.31  0.000316          0.339831       0.206325   \n",
       "4          24.57         30.69  0.000357          0.433634       0.254329   \n",
       "\n",
       "   pbt_warmup_epochs  pbt_batch_size  \n",
       "0                  5             256  \n",
       "1                  5             256  \n",
       "2                  5             256  \n",
       "3                  5             256  \n",
       "4                  5             256  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def parse_seed_from_name(p: Path) -> int:\n",
    "    m = SEED_RE.search(p.name)\n",
    "    if not m:\n",
    "        raise ValueError(f\"Could not infer seed from filename: {p.name}\")\n",
    "    return int(m.group(1))\n",
    "\n",
    "def parse_pbt_pdf(path: Path) -> dict:\n",
    "    seed = parse_seed_from_name(path)\n",
    "    text = extract_pdf_text(path)\n",
    "    lines = [ln.strip() for ln in text.splitlines() if ln.strip()]\n",
    "\n",
    "    total_epochs = int(TOTAL_EPOCHS.search(text).group(\"t\")) if TOTAL_EPOCHS.search(text) else None\n",
    "    exploit_interval = int(EXPLOIT_INTERVAL.search(text).group(\"t\")) if EXPLOIT_INTERVAL.search(text) else None\n",
    "\n",
    "    # (1) Initial hyperparams: take the first \"Hyperparameteres for model k ...\" block per member.\n",
    "    initial_hp = {}\n",
    "    i = 0\n",
    "    while i < len(lines):\n",
    "        m = HYPER_HDR.search(lines[i])\n",
    "        if m:\n",
    "            member = int(m.group(\"member\"))\n",
    "            hp = {}\n",
    "            for j in range(1, 12):\n",
    "                if i + j >= len(lines): break\n",
    "                mm = HP_LINE.match(lines[i + j])\n",
    "                if mm:\n",
    "                    k = mm.group(\"k\").lower()\n",
    "                    v = float(mm.group(\"v\"))\n",
    "                    if k in (\"warmup_epochs\",\"batch_size\"):\n",
    "                        v = int(round(v))\n",
    "                    hp[k] = v\n",
    "                if {\"lr\",\"weight_decay\",\"drop_path\",\"warmup_epochs\",\"batch_size\"}.issubset(hp.keys()):\n",
    "                    break\n",
    "            if member not in initial_hp and hp:\n",
    "                initial_hp[member] = hp\n",
    "        i += 1\n",
    "\n",
    "    # (2) Per-epoch metrics + update events\n",
    "    records = []\n",
    "    change_events, copy_events, post_events = [], [], []\n",
    "    summaries = []\n",
    "\n",
    "    current_epoch = None\n",
    "    current_epoch_total = None\n",
    "    current_block = None\n",
    "    current_update_epoch = None\n",
    "    in_summary = False\n",
    "\n",
    "    for ln in lines:\n",
    "        m = EPOCH_HDR.search(ln)\n",
    "        if m and \"Summary\" not in ln:\n",
    "            current_epoch = int(m.group(\"epoch\"))\n",
    "            current_epoch_total = int(m.group(\"epoch_total\"))\n",
    "            in_summary = False\n",
    "\n",
    "        m = TRAIN_MEMBER.search(ln)\n",
    "        if m:\n",
    "            current_block = {\n",
    "                \"seed\": seed,\n",
    "                \"epoch\": current_epoch,\n",
    "                \"epoch_total\": current_epoch_total,\n",
    "                \"member\": int(m.group(\"member\")),\n",
    "                \"train_batch_size\": int(m.group(\"bs\")),\n",
    "            }\n",
    "            continue\n",
    "\n",
    "        if current_block is not None:\n",
    "            m = BUILT.search(ln)\n",
    "            if m: current_block[\"data_build_s\"] = float(m.group(\"t\"))\n",
    "            m = LR_CHANGED.search(ln)\n",
    "            if m:\n",
    "                # LR schedule ONLY from this line type\n",
    "                current_block[\"lr_sched_start\"] = float(m.group(\"start\"))\n",
    "                current_block[\"lr_sched_end\"] = float(m.group(\"end\"))\n",
    "            m = TRAIN_TIME.search(ln)\n",
    "            if m: current_block[\"train_time_s\"] = float(m.group(\"t\"))\n",
    "            m = EVAL_TIME.search(ln)\n",
    "            if m: current_block[\"eval_time_s\"] = float(m.group(\"t\"))\n",
    "            m = LOSS.search(ln)\n",
    "            if m: current_block[\"loss\"] = float(m.group(\"loss\"))\n",
    "            m = TRAIN_ACC.search(ln)\n",
    "            if m: current_block[\"train_acc_pct\"] = float(m.group(\"acc\"))\n",
    "            m = TEST_ACC.search(ln)\n",
    "            if m:\n",
    "                current_block[\"test_acc_pct\"] = float(m.group(\"acc\"))\n",
    "                records.append(current_block)\n",
    "                current_block = None\n",
    "            continue\n",
    "\n",
    "        m = POP_UPDATE.search(ln)\n",
    "        if m:\n",
    "            current_update_epoch = int(m.group(\"epoch\"))\n",
    "            continue\n",
    "        m = CHANGE_LINE.search(ln)\n",
    "        if m:\n",
    "            change_events.append({\n",
    "                \"seed\": seed,\n",
    "                \"update_epoch\": current_update_epoch,\n",
    "                \"member\": int(m.group(\"member\")),\n",
    "                \"param\": m.group(\"param\").lower(),\n",
    "                \"old\": float(m.group(\"old\")),\n",
    "                \"new\": float(m.group(\"new\")),\n",
    "            })\n",
    "            continue\n",
    "        m = COPIED_LINE.search(ln)\n",
    "        if m:\n",
    "            copy_events.append({\n",
    "                \"seed\": seed,\n",
    "                \"update_epoch\": current_update_epoch,\n",
    "                \"member\": int(m.group(\"member\")),\n",
    "                \"copied_from\": int(m.group(\"src\")),\n",
    "            })\n",
    "            continue\n",
    "        m = POST_LINE.search(ln)\n",
    "        if m:\n",
    "            post_events.append({\n",
    "                \"seed\": seed,\n",
    "                \"update_epoch\": current_update_epoch,\n",
    "                \"lr\": float(m.group(\"lr\")),\n",
    "                \"weight_decay\": float(m.group(\"wd\")),\n",
    "                \"drop_path\": float(m.group(\"dp\")),\n",
    "                \"warmup_epochs\": int(m.group(\"warm\")),\n",
    "                \"batch_size\": int(m.group(\"bs\")),\n",
    "            })\n",
    "            continue\n",
    "\n",
    "        m = SUMMARY_HDR.search(ln)\n",
    "        if m:\n",
    "            summaries.append({\"seed\": seed, \"epoch\": int(m.group(\"epoch\"))})\n",
    "            in_summary = True\n",
    "            continue\n",
    "        if in_summary and summaries:\n",
    "            cur = summaries[-1]\n",
    "            m = SUMMARY_TIME.search(ln)\n",
    "            if m:\n",
    "                cur[\"epoch_time_s\"] = float(m.group(\"time\"))\n",
    "                cur[\"avg_member_time_s\"] = float(m.group(\"avg\"))\n",
    "            m = POP_MEAN_ACC.search(ln)\n",
    "            if m: cur[\"pop_mean_acc_pct\"] = float(m.group(\"acc\"))\n",
    "            m = BEST_MEMBER_ACC.search(ln)\n",
    "            if m: cur[\"best_member_acc_pct\"] = float(m.group(\"acc\"))\n",
    "            m = MEAN_BS.search(ln)\n",
    "            if m: cur[\"mean_batch_size\"] = int(m.group(\"bs\"))\n",
    "            m = MEAN_LR.search(ln)\n",
    "            if m: cur[\"mean_lr\"] = float(m.group(\"lr\"))\n",
    "            m = MEAN_WD.search(ln)\n",
    "            if m: cur[\"mean_weight_decay\"] = float(m.group(\"wd\"))\n",
    "\n",
    "    df_metrics = pd.DataFrame(records)\n",
    "    df_changes = pd.DataFrame(change_events)\n",
    "    df_copies = pd.DataFrame(copy_events)\n",
    "    df_post = pd.DataFrame(post_events)\n",
    "    df_summary = pd.DataFrame(summaries)\n",
    "\n",
    "    if total_epochs is None and len(df_metrics):\n",
    "        total_epochs = int(df_metrics[\"epoch\"].max())\n",
    "\n",
    "    # (3) Reconstruct hyperparams:\n",
    "    # Base = initial_hp, then apply changes at update_epoch u for epochs > u.\n",
    "    members = sorted(df_metrics[\"member\"].unique().tolist()) if len(df_metrics) else sorted(initial_hp.keys())\n",
    "    params = [\"lr\",\"weight_decay\",\"drop_path\",\"warmup_epochs\",\"batch_size\"]\n",
    "\n",
    "    change_map = {}\n",
    "    if len(df_changes):\n",
    "        for (mem, par), g in df_changes.groupby([\"member\",\"param\"]):\n",
    "            change_map[(int(mem), str(par))] = sorted(\n",
    "                [(int(u), float(n)) for u, n in zip(g[\"update_epoch\"], g[\"new\"])],\n",
    "                key=lambda x: x[0],\n",
    "            )\n",
    "\n",
    "    hp_rows = []\n",
    "    for mem in members:\n",
    "        base = initial_hp.get(mem, {})\n",
    "        for ep in range(1, total_epochs + 1):\n",
    "            row = {\"seed\": seed, \"epoch\": ep, \"member\": mem}\n",
    "            for par in params:\n",
    "                val = base.get(par)\n",
    "                for u, newv in change_map.get((mem, par), []):\n",
    "                    if ep > u:\n",
    "                        val = newv\n",
    "                    else:\n",
    "                        break\n",
    "                if par in (\"warmup_epochs\",\"batch_size\") and val is not None:\n",
    "                    val = int(round(val))\n",
    "                row[f\"pbt_{par}\"] = val\n",
    "            hp_rows.append(row)\n",
    "\n",
    "    df_hp = pd.DataFrame(hp_rows)\n",
    "    df_main = df_metrics.merge(df_hp, on=[\"seed\",\"epoch\",\"member\"], how=\"left\") if len(df_metrics) else df_hp\n",
    "    if \"train_batch_size\" in df_main.columns:\n",
    "        df_main[\"pbt_batch_size\"] = df_main[\"pbt_batch_size\"].fillna(df_main[\"train_batch_size\"])\n",
    "\n",
    "    return df_main, df_changes, df_copies, df_post, df_summary\n",
    "\n",
    "df_main, df_changes, df_copies, df_post, df_summary = parse_pbt_pdf(INPUT_PDF_PATH)\n",
    "df_main.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f25c2aa",
   "metadata": {},
   "source": [
    "## 4) Export CSV + Excel (with leaderboard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f7f6fab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote: ../Structured Outputs/PBT/pbt_wd_ablation_parsed.csv\n",
      "Wrote: ../Structured Outputs/PBT/pbt_wd_ablation_parsed.xlsx\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rank</th>\n",
       "      <th>seed</th>\n",
       "      <th>epoch</th>\n",
       "      <th>epoch_total</th>\n",
       "      <th>member</th>\n",
       "      <th>train_batch_size</th>\n",
       "      <th>data_build_s</th>\n",
       "      <th>lr_sched_start</th>\n",
       "      <th>lr_sched_end</th>\n",
       "      <th>train_time_s</th>\n",
       "      <th>eval_time_s</th>\n",
       "      <th>loss</th>\n",
       "      <th>train_acc_pct</th>\n",
       "      <th>test_acc_pct</th>\n",
       "      <th>pbt_lr</th>\n",
       "      <th>pbt_weight_decay</th>\n",
       "      <th>pbt_drop_path</th>\n",
       "      <th>pbt_warmup_epochs</th>\n",
       "      <th>pbt_batch_size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>38042</td>\n",
       "      <td>69</td>\n",
       "      <td>70</td>\n",
       "      <td>2</td>\n",
       "      <td>256</td>\n",
       "      <td>1.908095</td>\n",
       "      <td>2.080000e-06</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>24.330950</td>\n",
       "      <td>3.479003</td>\n",
       "      <td>0.3420</td>\n",
       "      <td>87.98</td>\n",
       "      <td>79.46</td>\n",
       "      <td>0.000466</td>\n",
       "      <td>0.083222</td>\n",
       "      <td>0.217043</td>\n",
       "      <td>5</td>\n",
       "      <td>256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>38042</td>\n",
       "      <td>70</td>\n",
       "      <td>70</td>\n",
       "      <td>2</td>\n",
       "      <td>256</td>\n",
       "      <td>1.753601</td>\n",
       "      <td>1.270000e-06</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>24.591279</td>\n",
       "      <td>3.234564</td>\n",
       "      <td>0.3463</td>\n",
       "      <td>87.67</td>\n",
       "      <td>79.40</td>\n",
       "      <td>0.000466</td>\n",
       "      <td>0.083222</td>\n",
       "      <td>0.217043</td>\n",
       "      <td>5</td>\n",
       "      <td>256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>38042</td>\n",
       "      <td>66</td>\n",
       "      <td>70</td>\n",
       "      <td>2</td>\n",
       "      <td>256</td>\n",
       "      <td>1.723484</td>\n",
       "      <td>7.740000e-06</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>25.304451</td>\n",
       "      <td>2.629037</td>\n",
       "      <td>0.3488</td>\n",
       "      <td>87.62</td>\n",
       "      <td>79.35</td>\n",
       "      <td>0.000466</td>\n",
       "      <td>0.083222</td>\n",
       "      <td>0.217043</td>\n",
       "      <td>5</td>\n",
       "      <td>256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>38042</td>\n",
       "      <td>61</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>256</td>\n",
       "      <td>1.618949</td>\n",
       "      <td>2.940000e-07</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>25.630297</td>\n",
       "      <td>2.290167</td>\n",
       "      <td>0.3906</td>\n",
       "      <td>86.03</td>\n",
       "      <td>79.34</td>\n",
       "      <td>0.000288</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.344676</td>\n",
       "      <td>5</td>\n",
       "      <td>256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>38042</td>\n",
       "      <td>67</td>\n",
       "      <td>70</td>\n",
       "      <td>2</td>\n",
       "      <td>256</td>\n",
       "      <td>1.762107</td>\n",
       "      <td>5.320000e-06</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>24.696815</td>\n",
       "      <td>3.206165</td>\n",
       "      <td>0.3471</td>\n",
       "      <td>87.63</td>\n",
       "      <td>79.31</td>\n",
       "      <td>0.000466</td>\n",
       "      <td>0.083222</td>\n",
       "      <td>0.217043</td>\n",
       "      <td>5</td>\n",
       "      <td>256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>38042</td>\n",
       "      <td>64</td>\n",
       "      <td>70</td>\n",
       "      <td>2</td>\n",
       "      <td>256</td>\n",
       "      <td>1.601513</td>\n",
       "      <td>1.420000e-05</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>26.066447</td>\n",
       "      <td>2.329392</td>\n",
       "      <td>0.3530</td>\n",
       "      <td>87.49</td>\n",
       "      <td>79.29</td>\n",
       "      <td>0.000466</td>\n",
       "      <td>0.083222</td>\n",
       "      <td>0.217043</td>\n",
       "      <td>5</td>\n",
       "      <td>256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>38042</td>\n",
       "      <td>63</td>\n",
       "      <td>70</td>\n",
       "      <td>2</td>\n",
       "      <td>256</td>\n",
       "      <td>1.608932</td>\n",
       "      <td>1.810000e-05</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>25.692919</td>\n",
       "      <td>2.289454</td>\n",
       "      <td>0.3581</td>\n",
       "      <td>87.23</td>\n",
       "      <td>79.28</td>\n",
       "      <td>0.000466</td>\n",
       "      <td>0.083222</td>\n",
       "      <td>0.217043</td>\n",
       "      <td>5</td>\n",
       "      <td>256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>38042</td>\n",
       "      <td>60</td>\n",
       "      <td>70</td>\n",
       "      <td>2</td>\n",
       "      <td>256</td>\n",
       "      <td>1.608280</td>\n",
       "      <td>3.300000e-05</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>25.643334</td>\n",
       "      <td>2.217817</td>\n",
       "      <td>0.3736</td>\n",
       "      <td>86.58</td>\n",
       "      <td>79.26</td>\n",
       "      <td>0.000466</td>\n",
       "      <td>0.083222</td>\n",
       "      <td>0.217043</td>\n",
       "      <td>5</td>\n",
       "      <td>256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>38042</td>\n",
       "      <td>61</td>\n",
       "      <td>70</td>\n",
       "      <td>2</td>\n",
       "      <td>256</td>\n",
       "      <td>1.603621</td>\n",
       "      <td>2.760000e-05</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>25.282570</td>\n",
       "      <td>2.289982</td>\n",
       "      <td>0.3648</td>\n",
       "      <td>86.97</td>\n",
       "      <td>79.26</td>\n",
       "      <td>0.000466</td>\n",
       "      <td>0.083222</td>\n",
       "      <td>0.217043</td>\n",
       "      <td>5</td>\n",
       "      <td>256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>38042</td>\n",
       "      <td>68</td>\n",
       "      <td>70</td>\n",
       "      <td>2</td>\n",
       "      <td>256</td>\n",
       "      <td>1.774074</td>\n",
       "      <td>3.430000e-06</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>25.192173</td>\n",
       "      <td>3.032083</td>\n",
       "      <td>0.3427</td>\n",
       "      <td>87.73</td>\n",
       "      <td>79.26</td>\n",
       "      <td>0.000466</td>\n",
       "      <td>0.083222</td>\n",
       "      <td>0.217043</td>\n",
       "      <td>5</td>\n",
       "      <td>256</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rank   seed  epoch  epoch_total  member  train_batch_size  data_build_s  \\\n",
       "0     1  38042     69           70       2               256      1.908095   \n",
       "1     2  38042     70           70       2               256      1.753601   \n",
       "2     3  38042     66           70       2               256      1.723484   \n",
       "3     4  38042     61           70       1               256      1.618949   \n",
       "4     5  38042     67           70       2               256      1.762107   \n",
       "5     6  38042     64           70       2               256      1.601513   \n",
       "6     7  38042     63           70       2               256      1.608932   \n",
       "7     8  38042     60           70       2               256      1.608280   \n",
       "8     9  38042     61           70       2               256      1.603621   \n",
       "9    10  38042     68           70       2               256      1.774074   \n",
       "\n",
       "   lr_sched_start  lr_sched_end  train_time_s  eval_time_s    loss  \\\n",
       "0    2.080000e-06      0.000001     24.330950     3.479003  0.3420   \n",
       "1    1.270000e-06      0.000001     24.591279     3.234564  0.3463   \n",
       "2    7.740000e-06      0.000005     25.304451     2.629037  0.3488   \n",
       "3    2.940000e-07      0.000058     25.630297     2.290167  0.3906   \n",
       "4    5.320000e-06      0.000003     24.696815     3.206165  0.3471   \n",
       "5    1.420000e-05      0.000011     26.066447     2.329392  0.3530   \n",
       "6    1.810000e-05      0.000014     25.692919     2.289454  0.3581   \n",
       "7    3.300000e-05      0.000028     25.643334     2.217817  0.3736   \n",
       "8    2.760000e-05      0.000023     25.282570     2.289982  0.3648   \n",
       "9    3.430000e-06      0.000002     25.192173     3.032083  0.3427   \n",
       "\n",
       "   train_acc_pct  test_acc_pct    pbt_lr  pbt_weight_decay  pbt_drop_path  \\\n",
       "0          87.98         79.46  0.000466          0.083222       0.217043   \n",
       "1          87.67         79.40  0.000466          0.083222       0.217043   \n",
       "2          87.62         79.35  0.000466          0.083222       0.217043   \n",
       "3          86.03         79.34  0.000288          0.500000       0.344676   \n",
       "4          87.63         79.31  0.000466          0.083222       0.217043   \n",
       "5          87.49         79.29  0.000466          0.083222       0.217043   \n",
       "6          87.23         79.28  0.000466          0.083222       0.217043   \n",
       "7          86.58         79.26  0.000466          0.083222       0.217043   \n",
       "8          86.97         79.26  0.000466          0.083222       0.217043   \n",
       "9          87.73         79.26  0.000466          0.083222       0.217043   \n",
       "\n",
       "   pbt_warmup_epochs  pbt_batch_size  \n",
       "0                  5             256  \n",
       "1                  5             256  \n",
       "2                  5             256  \n",
       "3                  5             256  \n",
       "4                  5             256  \n",
       "5                  5             256  \n",
       "6                  5             256  \n",
       "7                  5             256  \n",
       "8                  5             256  \n",
       "9                  5             256  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def top_snapshots(df_in: pd.DataFrame, topn=200) -> pd.DataFrame:\n",
    "    s = df_in.dropna(subset=[\"test_acc_pct\"]).copy()\n",
    "    s = s.sort_values([\"test_acc_pct\",\"epoch\"], ascending=[False, True]).reset_index(drop=True)\n",
    "    s.insert(0,\"rank\", s.index + 1)\n",
    "    return s.head(topn)\n",
    "\n",
    "# CSV\n",
    "df_main.to_csv(OUTPUT_CSV_PATH, index=False)\n",
    "\n",
    "# Excel\n",
    "lb = top_snapshots(df_main, 200)\n",
    "\n",
    "with pd.ExcelWriter(OUTPUT_XLSX_PATH, engine=\"openpyxl\") as writer:\n",
    "    df_main.to_excel(writer, sheet_name=\"epoch_member_metrics\", index=False)\n",
    "    if len(df_summary): df_summary.sort_values([\"epoch\"]).to_excel(writer, sheet_name=\"epoch_summary\", index=False)\n",
    "    if len(df_changes): df_changes.sort_values([\"update_epoch\",\"member\",\"param\"]).to_excel(writer, sheet_name=\"hyperparam_changes\", index=False)\n",
    "    if len(df_copies): df_copies.sort_values([\"update_epoch\",\"member\"]).to_excel(writer, sheet_name=\"copy_events\", index=False)\n",
    "    if len(df_post): df_post.sort_values([\"update_epoch\"]).to_excel(writer, sheet_name=\"post_update_lines\", index=False)\n",
    "    lb.to_excel(writer, sheet_name=\"leaderboard_top200\", index=False)\n",
    "\n",
    "print(\"Wrote:\", OUTPUT_CSV_PATH)\n",
    "print(\"Wrote:\", OUTPUT_XLSX_PATH)\n",
    "lb.head(10)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
