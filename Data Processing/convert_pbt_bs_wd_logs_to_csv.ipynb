{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ad11414",
   "metadata": {},
   "source": [
    "# Convert multiple PBT logs → machine-readable CSV + Excel workbook\n",
    "\n",
    "This notebook handles **multiple PBT log files** and **adds a `seed` column** inferred from the filename\n",
    "(e.g., `bs+wd_seed_38042.log` → seed `38042`).\n",
    "\n",
    "For each seed/log, we parse:\n",
    "- per-epoch/per-member metrics\n",
    "- PBT update events (hyperparameter changes + copy events)\n",
    "- optional post-update hyperparameter lines\n",
    "- population-level epoch summaries\n",
    "\n",
    "Then we reconstruct **per-epoch PBT hyperparameters** (since they change over time), concatenate all seeds,\n",
    "and export:\n",
    "- `results/pbt_bs_wd_parsed.csv`\n",
    "- `results/pbt_bs_wd_parsed.xlsx` (with leaderboards, plus per-seed top-200 sheets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "182aad87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will read: [PosixPath('../Raw Outputs/PBT/Full Logs/bs+wd_seed_38042.log'), PosixPath('../Raw Outputs/PBT/Full Logs/bs+wd_seed_217401.log')]\n",
      "Will write CSV: ../Structured Outputs/PBT/pbt_bs_wd_parsed.csv\n",
      "Will write XLSX: ../Structured Outputs/PBT/pbt_bs_wd_parsed.xlsx\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# =============================================================================\n",
    "# CONFIGURATION\n",
    "# =============================================================================\n",
    "\n",
    "# Global variable: relative directory where outputs will be written.\n",
    "CSV_REL_DIR = \"../Structured Outputs/PBT/\"\n",
    "\n",
    "# Provide one or more PBT log files here. Seed is inferred from the filename.\n",
    "# Example names: bs+wd_seed_38042.log, bs+wd_seed_217401.log\n",
    "COMMON_PATH = Path(\"../Raw Outputs/PBT/Full Logs/\")\n",
    "INPUT_LOG_PATHS = [\n",
    "    COMMON_PATH / \"bs+wd_seed_38042.log\",\n",
    "    COMMON_PATH / \"bs+wd_seed_217401.log\",\n",
    "]\n",
    "\n",
    "# Output names (written inside CSV_REL_DIR)\n",
    "OUTPUT_CSV_NAME = \"pbt_bs_wd_parsed.csv\"\n",
    "OUTPUT_XLSX_NAME = \"pbt_bs_wd_parsed.xlsx\"\n",
    "\n",
    "# Derived paths\n",
    "OUTPUT_DIR = Path(CSV_REL_DIR)\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "OUTPUT_CSV_PATH = OUTPUT_DIR / OUTPUT_CSV_NAME\n",
    "OUTPUT_XLSX_PATH = OUTPUT_DIR / OUTPUT_XLSX_NAME\n",
    "\n",
    "print(\"Will read:\", INPUT_LOG_PATHS)\n",
    "print(\"Will write CSV:\", OUTPUT_CSV_PATH)\n",
    "print(\"Will write XLSX:\", OUTPUT_XLSX_PATH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f9b2a74",
   "metadata": {},
   "source": [
    "## Parser implementation\n",
    "\n",
    "We implement a function `parse_pbt_log(path)` which:\n",
    "1. Infers `seed` from the filename (`seed_XXXXX`).\n",
    "2. Parses:\n",
    "   - `(epoch, member)` metrics\n",
    "   - PBT hyperparameter change events (`old -> new`) per member and per update epoch\n",
    "   - copy events\n",
    "   - population summaries\n",
    "3. Reconstructs per-epoch PBT hyperparameters (`pbt_lr`, `pbt_weight_decay`, `pbt_drop_path`, `pbt_batch_size`)\n",
    "   because PBT mutates hyperparameters over time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee98e7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# ----------------------------\n",
    "# Regex patterns\n",
    "# ----------------------------\n",
    "EPOCH_HEADER_RE = re.compile(r\"Epoch\\s+(?P<epoch>\\d+)\\s*/\\s*(?P<epoch_total>\\d+)\")\n",
    "TRAIN_MEMBER_RE = re.compile(r\"---\\s*Training Member\\s+(?P<member>\\d+)\\s+\\(Batch size:\\s+(?P<bs>\\d+)\\)\\s+---\")\n",
    "\n",
    "LR_SCHED_RE = re.compile(r\"LR changed during epoch:\\s+(?P<start>[-+0-9.eE]+)\\s+->\\s+(?P<end>[-+0-9.eE]+)\")\n",
    "LOSS_RE = re.compile(r\"Loss:\\s+(?P<loss>[-+0-9.eE]+)\")\n",
    "TRAIN_ACC_RE = re.compile(r\"Train Accuracy:\\s+(?P<acc>[-+0-9.]+)%\")\n",
    "TEST_ACC_RE = re.compile(r\"Test Accuracy:\\s+(?P<acc>[-+0-9.]+)%\")\n",
    "\n",
    "BUILT_RE = re.compile(r\"built data in\\s+(?P<t>[-+0-9.eE]+)\\s+seconds\")\n",
    "TRAIN_TIME_RE = re.compile(r\"total runtime to train this model was\\s+(?P<t>[-+0-9.eE]+)\\s+seconds\")\n",
    "EVAL_TIME_RE = re.compile(r\"evaluation in\\s+(?P<t>[-+0-9.eE]+)\\s+seconds\")\n",
    "\n",
    "POP_UPDATE_RE = re.compile(r\"--- Population Update \\(Epoch\\s+(?P<epoch>\\d+)\\)\\s+---\")\n",
    "MEMBER_CHANGE_RE = re.compile(r\"Member\\s+(?P<member>\\d+):\\s+(?P<param>lr|weight_decay|drop_path|batch_size)\\s+changed from\\s+(?P<old>[-+0-9.eE]+)\\s+to\\s+(?P<new>[-+0-9.eE]+)\")\n",
    "MEMBER_COPIED_RE = re.compile(r\"Member\\s+(?P<member>\\d+)\\s+copied from\\s+(?P<src>\\d+)\")\n",
    "\n",
    "POST_LINE_RE = re.compile(r\"LR=(?P<lr>[-+0-9.eE]+),\\s*WD=(?P<wd>[-+0-9.eE]+),\\s*DropPath=(?P<dp>[-+0-9.eE]+).*Batch=(?P<bs>\\d+)\")\n",
    "\n",
    "SUMMARY_START_RE = re.compile(r\"Epoch\\s+(?P<epoch>\\d+)\\s+Summary:\")\n",
    "SUMMARY_TIME_RE = re.compile(r\"Time:\\s+(?P<time>[-+0-9.]+)s.*Avg member:\\s+(?P<avg>[-+0-9.]+)s\")\n",
    "POP_MEAN_ACC_RE = re.compile(r\"Population Mean Accuracy:\\s+(?P<acc>[-+0-9.]+)%\")\n",
    "BEST_MEMBER_ACC_RE = re.compile(r\"Best Member Accuracy:\\s+(?P<acc>[-+0-9.]+)%\")\n",
    "MEAN_BS_RE = re.compile(r\"Mean Batch Size:\\s+(?P<bs>\\d+)\")\n",
    "MEAN_LR_RE = re.compile(r\"Mean Learning Rate:\\s+(?P<lr>[-+0-9.eE]+)\")\n",
    "MEAN_WD_RE = re.compile(r\"Mean Weight Decay:\\s+(?P<wd>[-+0-9.eE]+)\")\n",
    "\n",
    "SEED_FROM_NAME_RE = re.compile(r\"seed[_=](?P<seed>\\d+)\", re.IGNORECASE)\n",
    "\n",
    "def parse_seed_from_filename(p: Path) -> int:\n",
    "    m = SEED_FROM_NAME_RE.search(p.name)\n",
    "    if not m:\n",
    "        raise ValueError(f\"Could not infer seed from filename: {p.name}\")\n",
    "    return int(m.group(\"seed\"))\n",
    "\n",
    "def parse_pbt_log(path: Path) -> dict:\n",
    "    seed = parse_seed_from_filename(path)\n",
    "    text = path.read_text(encoding=\"utf-8\", errors=\"replace\")\n",
    "    lines = text.splitlines()\n",
    "\n",
    "    m = re.search(r\"Total epochs:\\s*(\\d+)\", text)\n",
    "    total_epochs = int(m.group(1)) if m else None\n",
    "\n",
    "    epoch_member_records = []\n",
    "    epoch_summary_records = []\n",
    "    hyperparam_change_records = []\n",
    "    copy_records = []\n",
    "    post_update_records = []\n",
    "\n",
    "    current_epoch = None\n",
    "    current_epoch_total = None\n",
    "    current_update_epoch = None\n",
    "    last_changed_member = None\n",
    "    current_block = {}\n",
    "\n",
    "    def start_new_member_block(epoch, epoch_total, member, bs):\n",
    "        return {\"seed\": seed, \"epoch\": epoch, \"epoch_total\": epoch_total, \"member\": member, \"train_batch_size\": bs}\n",
    "\n",
    "    for line in lines:\n",
    "        m = EPOCH_HEADER_RE.search(line)\n",
    "        if m and \"Summary\" not in line:\n",
    "            current_epoch = int(m.group(\"epoch\"))\n",
    "            current_epoch_total = int(m.group(\"epoch_total\"))\n",
    "            continue\n",
    "\n",
    "        m = TRAIN_MEMBER_RE.search(line)\n",
    "        if m:\n",
    "            current_block = start_new_member_block(current_epoch, current_epoch_total, int(m.group(\"member\")), int(m.group(\"bs\")))\n",
    "            continue\n",
    "\n",
    "        if current_block:\n",
    "            m = BUILT_RE.search(line)\n",
    "            if m: current_block[\"data_build_s\"] = float(m.group(\"t\"))\n",
    "            m = LR_SCHED_RE.search(line)\n",
    "            if m:\n",
    "                current_block[\"lr_sched_start\"] = float(m.group(\"start\"))\n",
    "                current_block[\"lr_sched_end\"] = float(m.group(\"end\"))\n",
    "            m = TRAIN_TIME_RE.search(line)\n",
    "            if m: current_block[\"train_time_s\"] = float(m.group(\"t\"))\n",
    "            m = EVAL_TIME_RE.search(line)\n",
    "            if m: current_block[\"eval_time_s\"] = float(m.group(\"t\"))\n",
    "            m = LOSS_RE.search(line)\n",
    "            if m: current_block[\"loss\"] = float(m.group(\"loss\"))\n",
    "            m = TRAIN_ACC_RE.search(line)\n",
    "            if m: current_block[\"train_acc_pct\"] = float(m.group(\"acc\"))\n",
    "            m = TEST_ACC_RE.search(line)\n",
    "            if m:\n",
    "                current_block[\"test_acc_pct\"] = float(m.group(\"acc\"))\n",
    "                epoch_member_records.append(current_block.copy())\n",
    "                current_block = {}\n",
    "            continue\n",
    "\n",
    "        m = POP_UPDATE_RE.search(line)\n",
    "        if m:\n",
    "            current_update_epoch = int(m.group(\"epoch\"))\n",
    "            continue\n",
    "\n",
    "        m = MEMBER_CHANGE_RE.search(line)\n",
    "        if m:\n",
    "            hyperparam_change_records.append({\n",
    "                \"seed\": seed, \"update_epoch\": current_update_epoch, \"member\": int(m.group(\"member\")),\n",
    "                \"param\": m.group(\"param\"), \"old\": float(m.group(\"old\")), \"new\": float(m.group(\"new\")),\n",
    "            })\n",
    "            last_changed_member = int(m.group(\"member\"))\n",
    "            continue\n",
    "\n",
    "        m = MEMBER_COPIED_RE.search(line)\n",
    "        if m:\n",
    "            copy_records.append({\"seed\": seed, \"update_epoch\": current_update_epoch, \"member\": int(m.group(\"member\")), \"copied_from\": int(m.group(\"src\"))})\n",
    "            last_changed_member = int(m.group(\"member\"))\n",
    "            continue\n",
    "\n",
    "        m = POST_LINE_RE.search(line)\n",
    "        if m:\n",
    "            post_update_records.append({\n",
    "                \"seed\": seed, \"update_epoch\": current_update_epoch, \"member\": last_changed_member,\n",
    "                \"lr\": float(m.group(\"lr\")), \"weight_decay\": float(m.group(\"wd\")),\n",
    "                \"drop_path\": float(m.group(\"dp\")), \"batch_size\": int(m.group(\"bs\")),\n",
    "            })\n",
    "            continue\n",
    "\n",
    "        m = SUMMARY_START_RE.search(line)\n",
    "        if m:\n",
    "            epoch_summary_records.append({\"seed\": seed, \"epoch\": int(m.group(\"epoch\"))})\n",
    "            continue\n",
    "\n",
    "        if epoch_summary_records:\n",
    "            cur = epoch_summary_records[-1]\n",
    "            m = SUMMARY_TIME_RE.search(line)\n",
    "            if m: cur[\"epoch_time_s\"] = float(m.group(\"time\")); cur[\"avg_member_time_s\"] = float(m.group(\"avg\"))\n",
    "            m = POP_MEAN_ACC_RE.search(line)\n",
    "            if m: cur[\"pop_mean_acc_pct\"] = float(m.group(\"acc\"))\n",
    "            m = BEST_MEMBER_ACC_RE.search(line)\n",
    "            if m: cur[\"best_member_acc_pct\"] = float(m.group(\"acc\"))\n",
    "            m = MEAN_BS_RE.search(line)\n",
    "            if m: cur[\"mean_batch_size\"] = int(m.group(\"bs\"))\n",
    "            m = MEAN_LR_RE.search(line)\n",
    "            if m: cur[\"mean_lr\"] = float(m.group(\"lr\"))\n",
    "            m = MEAN_WD_RE.search(line)\n",
    "            if m: cur[\"mean_weight_decay\"] = float(m.group(\"wd\"))\n",
    "\n",
    "    df_epochs = pd.DataFrame(epoch_member_records)\n",
    "    df_changes = pd.DataFrame(hyperparam_change_records)\n",
    "    df_copies = pd.DataFrame(copy_records)\n",
    "    df_post = pd.DataFrame(post_update_records)\n",
    "    df_summary = pd.DataFrame(epoch_summary_records)\n",
    "\n",
    "    if total_epochs is None and len(df_epochs):\n",
    "        total_epochs = int(df_epochs[\"epoch\"].max())\n",
    "\n",
    "    # Reconstruct per-epoch hyperparameters (piecewise-constant, using change events)\n",
    "    members = sorted(df_epochs[\"member\"].unique().tolist()) if len(df_epochs) else []\n",
    "    params = [\"lr\", \"weight_decay\", \"drop_path\", \"batch_size\"]\n",
    "\n",
    "    pbt_rows = []\n",
    "    if total_epochs is not None and len(df_changes):\n",
    "        for member in members:\n",
    "            for param in params:\n",
    "                values = [None] * (total_epochs + 1)\n",
    "                evts = df_changes[(df_changes[\"member\"] == member) & (df_changes[\"param\"] == param)].sort_values(\"update_epoch\")\n",
    "                for _, e in evts.iterrows():\n",
    "                    u = int(e[\"update_epoch\"]); old = float(e[\"old\"]); new = float(e[\"new\"])\n",
    "                    for ep in range(1, u + 1):\n",
    "                        if values[ep] is None: values[ep] = old\n",
    "                    for ep in range(u + 1, total_epochs + 1):\n",
    "                        values[ep] = new\n",
    "                for ep in range(1, total_epochs + 1):\n",
    "                    pbt_rows.append({\"epoch\": ep, \"member\": member, f\"pbt_{param}\": values[ep]})\n",
    "\n",
    "    df_pbt = pd.DataFrame(pbt_rows)\n",
    "    if len(df_pbt):\n",
    "        df_pbt = df_pbt.pivot_table(index=[\"epoch\", \"member\"], values=[c for c in df_pbt.columns if c.startswith(\"pbt_\")], aggfunc=\"first\").reset_index()\n",
    "        df_main = df_epochs.merge(df_pbt, on=[\"epoch\", \"member\"], how=\"left\")\n",
    "    else:\n",
    "        df_main = df_epochs.copy()\n",
    "\n",
    "    # Prefer batch size used during training if mismatch\n",
    "    if \"pbt_batch_size\" in df_main.columns and \"train_batch_size\" in df_main.columns:\n",
    "        mism = df_main[df_main[\"pbt_batch_size\"].notna() & (df_main[\"train_batch_size\"] != df_main[\"pbt_batch_size\"])]\n",
    "        if len(mism):\n",
    "            df_main.loc[mism.index, \"pbt_batch_size\"] = df_main.loc[mism.index, \"train_batch_size\"]\n",
    "\n",
    "    preferred_cols = [\n",
    "        \"seed\", \"epoch\", \"epoch_total\", \"member\",\n",
    "        \"train_batch_size\",\n",
    "        \"pbt_lr\", \"pbt_weight_decay\", \"pbt_drop_path\", \"pbt_batch_size\",\n",
    "        \"lr_sched_start\", \"lr_sched_end\",\n",
    "        \"loss\", \"train_acc_pct\", \"test_acc_pct\",\n",
    "        \"data_build_s\", \"train_time_s\", \"eval_time_s\",\n",
    "    ]\n",
    "    cols = preferred_cols + [c for c in df_main.columns if c not in preferred_cols]\n",
    "    df_main = df_main[cols].sort_values([\"epoch\", \"member\"]).reset_index(drop=True)\n",
    "\n",
    "    return {\"seed\": seed, \"main\": df_main, \"summary\": df_summary, \"changes\": df_changes, \"copies\": df_copies, \"post\": df_post}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5fbd367",
   "metadata": {},
   "source": [
    "## Run parser for all logs, concatenate, and export\n",
    "\n",
    "Outputs:\n",
    "- CSV: `results/pbt_bs_wd_parsed.csv`\n",
    "- XLSX: `results/pbt_bs_wd_parsed.xlsx` with:\n",
    "  - `epoch_member_metrics` (all seeds)\n",
    "  - `epoch_summary`, `hyperparam_changes`, `copy_events`, `post_update_hparams` (all seeds, with seed column)\n",
    "  - `leaderboard_top200` (combined)\n",
    "  - `leaderboard_per_member` (combined)\n",
    "  - `top200_<seed>` (per-seed top 200 snapshots)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92c51413",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seeds parsed: [38042, 217401]\n",
      "Wrote CSV: /Users/etaashpatel/Documents/Final Project/Structured Outputs/PBT/pbt_bs_wd_parsed.csv\n",
      "Wrote XLSX: /Users/etaashpatel/Documents/Final Project/Structured Outputs/PBT/pbt_bs_wd_parsed.xlsx\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seed</th>\n",
       "      <th>epoch</th>\n",
       "      <th>epoch_total</th>\n",
       "      <th>member</th>\n",
       "      <th>train_batch_size</th>\n",
       "      <th>pbt_lr</th>\n",
       "      <th>pbt_weight_decay</th>\n",
       "      <th>pbt_drop_path</th>\n",
       "      <th>pbt_batch_size</th>\n",
       "      <th>lr_sched_start</th>\n",
       "      <th>lr_sched_end</th>\n",
       "      <th>loss</th>\n",
       "      <th>train_acc_pct</th>\n",
       "      <th>test_acc_pct</th>\n",
       "      <th>data_build_s</th>\n",
       "      <th>train_time_s</th>\n",
       "      <th>eval_time_s</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>38042</td>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "      <td>0</td>\n",
       "      <td>64</td>\n",
       "      <td>0.000189</td>\n",
       "      <td>0.196476</td>\n",
       "      <td>0.123035</td>\n",
       "      <td>64.0</td>\n",
       "      <td>4.840000e-08</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>1.8867</td>\n",
       "      <td>29.42</td>\n",
       "      <td>36.69</td>\n",
       "      <td>5.395643</td>\n",
       "      <td>61.999089</td>\n",
       "      <td>11.392097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>38042</td>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "      <td>1</td>\n",
       "      <td>128</td>\n",
       "      <td>0.000261</td>\n",
       "      <td>0.267560</td>\n",
       "      <td>0.119554</td>\n",
       "      <td>128.0</td>\n",
       "      <td>1.340000e-07</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>1.8981</td>\n",
       "      <td>29.23</td>\n",
       "      <td>35.01</td>\n",
       "      <td>1.661272</td>\n",
       "      <td>63.004851</td>\n",
       "      <td>13.567882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38042</td>\n",
       "      <td>2</td>\n",
       "      <td>60</td>\n",
       "      <td>0</td>\n",
       "      <td>64</td>\n",
       "      <td>0.000189</td>\n",
       "      <td>0.196476</td>\n",
       "      <td>0.123035</td>\n",
       "      <td>64.0</td>\n",
       "      <td>3.790000e-05</td>\n",
       "      <td>0.000076</td>\n",
       "      <td>1.6182</td>\n",
       "      <td>40.38</td>\n",
       "      <td>43.57</td>\n",
       "      <td>1.626179</td>\n",
       "      <td>68.211169</td>\n",
       "      <td>13.086053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>38042</td>\n",
       "      <td>2</td>\n",
       "      <td>60</td>\n",
       "      <td>1</td>\n",
       "      <td>128</td>\n",
       "      <td>0.000261</td>\n",
       "      <td>0.267560</td>\n",
       "      <td>0.119554</td>\n",
       "      <td>128.0</td>\n",
       "      <td>5.230000e-05</td>\n",
       "      <td>0.000105</td>\n",
       "      <td>1.6295</td>\n",
       "      <td>40.16</td>\n",
       "      <td>44.72</td>\n",
       "      <td>1.675981</td>\n",
       "      <td>64.784817</td>\n",
       "      <td>13.043758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>38042</td>\n",
       "      <td>3</td>\n",
       "      <td>60</td>\n",
       "      <td>0</td>\n",
       "      <td>64</td>\n",
       "      <td>0.000189</td>\n",
       "      <td>0.196476</td>\n",
       "      <td>0.123035</td>\n",
       "      <td>64.0</td>\n",
       "      <td>7.580000e-05</td>\n",
       "      <td>0.000114</td>\n",
       "      <td>1.4237</td>\n",
       "      <td>47.84</td>\n",
       "      <td>49.85</td>\n",
       "      <td>1.730894</td>\n",
       "      <td>68.868857</td>\n",
       "      <td>13.099745</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    seed  epoch  epoch_total  member  train_batch_size    pbt_lr  \\\n",
       "0  38042      1           60       0                64  0.000189   \n",
       "1  38042      1           60       1               128  0.000261   \n",
       "2  38042      2           60       0                64  0.000189   \n",
       "3  38042      2           60       1               128  0.000261   \n",
       "4  38042      3           60       0                64  0.000189   \n",
       "\n",
       "   pbt_weight_decay  pbt_drop_path  pbt_batch_size  lr_sched_start  \\\n",
       "0          0.196476       0.123035            64.0    4.840000e-08   \n",
       "1          0.267560       0.119554           128.0    1.340000e-07   \n",
       "2          0.196476       0.123035            64.0    3.790000e-05   \n",
       "3          0.267560       0.119554           128.0    5.230000e-05   \n",
       "4          0.196476       0.123035            64.0    7.580000e-05   \n",
       "\n",
       "   lr_sched_end    loss  train_acc_pct  test_acc_pct  data_build_s  \\\n",
       "0      0.000038  1.8867          29.42         36.69      5.395643   \n",
       "1      0.000052  1.8981          29.23         35.01      1.661272   \n",
       "2      0.000076  1.6182          40.38         43.57      1.626179   \n",
       "3      0.000105  1.6295          40.16         44.72      1.675981   \n",
       "4      0.000114  1.4237          47.84         49.85      1.730894   \n",
       "\n",
       "   train_time_s  eval_time_s  \n",
       "0     61.999089    11.392097  \n",
       "1     63.004851    13.567882  \n",
       "2     68.211169    13.086053  \n",
       "3     64.784817    13.043758  \n",
       "4     68.868857    13.099745  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Parse each log\n",
    "parsed = [parse_pbt_log(p) for p in INPUT_LOG_PATHS]\n",
    "parsed = sorted(parsed, key=lambda d: d[\"seed\"])\n",
    "seed_values = [d[\"seed\"] for d in parsed]\n",
    "\n",
    "df_all = pd.concat([d[\"main\"] for d in parsed], ignore_index=True) if parsed else pd.DataFrame()\n",
    "df_summary_all = pd.concat([d[\"summary\"] for d in parsed if len(d[\"summary\"])], ignore_index=True)\n",
    "df_changes_all = pd.concat([d[\"changes\"] for d in parsed if len(d[\"changes\"])], ignore_index=True)\n",
    "df_copies_all = pd.concat([d[\"copies\"] for d in parsed if len(d[\"copies\"])], ignore_index=True)\n",
    "df_post_all = pd.concat([d[\"post\"] for d in parsed if len(d[\"post\"])], ignore_index=True)\n",
    "\n",
    "# Write CSV\n",
    "df_all.to_csv(OUTPUT_CSV_PATH, index=False)\n",
    "\n",
    "def leaderboard_top(df_in: pd.DataFrame, topn: int = 200) -> pd.DataFrame:\n",
    "    snap = df_in.dropna(subset=[\"test_acc_pct\"]).copy()\n",
    "    snap = snap.sort_values([\"test_acc_pct\", \"epoch\"], ascending=[False, True]).reset_index(drop=True)\n",
    "    snap.insert(0, \"rank\", snap.index + 1)\n",
    "    return snap.head(topn)\n",
    "\n",
    "def leaderboard_per_member(df_in: pd.DataFrame) -> pd.DataFrame:\n",
    "    if not len(df_in):\n",
    "        return pd.DataFrame()\n",
    "    return (\n",
    "        df_in.dropna(subset=[\"test_acc_pct\"])\n",
    "            .sort_values([\"seed\", \"member\", \"test_acc_pct\", \"epoch\"], ascending=[True, True, False, True])\n",
    "            .groupby([\"seed\", \"member\"], as_index=False)\n",
    "            .head(1)\n",
    "            .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "lb_combined = leaderboard_top(df_all, topn=200)\n",
    "lb_per_member = leaderboard_per_member(df_all)\n",
    "\n",
    "# Write XLSX\n",
    "with pd.ExcelWriter(OUTPUT_XLSX_PATH, engine=\"openpyxl\") as writer:\n",
    "    df_all.to_excel(writer, sheet_name=\"epoch_member_metrics\", index=False)\n",
    "    if len(df_summary_all): df_summary_all.to_excel(writer, sheet_name=\"epoch_summary\", index=False)\n",
    "    if len(df_changes_all): df_changes_all.to_excel(writer, sheet_name=\"hyperparam_changes\", index=False)\n",
    "    if len(df_copies_all): df_copies_all.to_excel(writer, sheet_name=\"copy_events\", index=False)\n",
    "    if len(df_post_all): df_post_all.to_excel(writer, sheet_name=\"post_update_hparams\", index=False)\n",
    "\n",
    "    lb_combined.to_excel(writer, sheet_name=\"leaderboard_top200\", index=False)\n",
    "    lb_per_member.to_excel(writer, sheet_name=\"leaderboard_per_member\", index=False)\n",
    "\n",
    "    for seed in seed_values:\n",
    "        leaderboard_top(df_all[df_all[\"seed\"] == seed], topn=200).to_excel(writer, sheet_name=f\"top200_{seed}\"[:31], index=False)\n",
    "\n",
    "print(\"Seeds parsed:\", seed_values)\n",
    "print(\"Wrote CSV:\", OUTPUT_CSV_PATH.resolve())\n",
    "print(\"Wrote XLSX:\", OUTPUT_XLSX_PATH.resolve())\n",
    "df_all.head()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
