{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41dd854d",
   "metadata": {},
   "source": [
    "# Convert PBT PDF logs â†’ machine-readable CSV + Excel workbook\n",
    "\n",
    "This notebook parses **PBT training logs saved as PDFs** and exports:\n",
    "- a combined CSV (`results/pbt_batchsize_pdfs_parsed.csv`)\n",
    "- an Excel workbook (`results/pbt_batchsize_pdfs_parsed.xlsx`) with extra sheets + leaderboards\n",
    "\n",
    "Key robustness rules (matching your requirements):\n",
    "1. **PDF text can be noisy**, so parsing uses conservative regex patterns.\n",
    "2. **Learning-rate schedule** is parsed *only* from lines like:  \n",
    "   `LR changed during epoch: 6.17e-05 -> 5.46e-05`  \n",
    "   and stored as `lr_sched_start`, `lr_sched_end`.\n",
    "3. **Initial hyperparameter configuration** per member is taken from the block:\n",
    "   ```\n",
    "   Hyperparameteres for model k at epoch 1\n",
    "   lr: ...\n",
    "   weight_decay: ...\n",
    "   drop_path: ...\n",
    "   warmup_epochs: ...\n",
    "   batch_size: ...\n",
    "   ```\n",
    "4. **Hyperparameter updates** are applied *only* after update blocks that contain lines like:\n",
    "   `Member k: lr changed from ... to ...`  \n",
    "   Updates at epoch `u` are applied starting at epoch `u+1` (backfilled between updates).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a807b89e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs: [PosixPath('../Raw Outputs/PBT/Full Logs/pbt_batchsize_ablation_experiment_output_seed_38042.pdf'), PosixPath('../Raw Outputs/PBT/Full Logs/pbt_batchsize_ablation_experiment_output_seed_217401.pdf')]\n",
      "CSV: ../Structured Outputs/PBT/pbt_batchsize_pdfs_parsed.csv\n",
      "XLSX: ../Structured Outputs/PBT/pbt_batchsize_pdfs_parsed.xlsx\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# =============================================================================\n",
    "# CONFIGURATION\n",
    "# =============================================================================\n",
    "\n",
    "# Global variable: relative directory where outputs will be written.\n",
    "CSV_REL_DIR = \"../Structured Outputs/PBT/\"\n",
    "\n",
    "# Input PDFs (seed is inferred from filename substring like 'seed_38042')\n",
    "COMMON_PATH = Path(\"../Raw Outputs/PBT/Full Logs/\")\n",
    "INPUT_LOG_PATHS = [\n",
    "    COMMON_PATH / \"pbt_batchsize_ablation_experiment_output_seed_38042.pdf\",\n",
    "    COMMON_PATH / \"pbt_batchsize_ablation_experiment_output_seed_217401.pdf\",\n",
    "]\n",
    "\n",
    "# Output filenames (written inside CSV_REL_DIR)\n",
    "OUTPUT_CSV_NAME = \"pbt_batchsize_pdfs_parsed.csv\"\n",
    "OUTPUT_XLSX_NAME = \"pbt_batchsize_pdfs_parsed.xlsx\"\n",
    "\n",
    "OUTPUT_DIR = Path(CSV_REL_DIR)\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "OUTPUT_CSV_PATH = OUTPUT_DIR / OUTPUT_CSV_NAME\n",
    "OUTPUT_XLSX_PATH = OUTPUT_DIR / OUTPUT_XLSX_NAME\n",
    "\n",
    "print(\"Inputs:\", INPUT_LOG_PATHS)\n",
    "print(\"CSV:\", OUTPUT_CSV_PATH)\n",
    "print(\"XLSX:\", OUTPUT_XLSX_PATH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c5673b",
   "metadata": {},
   "source": [
    "## 1) PDF text extraction\n",
    "\n",
    "We try **PyMuPDF** (`fitz`) first (fast + reliable). If it isn't available, we fall back to `pdfplumber`.\n",
    "No OCR is used.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a10a0e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "def extract_pdf_text(path: Path) -> str:\n",
    "    try:\n",
    "        import fitz  # PyMuPDF\n",
    "        doc = fitz.open(str(path))\n",
    "        parts = [page.get_text(\"text\") for page in doc]\n",
    "        doc.close()\n",
    "        return \"\\n\".join(parts)\n",
    "    except Exception:\n",
    "        import pdfplumber\n",
    "        parts = []\n",
    "        with pdfplumber.open(str(path)) as pdf:\n",
    "            for page in pdf.pages:\n",
    "                parts.append(page.extract_text() or \"\")\n",
    "        return \"\\n\".join(parts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d7b0516",
   "metadata": {},
   "source": [
    "## 2) Conservative regex patterns\n",
    "\n",
    "We only trust:\n",
    "- per-epoch LR schedule from `LR changed during epoch: ... -> ...`\n",
    "- initial hyperparams from the `Hyperparameteres for model ... at epoch 1` block\n",
    "- hyperparam changes from the `Member k: ... changed from ... to ...` lines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ca77f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED_RE = re.compile(r\"seed[_=](\\d+)\", re.IGNORECASE)\n",
    "\n",
    "EPOCH_HDR = re.compile(r\"\\bEpoch\\s+(?P<epoch>\\d+)\\s*/\\s*(?P<epoch_total>\\d+)\\b\")\n",
    "TRAIN_MEMBER = re.compile(r\"---\\s*Training\\s+Member\\s+(?P<member>\\d+)\\s*\\(Batch\\s+size:\\s*(?P<bs>\\d+)\\)\\s*---\")\n",
    "\n",
    "# Requirement (2): LR schedule is ONLY parsed from this pattern.\n",
    "LR_CHANGED = re.compile(r\"LR\\s+changed\\s+during\\s+epoch:\\s*(?P<start>[-+0-9.eE]+)\\s*->\\s*(?P<end>[-+0-9.eE]+)\")\n",
    "\n",
    "LOSS = re.compile(r\"Loss:\\s*(?P<loss>[-+0-9.eE]+)\")\n",
    "TRAIN_ACC = re.compile(r\"Train\\s+Accuracy:\\s*(?P<acc>[-+0-9.]+)\\s*%\")\n",
    "TEST_ACC = re.compile(r\"Test\\s+Accuracy:\\s*(?P<acc>[-+0-9.]+)\\s*%\")\n",
    "\n",
    "BUILT = re.compile(r\"built\\s+data\\s+in\\s+(?P<t>[-+0-9.eE]+)\\s+seconds\")\n",
    "TRAIN_TIME = re.compile(r\"total\\s+runtime\\s+to\\s+train\\s+this\\s+model\\s+was\\s+(?P<t>[-+0-9.eE]+)\\s+seconds\")\n",
    "EVAL_TIME = re.compile(r\"evaluation\\s+in\\s+(?P<t>[-+0-9.eE]+)\\s+seconds\")\n",
    "\n",
    "TOTAL_EPOCHS = re.compile(r\"Total\\s+epochs:\\s*(?P<t>\\d+)\")\n",
    "EXPLOIT_INTERVAL = re.compile(r\"Exploit\\s+interval:\\s*(?P<t>\\d+)\\s+epochs\")\n",
    "\n",
    "# Requirement (3): initial hyperparams come from these key:value lines\n",
    "HYPER_HDR = re.compile(r\"Hyperparameteres\\s+for\\s+model\\s+(?P<member>\\d+)\\s+at\\s+epoch\\s+(?P<epoch>\\d+)\", re.IGNORECASE)\n",
    "HP_LINE = re.compile(r\"^(?P<k>lr|weight_decay|drop_path|warmup_epochs|batch_size)\\s*:\\s*(?P<v>[-+0-9.eE]+)\\s*$\", re.IGNORECASE)\n",
    "\n",
    "# Requirement (4): hyperparams change only after these update lines\n",
    "POP_UPDATE = re.compile(r\"---\\s*Population\\s+Update\\s+\\(Epoch\\s+(?P<epoch>\\d+)\\)\\s*---\")\n",
    "CHANGE_LINE = re.compile(r\"Member\\s+(?P<member>\\d+):\\s*(?P<param>lr|weight_decay|drop_path|batch_size)\\s+changed\\s+from\\s+(?P<old>[-+0-9.eE]+)\\s+to\\s+(?P<new>[-+0-9.eE]+)\", re.IGNORECASE)\n",
    "COPIED_LINE = re.compile(r\"Member\\s+(?P<member>\\d+)\\s+copied\\s+from\\s+(?P<src>\\d+)\", re.IGNORECASE)\n",
    "\n",
    "# Optional: consolidated post-update line (sanity checking)\n",
    "POST_LINE = re.compile(\n",
    "    r\"LR=(?P<lr>[-+0-9.eE]+),\\s*WD=(?P<wd>[-+0-9.eE]+),\\s*DropPath=(?P<dp>[-+0-9.eE]+),\\s*Warmup=(?P<warm>\\d+)\\s*epochs,\\s*Batch=(?P<bs>\\d+)\",\n",
    "    re.IGNORECASE,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14dba08f",
   "metadata": {},
   "source": [
    "## 3) Parse one PDF into tables\n",
    "\n",
    "Returns:\n",
    "- `df_main`: metrics per `(epoch, member)` + reconstructed hyperparams (`pbt_*`)\n",
    "- `df_changes`, `df_copies`, `df_post` (events)\n",
    "- `df_summary` (population-level summaries, optional)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e9e1c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_seed_from_name(p: Path) -> int:\n",
    "    m = SEED_RE.search(p.name)\n",
    "    if not m:\n",
    "        raise ValueError(f\"Could not infer seed from filename: {p.name}\")\n",
    "    return int(m.group(1))\n",
    "\n",
    "def parse_pdf_log(path: Path) -> dict:\n",
    "    seed = parse_seed_from_name(path)\n",
    "    text = extract_pdf_text(path)\n",
    "    lines = [ln.strip() for ln in text.splitlines() if ln.strip() != \"\"]\n",
    "\n",
    "    # Read global metadata if present\n",
    "    total_epochs = int(TOTAL_EPOCHS.search(text).group(\"t\")) if TOTAL_EPOCHS.search(text) else None\n",
    "    exploit_interval = int(EXPLOIT_INTERVAL.search(text).group(\"t\")) if EXPLOIT_INTERVAL.search(text) else None\n",
    "\n",
    "    # (A) initial hyperparams: first hp print per member (usually epoch 1)\n",
    "    initial_hp = {}\n",
    "    i = 0\n",
    "    while i < len(lines):\n",
    "        m = HYPER_HDR.search(lines[i])\n",
    "        if m:\n",
    "            member = int(m.group(\"member\"))\n",
    "            hp = {}\n",
    "            for j in range(1, 12):\n",
    "                if i + j >= len(lines): break\n",
    "                mm = HP_LINE.match(lines[i + j])\n",
    "                if mm:\n",
    "                    k = mm.group(\"k\").lower()\n",
    "                    v = float(mm.group(\"v\"))\n",
    "                    if k in (\"warmup_epochs\", \"batch_size\"):\n",
    "                        v = int(round(v))\n",
    "                    hp[k] = v\n",
    "                if {\"lr\",\"weight_decay\",\"drop_path\",\"warmup_epochs\",\"batch_size\"}.issubset(hp.keys()):\n",
    "                    break\n",
    "            if member not in initial_hp and hp:\n",
    "                initial_hp[member] = hp\n",
    "        i += 1\n",
    "\n",
    "    # (B) event tables + metrics\n",
    "    change_events, copy_events, post_events, records = [], [], [], []\n",
    "    current_update_epoch = None\n",
    "    current_epoch = None\n",
    "    current_epoch_total = None\n",
    "    current_block = None\n",
    "    in_summary = False\n",
    "    summaries = []\n",
    "\n",
    "    # summary regex (optional)\n",
    "    SUMMARY_HDR = re.compile(r\"Epoch\\s+(?P<epoch>\\d+)\\s+Summary:\", re.IGNORECASE)\n",
    "    SUMMARY_TIME = re.compile(r\"Time:\\s*(?P<time>[-+0-9.]+)s\\s*\\(Avg\\s+member:\\s*(?P<avg>[-+0-9.]+)s\\)\")\n",
    "    POP_MEAN_ACC = re.compile(r\"Population\\s+Mean\\s+Accuracy:\\s*(?P<acc>[-+0-9.]+)\\s*%\")\n",
    "    BEST_MEMBER_ACC = re.compile(r\"Best\\s+Member\\s+Accuracy:\\s*(?P<acc>[-+0-9.]+)\\s*%\")\n",
    "    MEAN_BS = re.compile(r\"Mean\\s+Batch\\s+Size:\\s*(?P<bs>\\d+)\")\n",
    "    MEAN_LR = re.compile(r\"Mean\\s+Learning\\s+Rate:\\s*(?P<lr>[-+0-9.eE]+)\")\n",
    "    MEAN_WD = re.compile(r\"Mean\\s+Weight\\s+Decay:\\s*(?P<wd>[-+0-9.eE]+)\")\n",
    "\n",
    "    for ln in lines:\n",
    "        m = EPOCH_HDR.search(ln)\n",
    "        if m and \"Summary\" not in ln:\n",
    "            current_epoch = int(m.group(\"epoch\"))\n",
    "            current_epoch_total = int(m.group(\"epoch_total\"))\n",
    "            in_summary = False\n",
    "\n",
    "        m = TRAIN_MEMBER.search(ln)\n",
    "        if m:\n",
    "            current_block = {\n",
    "                \"seed\": seed,\n",
    "                \"epoch\": current_epoch,\n",
    "                \"epoch_total\": current_epoch_total,\n",
    "                \"member\": int(m.group(\"member\")),\n",
    "                \"train_batch_size\": int(m.group(\"bs\")),\n",
    "            }\n",
    "            continue\n",
    "\n",
    "        if current_block is not None:\n",
    "            m = BUILT.search(ln)\n",
    "            if m: current_block[\"data_build_s\"] = float(m.group(\"t\"))\n",
    "            m = LR_CHANGED.search(ln)\n",
    "            if m:\n",
    "                # Requirement (2)\n",
    "                current_block[\"lr_sched_start\"] = float(m.group(\"start\"))\n",
    "                current_block[\"lr_sched_end\"] = float(m.group(\"end\"))\n",
    "            m = TRAIN_TIME.search(ln)\n",
    "            if m: current_block[\"train_time_s\"] = float(m.group(\"t\"))\n",
    "            m = EVAL_TIME.search(ln)\n",
    "            if m: current_block[\"eval_time_s\"] = float(m.group(\"t\"))\n",
    "            m = LOSS.search(ln)\n",
    "            if m: current_block[\"loss\"] = float(m.group(\"loss\"))\n",
    "            m = TRAIN_ACC.search(ln)\n",
    "            if m: current_block[\"train_acc_pct\"] = float(m.group(\"acc\"))\n",
    "            m = TEST_ACC.search(ln)\n",
    "            if m:\n",
    "                current_block[\"test_acc_pct\"] = float(m.group(\"acc\"))\n",
    "                records.append(current_block)\n",
    "                current_block = None\n",
    "            continue\n",
    "\n",
    "        m = POP_UPDATE.search(ln)\n",
    "        if m:\n",
    "            current_update_epoch = int(m.group(\"epoch\"))\n",
    "            continue\n",
    "        m = CHANGE_LINE.search(ln)\n",
    "        if m:\n",
    "            change_events.append({\n",
    "                \"seed\": seed,\n",
    "                \"update_epoch\": current_update_epoch,\n",
    "                \"member\": int(m.group(\"member\")),\n",
    "                \"param\": m.group(\"param\").lower(),\n",
    "                \"old\": float(m.group(\"old\")),\n",
    "                \"new\": float(m.group(\"new\")),\n",
    "            })\n",
    "            continue\n",
    "        m = COPIED_LINE.search(ln)\n",
    "        if m:\n",
    "            copy_events.append({\n",
    "                \"seed\": seed,\n",
    "                \"update_epoch\": current_update_epoch,\n",
    "                \"member\": int(m.group(\"member\")),\n",
    "                \"copied_from\": int(m.group(\"src\")),\n",
    "            })\n",
    "            continue\n",
    "        m = POST_LINE.search(ln)\n",
    "        if m:\n",
    "            post_events.append({\n",
    "                \"seed\": seed,\n",
    "                \"update_epoch\": current_update_epoch,\n",
    "                \"lr\": float(m.group(\"lr\")),\n",
    "                \"weight_decay\": float(m.group(\"wd\")),\n",
    "                \"drop_path\": float(m.group(\"dp\")),\n",
    "                \"warmup_epochs\": int(m.group(\"warm\")),\n",
    "                \"batch_size\": int(m.group(\"bs\")),\n",
    "            })\n",
    "            continue\n",
    "\n",
    "        m = SUMMARY_HDR.search(ln)\n",
    "        if m:\n",
    "            summaries.append({\"seed\": seed, \"epoch\": int(m.group(\"epoch\"))})\n",
    "            in_summary = True\n",
    "            continue\n",
    "        if in_summary and summaries:\n",
    "            cur = summaries[-1]\n",
    "            m = SUMMARY_TIME.search(ln)\n",
    "            if m: cur[\"epoch_time_s\"] = float(m.group(\"time\")); cur[\"avg_member_time_s\"] = float(m.group(\"avg\"))\n",
    "            m = POP_MEAN_ACC.search(ln)\n",
    "            if m: cur[\"pop_mean_acc_pct\"] = float(m.group(\"acc\"))\n",
    "            m = BEST_MEMBER_ACC.search(ln)\n",
    "            if m: cur[\"best_member_acc_pct\"] = float(m.group(\"acc\"))\n",
    "            m = MEAN_BS.search(ln)\n",
    "            if m: cur[\"mean_batch_size\"] = int(m.group(\"bs\"))\n",
    "            m = MEAN_LR.search(ln)\n",
    "            if m: cur[\"mean_lr\"] = float(m.group(\"lr\"))\n",
    "            m = MEAN_WD.search(ln)\n",
    "            if m: cur[\"mean_weight_decay\"] = float(m.group(\"wd\"))\n",
    "\n",
    "    df_metrics = pd.DataFrame(records)\n",
    "    df_changes = pd.DataFrame(change_events)\n",
    "    df_copies = pd.DataFrame(copy_events)\n",
    "    df_post = pd.DataFrame(post_events)\n",
    "    df_summary = pd.DataFrame(summaries)\n",
    "\n",
    "    if total_epochs is None and len(df_metrics):\n",
    "        total_epochs = int(df_metrics[\"epoch\"].max())\n",
    "\n",
    "    # (C) reconstruct piecewise-constant hyperparams per epoch-member using initial_hp + change events\n",
    "    members = sorted(df_metrics[\"member\"].unique().tolist()) if len(df_metrics) else sorted(initial_hp.keys())\n",
    "    params = [\"lr\", \"weight_decay\", \"drop_path\", \"warmup_epochs\", \"batch_size\"]\n",
    "\n",
    "    change_map = {}\n",
    "    if len(df_changes):\n",
    "        for (mem, par), g in df_changes.groupby([\"member\",\"param\"]):\n",
    "            change_map[(int(mem), str(par))] = sorted([(int(u), float(n)) for u, n in zip(g[\"update_epoch\"], g[\"new\"])], key=lambda x: x[0])\n",
    "\n",
    "    hp_rows = []\n",
    "    for mem in members:\n",
    "        base = initial_hp.get(mem, {})\n",
    "        for ep in range(1, total_epochs + 1):\n",
    "            row = {\"seed\": seed, \"epoch\": ep, \"member\": mem}\n",
    "            for par in params:\n",
    "                val = base.get(par)\n",
    "                for u, newv in change_map.get((mem, par), []):\n",
    "                    if ep > u:\n",
    "                        val = newv\n",
    "                    else:\n",
    "                        break\n",
    "                if par in (\"warmup_epochs\",\"batch_size\") and val is not None:\n",
    "                    val = int(round(val))\n",
    "                row[f\"pbt_{par}\"] = val\n",
    "            hp_rows.append(row)\n",
    "\n",
    "    df_hp = pd.DataFrame(hp_rows)\n",
    "    df_main = df_metrics.merge(df_hp, on=[\"seed\",\"epoch\",\"member\"], how=\"left\")\n",
    "\n",
    "    # batch size: if missing, fill from training header\n",
    "    df_main[\"pbt_batch_size\"] = df_main[\"pbt_batch_size\"].fillna(df_main[\"train_batch_size\"])\n",
    "\n",
    "    return {\n",
    "        \"meta\": {\"seed\": seed, \"total_epochs\": total_epochs, \"exploit_interval\": exploit_interval, \"members\": members},\n",
    "        \"main\": df_main,\n",
    "        \"changes\": df_changes,\n",
    "        \"copies\": df_copies,\n",
    "        \"post\": df_post,\n",
    "        \"summary\": df_summary,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef2b944a",
   "metadata": {},
   "source": [
    "## 4) Run on all PDFs and export CSV + Excel workbook\n",
    "\n",
    "Workbook sheets:\n",
    "- `epoch_member_metrics`\n",
    "- `epoch_summary`\n",
    "- `hyperparam_changes`\n",
    "- `copy_events`\n",
    "- `post_update_lines`\n",
    "- `leaderboard_top200` (combined)\n",
    "- `top200_<seed>` (per-seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b884e9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote CSV: /Users/etaashpatel/Documents/Final Project/Structured Outputs/PBT/pbt_batchsize_pdfs_parsed.csv\n",
      "Wrote XLSX: /Users/etaashpatel/Documents/Final Project/Structured Outputs/PBT/pbt_batchsize_pdfs_parsed.xlsx\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rank</th>\n",
       "      <th>seed</th>\n",
       "      <th>epoch</th>\n",
       "      <th>epoch_total</th>\n",
       "      <th>member</th>\n",
       "      <th>train_batch_size</th>\n",
       "      <th>data_build_s</th>\n",
       "      <th>lr_sched_start</th>\n",
       "      <th>lr_sched_end</th>\n",
       "      <th>train_time_s</th>\n",
       "      <th>eval_time_s</th>\n",
       "      <th>loss</th>\n",
       "      <th>train_acc_pct</th>\n",
       "      <th>test_acc_pct</th>\n",
       "      <th>pbt_lr</th>\n",
       "      <th>pbt_weight_decay</th>\n",
       "      <th>pbt_drop_path</th>\n",
       "      <th>pbt_warmup_epochs</th>\n",
       "      <th>pbt_batch_size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>38042</td>\n",
       "      <td>69</td>\n",
       "      <td>70</td>\n",
       "      <td>3</td>\n",
       "      <td>256</td>\n",
       "      <td>1.572514</td>\n",
       "      <td>0.000069</td>\n",
       "      <td>0.000062</td>\n",
       "      <td>25.865024</td>\n",
       "      <td>2.305712</td>\n",
       "      <td>0.2915</td>\n",
       "      <td>89.63</td>\n",
       "      <td>79.47</td>\n",
       "      <td>0.000428</td>\n",
       "      <td>0.107640</td>\n",
       "      <td>0.163788</td>\n",
       "      <td>5</td>\n",
       "      <td>256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>38042</td>\n",
       "      <td>70</td>\n",
       "      <td>70</td>\n",
       "      <td>3</td>\n",
       "      <td>256</td>\n",
       "      <td>1.620410</td>\n",
       "      <td>0.000062</td>\n",
       "      <td>0.000055</td>\n",
       "      <td>25.861118</td>\n",
       "      <td>2.258396</td>\n",
       "      <td>0.2818</td>\n",
       "      <td>90.10</td>\n",
       "      <td>79.32</td>\n",
       "      <td>0.000428</td>\n",
       "      <td>0.107640</td>\n",
       "      <td>0.163788</td>\n",
       "      <td>5</td>\n",
       "      <td>256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>38042</td>\n",
       "      <td>65</td>\n",
       "      <td>70</td>\n",
       "      <td>2</td>\n",
       "      <td>256</td>\n",
       "      <td>1.615275</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>25.889504</td>\n",
       "      <td>3.097810</td>\n",
       "      <td>0.3314</td>\n",
       "      <td>88.21</td>\n",
       "      <td>79.31</td>\n",
       "      <td>0.000466</td>\n",
       "      <td>0.041638</td>\n",
       "      <td>0.217043</td>\n",
       "      <td>5</td>\n",
       "      <td>256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>38042</td>\n",
       "      <td>67</td>\n",
       "      <td>70</td>\n",
       "      <td>2</td>\n",
       "      <td>256</td>\n",
       "      <td>1.577023</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>25.191667</td>\n",
       "      <td>2.200948</td>\n",
       "      <td>0.3234</td>\n",
       "      <td>88.59</td>\n",
       "      <td>79.31</td>\n",
       "      <td>0.000466</td>\n",
       "      <td>0.041638</td>\n",
       "      <td>0.217043</td>\n",
       "      <td>5</td>\n",
       "      <td>256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>38042</td>\n",
       "      <td>70</td>\n",
       "      <td>70</td>\n",
       "      <td>2</td>\n",
       "      <td>256</td>\n",
       "      <td>1.572048</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>26.126183</td>\n",
       "      <td>2.327622</td>\n",
       "      <td>0.3187</td>\n",
       "      <td>88.74</td>\n",
       "      <td>79.30</td>\n",
       "      <td>0.000466</td>\n",
       "      <td>0.041638</td>\n",
       "      <td>0.217043</td>\n",
       "      <td>5</td>\n",
       "      <td>256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>38042</td>\n",
       "      <td>62</td>\n",
       "      <td>70</td>\n",
       "      <td>2</td>\n",
       "      <td>256</td>\n",
       "      <td>1.569269</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>25.824163</td>\n",
       "      <td>2.255420</td>\n",
       "      <td>0.3419</td>\n",
       "      <td>87.86</td>\n",
       "      <td>79.27</td>\n",
       "      <td>0.000466</td>\n",
       "      <td>0.041638</td>\n",
       "      <td>0.217043</td>\n",
       "      <td>5</td>\n",
       "      <td>256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>38042</td>\n",
       "      <td>67</td>\n",
       "      <td>70</td>\n",
       "      <td>3</td>\n",
       "      <td>256</td>\n",
       "      <td>1.584517</td>\n",
       "      <td>0.000085</td>\n",
       "      <td>0.000077</td>\n",
       "      <td>25.759567</td>\n",
       "      <td>3.066950</td>\n",
       "      <td>0.3085</td>\n",
       "      <td>89.19</td>\n",
       "      <td>79.27</td>\n",
       "      <td>0.000428</td>\n",
       "      <td>0.107640</td>\n",
       "      <td>0.163788</td>\n",
       "      <td>5</td>\n",
       "      <td>256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>38042</td>\n",
       "      <td>69</td>\n",
       "      <td>70</td>\n",
       "      <td>2</td>\n",
       "      <td>256</td>\n",
       "      <td>1.591973</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>26.046602</td>\n",
       "      <td>2.322003</td>\n",
       "      <td>0.3209</td>\n",
       "      <td>88.62</td>\n",
       "      <td>79.26</td>\n",
       "      <td>0.000466</td>\n",
       "      <td>0.041638</td>\n",
       "      <td>0.217043</td>\n",
       "      <td>5</td>\n",
       "      <td>256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>38042</td>\n",
       "      <td>70</td>\n",
       "      <td>70</td>\n",
       "      <td>0</td>\n",
       "      <td>256</td>\n",
       "      <td>1.709418</td>\n",
       "      <td>0.000118</td>\n",
       "      <td>0.000112</td>\n",
       "      <td>25.994676</td>\n",
       "      <td>2.321771</td>\n",
       "      <td>0.3758</td>\n",
       "      <td>86.51</td>\n",
       "      <td>79.26</td>\n",
       "      <td>0.000254</td>\n",
       "      <td>0.115614</td>\n",
       "      <td>0.346303</td>\n",
       "      <td>5</td>\n",
       "      <td>256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>38042</td>\n",
       "      <td>68</td>\n",
       "      <td>70</td>\n",
       "      <td>3</td>\n",
       "      <td>256</td>\n",
       "      <td>1.604138</td>\n",
       "      <td>0.000077</td>\n",
       "      <td>0.000069</td>\n",
       "      <td>25.405401</td>\n",
       "      <td>2.348608</td>\n",
       "      <td>0.2965</td>\n",
       "      <td>89.34</td>\n",
       "      <td>79.23</td>\n",
       "      <td>0.000428</td>\n",
       "      <td>0.107640</td>\n",
       "      <td>0.163788</td>\n",
       "      <td>5</td>\n",
       "      <td>256</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rank   seed  epoch  epoch_total  member  train_batch_size  data_build_s  \\\n",
       "0     1  38042     69           70       3               256      1.572514   \n",
       "1     2  38042     70           70       3               256      1.620410   \n",
       "2     3  38042     65           70       2               256      1.615275   \n",
       "3     4  38042     67           70       2               256      1.577023   \n",
       "4     5  38042     70           70       2               256      1.572048   \n",
       "5     6  38042     62           70       2               256      1.569269   \n",
       "6     7  38042     67           70       3               256      1.584517   \n",
       "7     8  38042     69           70       2               256      1.591973   \n",
       "8     9  38042     70           70       0               256      1.709418   \n",
       "9    10  38042     68           70       3               256      1.604138   \n",
       "\n",
       "   lr_sched_start  lr_sched_end  train_time_s  eval_time_s    loss  \\\n",
       "0        0.000069      0.000062     25.865024     2.305712  0.2915   \n",
       "1        0.000062      0.000055     25.861118     2.258396  0.2818   \n",
       "2        0.000011      0.000008     25.889504     3.097810  0.3314   \n",
       "3        0.000005      0.000003     25.191667     2.200948  0.3234   \n",
       "4        0.000001      0.000001     26.126183     2.327622  0.3187   \n",
       "5        0.000023      0.000018     25.824163     2.255420  0.3419   \n",
       "6        0.000085      0.000077     25.759567     3.066950  0.3085   \n",
       "7        0.000002      0.000001     26.046602     2.322003  0.3209   \n",
       "8        0.000118      0.000112     25.994676     2.321771  0.3758   \n",
       "9        0.000077      0.000069     25.405401     2.348608  0.2965   \n",
       "\n",
       "   train_acc_pct  test_acc_pct    pbt_lr  pbt_weight_decay  pbt_drop_path  \\\n",
       "0          89.63         79.47  0.000428          0.107640       0.163788   \n",
       "1          90.10         79.32  0.000428          0.107640       0.163788   \n",
       "2          88.21         79.31  0.000466          0.041638       0.217043   \n",
       "3          88.59         79.31  0.000466          0.041638       0.217043   \n",
       "4          88.74         79.30  0.000466          0.041638       0.217043   \n",
       "5          87.86         79.27  0.000466          0.041638       0.217043   \n",
       "6          89.19         79.27  0.000428          0.107640       0.163788   \n",
       "7          88.62         79.26  0.000466          0.041638       0.217043   \n",
       "8          86.51         79.26  0.000254          0.115614       0.346303   \n",
       "9          89.34         79.23  0.000428          0.107640       0.163788   \n",
       "\n",
       "   pbt_warmup_epochs  pbt_batch_size  \n",
       "0                  5             256  \n",
       "1                  5             256  \n",
       "2                  5             256  \n",
       "3                  5             256  \n",
       "4                  5             256  \n",
       "5                  5             256  \n",
       "6                  5             256  \n",
       "7                  5             256  \n",
       "8                  5             256  \n",
       "9                  5             256  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsed = [parse_pdf_log(p) for p in INPUT_LOG_PATHS]\n",
    "parsed = sorted(parsed, key=lambda d: d[\"meta\"][\"seed\"])\n",
    "\n",
    "df_all = pd.concat([d[\"main\"] for d in parsed], ignore_index=True)\n",
    "df_changes_all = pd.concat([d[\"changes\"] for d in parsed if len(d[\"changes\"])], ignore_index=True)\n",
    "df_copies_all = pd.concat([d[\"copies\"] for d in parsed if len(d[\"copies\"])], ignore_index=True)\n",
    "df_post_all = pd.concat([d[\"post\"] for d in parsed if len(d[\"post\"])], ignore_index=True)\n",
    "df_summary_all = pd.concat([d[\"summary\"] for d in parsed if len(d[\"summary\"])], ignore_index=True)\n",
    "\n",
    "df_all.to_csv(OUTPUT_CSV_PATH, index=False)\n",
    "\n",
    "def top_snapshots(df_in: pd.DataFrame, topn=200) -> pd.DataFrame:\n",
    "    s = df_in.dropna(subset=[\"test_acc_pct\"]).copy()\n",
    "    s = s.sort_values([\"test_acc_pct\",\"epoch\"], ascending=[False, True]).reset_index(drop=True)\n",
    "    s.insert(0,\"rank\", s.index+1)\n",
    "    return s.head(topn)\n",
    "\n",
    "lb_combined = top_snapshots(df_all, 200)\n",
    "lbs_by_seed = {seed: top_snapshots(df_all[df_all[\"seed\"]==seed], 200) for seed in sorted(df_all[\"seed\"].unique().tolist())}\n",
    "\n",
    "with pd.ExcelWriter(OUTPUT_XLSX_PATH, engine=\"openpyxl\") as writer:\n",
    "    df_all.to_excel(writer, sheet_name=\"epoch_member_metrics\", index=False)\n",
    "    if len(df_summary_all): df_summary_all.sort_values([\"seed\",\"epoch\"]).to_excel(writer, sheet_name=\"epoch_summary\", index=False)\n",
    "    if len(df_changes_all): df_changes_all.sort_values([\"seed\",\"update_epoch\",\"member\",\"param\"]).to_excel(writer, sheet_name=\"hyperparam_changes\", index=False)\n",
    "    if len(df_copies_all): df_copies_all.sort_values([\"seed\",\"update_epoch\",\"member\"]).to_excel(writer, sheet_name=\"copy_events\", index=False)\n",
    "    if len(df_post_all): df_post_all.sort_values([\"seed\",\"update_epoch\"]).to_excel(writer, sheet_name=\"post_update_lines\", index=False)\n",
    "    lb_combined.to_excel(writer, sheet_name=\"leaderboard_top200\", index=False)\n",
    "    for seed, lb in lbs_by_seed.items():\n",
    "        lb.to_excel(writer, sheet_name=f\"top200_{seed}\"[:31], index=False)\n",
    "\n",
    "print(\"Wrote CSV:\", OUTPUT_CSV_PATH.resolve())\n",
    "print(\"Wrote XLSX:\", OUTPUT_XLSX_PATH.resolve())\n",
    "lb_combined.head(10)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
