{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "4ad11414",
      "metadata": {},
      "source": [
        "# Convert multiple PBT logs → machine-readable CSV + Excel workbook\n",
        "\n",
        "This notebook handles **multiple PBT log files** and **adds a `seed` column** inferred from the filename\n",
        "(e.g., `bs+wd_seed_38042.log` → seed `38042`).\n",
        "\n",
        "For each seed/log, we parse:\n",
        "- per-epoch/per-member metrics\n",
        "- PBT update events (hyperparameter changes + copy events)\n",
        "- optional post-update hyperparameter lines\n",
        "- population-level epoch summaries\n",
        "\n",
        "Then we reconstruct **per-epoch PBT hyperparameters** (since they change over time), concatenate all seeds,\n",
        "and export:\n",
        "- `results/pbt_bs_wd_parsed.csv`\n",
        "- `results/pbt_bs_wd_parsed.xlsx` (with leaderboards, plus per-seed top-200 sheets)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "182aad87",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Will read: [PosixPath('../Raw Outputs/PBT/Full Logs/bs_seed_38042.log'), PosixPath('../Raw Outputs/PBT/Full Logs/bs_seed_217401.log')]\n",
            "Will write CSV: ../Structured Outputs/PBT/pbt_bs_parsed.csv\n",
            "Will write XLSX: ../Structured Outputs/PBT/pbt_bs_parsed.xlsx\n"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "# =============================================================================\n",
        "# CONFIGURATION\n",
        "# =============================================================================\n",
        "\n",
        "# Global variable: relative directory where outputs will be written.\n",
        "CSV_REL_DIR = \"../Structured Outputs/PBT/\"\n",
        "\n",
        "# Provide one or more PBT log files here. Seed is inferred from the filename.\n",
        "# Example names: bs+wd_seed_38042.log, bs+wd_seed_217401.log\n",
        "COMMON_PATH = Path(\"../Raw Outputs/PBT/Full Logs/\")\n",
        "INPUT_LOG_PATHS = [\n",
        "    COMMON_PATH / \"bs_seed_38042.log\",\n",
        "    COMMON_PATH / \"bs_seed_217401.log\",\n",
        "]\n",
        "\n",
        "# Output names (written inside CSV_REL_DIR)\n",
        "OUTPUT_CSV_NAME = \"pbt_bs_parsed.csv\"\n",
        "OUTPUT_XLSX_NAME = \"pbt_bs_parsed.xlsx\"\n",
        "\n",
        "# Derived paths\n",
        "OUTPUT_DIR = Path(CSV_REL_DIR)\n",
        "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "OUTPUT_CSV_PATH = OUTPUT_DIR / OUTPUT_CSV_NAME\n",
        "OUTPUT_XLSX_PATH = OUTPUT_DIR / OUTPUT_XLSX_NAME\n",
        "\n",
        "print(\"Will read:\", INPUT_LOG_PATHS)\n",
        "print(\"Will write CSV:\", OUTPUT_CSV_PATH)\n",
        "print(\"Will write XLSX:\", OUTPUT_XLSX_PATH)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8f9b2a74",
      "metadata": {},
      "source": [
        "## Parser implementation\n",
        "\n",
        "We implement a function `parse_pbt_log(path)` which:\n",
        "1. Infers `seed` from the filename (`seed_XXXXX`).\n",
        "2. Parses:\n",
        "   - `(epoch, member)` metrics\n",
        "   - PBT hyperparameter change events (`old -> new`) per member and per update epoch\n",
        "   - copy events\n",
        "   - population summaries\n",
        "3. Reconstructs per-epoch PBT hyperparameters (`pbt_lr`, `pbt_weight_decay`, `pbt_drop_path`, `pbt_batch_size`)\n",
        "   because PBT mutates hyperparameters over time.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "ee98e7fe",
      "metadata": {},
      "outputs": [],
      "source": [
        "# -------------------------------\n",
        "# Parsing utilities (UPDATED)\n",
        "# -------------------------------\n",
        "from pathlib import Path\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Epoch + member blocks\n",
        "EPOCH_HDR = re.compile(r\"\\bEpoch\\s+(?P<epoch>\\d+)\\s*/\\s*(?P<epoch_total>\\d+)\\b\", re.IGNORECASE)\n",
        "TRAIN_MEMBER = re.compile(\n",
        "    r\"---\\s*Training\\s+Member\\s+(?P<member>\\d+)\\s*\\(Batch\\s+size:\\s*(?P<bs>\\d+)\\)\\s*---\",\n",
        "    re.IGNORECASE,\n",
        ")\n",
        "\n",
        "# LR + batch size are taken ONLY from per-member training blocks:\n",
        "LR_CHANGED = re.compile(\n",
        "    r\"LR\\s+changed\\s+during\\s+epoch:\\s*(?P<start>[-+0-9.eE]+)\\s*->\\s*(?P<end>[-+0-9.eE]+)\",\n",
        "    re.IGNORECASE,\n",
        ")\n",
        "\n",
        "# Per-member metrics\n",
        "BUILT = re.compile(r\"built\\s+data\\s+in\\s+(?P<t>[-+0-9.eE]+)\\s+seconds\", re.IGNORECASE)\n",
        "TRAIN_TIME = re.compile(r\"total\\s+runtime\\s+to\\s+train\\s+this\\s+model\\s+was\\s+(?P<t>[-+0-9.eE]+)\\s+seconds\", re.IGNORECASE)\n",
        "EVAL_TIME = re.compile(r\"evaluation\\s+in\\s+(?P<t>[-+0-9.eE]+)\\s+seconds\", re.IGNORECASE)\n",
        "LOSS = re.compile(r\"Loss:\\s*(?P<loss>[-+0-9.eE]+)\", re.IGNORECASE)\n",
        "TRAIN_ACC = re.compile(r\"Train\\s+Accuracy:\\s*(?P<acc>[-+0-9.]+)\\s*%\", re.IGNORECASE)\n",
        "TEST_ACC = re.compile(r\"Test\\s+Accuracy:\\s*(?P<acc>[-+0-9.]+)\\s*%\", re.IGNORECASE)\n",
        "\n",
        "# Population update blocks (non-LR/non-batch hyperparams come from here)\n",
        "POP_UPDATE_HDR = re.compile(r\"---\\s*Population\\s+Update\\s*\\(Epoch\\s+(?P<epoch>\\d+)\\)\\s*---\", re.IGNORECASE)\n",
        "CHANGE_LINE = re.compile(\n",
        "    r\"Member\\s+(?P<member>\\d+):\\s*(?P<param>[A-Za-z0-9_]+)\\s+changed\\s+from\\s+(?P<old>[-+0-9.eE]+)\\s+to\\s+(?P<new>[-+0-9.eE]+)\",\n",
        "    re.IGNORECASE,\n",
        ")\n",
        "COPIED_LINE = re.compile(r\"Member\\s+(?P<member>\\d+)\\s+copied\\s+from\\s+(?P<src>\\d+)\", re.IGNORECASE)\n",
        "POST_LINE = re.compile(\n",
        "    r\"LR=(?P<lr>[-+0-9.eE]+),\\s*WD=(?P<wd>[-+0-9.eE]+),\\s*DropPath=(?P<dp>[-+0-9.eE]+),\\s*Warmup=(?P<warm>\\d+)\\s*epochs,\\s*Batch=(?P<bs>\\d+)\",\n",
        "    re.IGNORECASE,\n",
        ")\n",
        "\n",
        "# Epoch summary blocks\n",
        "SUMMARY_HDR = re.compile(r\"Epoch\\s+(?P<epoch>\\d+)\\s+Summary:\", re.IGNORECASE)\n",
        "SUMMARY_TIME = re.compile(r\"Time:\\s*(?P<time>[-+0-9.]+)s\\s*\\(Avg\\s+member:\\s*(?P<avg>[-+0-9.]+)s\\)\", re.IGNORECASE)\n",
        "POP_MEAN_ACC = re.compile(r\"Population\\s+Mean\\s+Accuracy:\\s*(?P<acc>[-+0-9.]+)\\s*%\", re.IGNORECASE)\n",
        "BEST_MEMBER_ACC = re.compile(r\"Best\\s+Member\\s+Accuracy:\\s*(?P<acc>[-+0-9.]+)\\s*%\", re.IGNORECASE)\n",
        "MEAN_BS = re.compile(r\"Mean\\s+Batch\\s+Size:\\s*(?P<bs>\\d+)\", re.IGNORECASE)\n",
        "MEAN_LR = re.compile(r\"Mean\\s+Learning\\s+Rate:\\s*(?P<lr>[-+0-9.eE]+)\", re.IGNORECASE)\n",
        "MEAN_WD = re.compile(r\"Mean\\s+Weight\\s+Decay:\\s*(?P<wd>[-+0-9.eE]+)\", re.IGNORECASE)\n",
        "\n",
        "SEED_RE = re.compile(r\"seed[_=](\\d+)\", re.IGNORECASE)\n",
        "\n",
        "def _norm_param_name(p: str) -> str:\n",
        "    p = p.lower().strip()\n",
        "    if p in (\"weightdecay\", \"weight_decay\", \"wd\"):\n",
        "        return \"weight_decay\"\n",
        "    if p in (\"drop_path\", \"droppath\", \"drop_path_rate\", \"dpr\"):\n",
        "        return \"drop_path\"\n",
        "    if p in (\"warmup\", \"warmup_epochs\", \"warmup_epoch\"):\n",
        "        return \"warmup_epochs\"\n",
        "    if p in (\"lr\", \"learning_rate\", \"learningrate\"):\n",
        "        return \"lr\"\n",
        "    if p in (\"batch\", \"batch_size\", \"bs\"):\n",
        "        return \"batch_size\"\n",
        "    return p\n",
        "\n",
        "def parse_seed_from_name(p: Path) -> int:\n",
        "    m = SEED_RE.search(p.name)\n",
        "    if not m:\n",
        "        raise ValueError(f\"Could not infer seed from filename: {p.name}\")\n",
        "    return int(m.group(1))\n",
        "\n",
        "def parse_pbt_text(text: str, seed: int) -> dict:\n",
        "    \"\"\"\n",
        "    Parse a PBT log into:\n",
        "      - df_main: per-epoch, per-member metrics + hyperparams\n",
        "      - df_changes: hyperparameter-change events (from Population Update blocks)\n",
        "      - df_copies: copy events (from Population Update blocks)\n",
        "      - df_post: post-exploit hyperparam summary lines\n",
        "      - df_summary: epoch-level population summary\n",
        "    Policy:\n",
        "      - LR + batch size ONLY from per-member training blocks.\n",
        "      - All other hyperparameters ONLY from Population Update blocks.\n",
        "      - Non-(LR, batch) hyperparams are forward-filled, then backfilled per member to\n",
        "        remove leading NaNs (per the requested backfill behavior).\n",
        "      - When a member is copied from another, we also anchor the *source* member's\n",
        "        non-(LR, batch) hyperparams using the destination's post line for backfill.\n",
        "    \"\"\"\n",
        "    lines = [ln.strip() for ln in text.splitlines() if ln.strip()]\n",
        "\n",
        "    # state tracks non-(LR, batch) hyperparams at the start of each epoch for each member\n",
        "    state = {}  # member -> dict of {\"weight_decay\":..., \"drop_path\":..., \"warmup_epochs\":...}\n",
        "\n",
        "    def ensure(m: int):\n",
        "        if m not in state:\n",
        "            state[m] = {}\n",
        "\n",
        "    rows = []\n",
        "    changes = []\n",
        "    copies = []\n",
        "    post_rows = []\n",
        "    summary_rows = []\n",
        "    anchors = []  # for backfilling source members: {\"member\":src, \"epoch\":u, ...}\n",
        "\n",
        "    current_epoch = None\n",
        "    epoch_total = None\n",
        "    current_member = None\n",
        "    last_row_idx = None\n",
        "\n",
        "    in_pop_update = False\n",
        "    update_epoch = None\n",
        "    pending_updates = {}  # dest_member -> {param: new_value} (non-(LR,batch) only)\n",
        "    copied_from = {}      # dest_member -> src_member\n",
        "    last_ref_member = None\n",
        "\n",
        "    in_summary = False\n",
        "\n",
        "    def finish_update_block():\n",
        "        nonlocal in_pop_update, update_epoch, pending_updates, copied_from, last_ref_member\n",
        "        if not in_pop_update:\n",
        "            return\n",
        "        # Apply pending updates to state for the next epoch (state is used when we hit the next epoch's TRAIN_MEMBER blocks).\n",
        "        for dest, upd in pending_updates.items():\n",
        "            ensure(dest)\n",
        "            base = state[dest].copy()\n",
        "            if dest in copied_from:\n",
        "                src = copied_from[dest]\n",
        "                ensure(src)\n",
        "                # inherit from source when available\n",
        "                if len(state[src]) > 0:\n",
        "                    base = state[src].copy()\n",
        "            base.update(upd)\n",
        "            state[dest] = base\n",
        "        # reset\n",
        "        in_pop_update = False\n",
        "        update_epoch = None\n",
        "        pending_updates = {}\n",
        "        copied_from = {}\n",
        "        last_ref_member = None\n",
        "\n",
        "    for ln in lines:\n",
        "        m = EPOCH_HDR.search(ln)\n",
        "        if m and \"Summary\" not in ln:\n",
        "            # entering a new epoch: close any pending update blocks\n",
        "            finish_update_block()\n",
        "            in_summary = False\n",
        "            current_member = None\n",
        "            current_epoch = int(m.group(\"epoch\"))\n",
        "            epoch_total = int(m.group(\"epoch_total\"))\n",
        "            continue\n",
        "\n",
        "        m = POP_UPDATE_HDR.search(ln)\n",
        "        if m:\n",
        "            finish_update_block()\n",
        "            in_pop_update = True\n",
        "            update_epoch = int(m.group(\"epoch\"))\n",
        "            pending_updates = {}\n",
        "            copied_from = {}\n",
        "            last_ref_member = None\n",
        "            continue\n",
        "\n",
        "        m = SUMMARY_HDR.search(ln)\n",
        "        if m:\n",
        "            in_summary = True\n",
        "            summary_rows.append({\"seed\": seed, \"epoch\": int(m.group(\"epoch\"))})\n",
        "            continue\n",
        "\n",
        "        if in_summary:\n",
        "            srow = summary_rows[-1]\n",
        "            m = SUMMARY_TIME.search(ln)\n",
        "            if m:\n",
        "                srow[\"time_s\"] = float(m.group(\"time\"))\n",
        "                srow[\"avg_member_s\"] = float(m.group(\"avg\"))\n",
        "                continue\n",
        "            m = POP_MEAN_ACC.search(ln)\n",
        "            if m:\n",
        "                srow[\"pop_mean_acc_pct\"] = float(m.group(\"acc\"))\n",
        "                continue\n",
        "            m = BEST_MEMBER_ACC.search(ln)\n",
        "            if m:\n",
        "                srow[\"best_member_acc_pct\"] = float(m.group(\"acc\"))\n",
        "                continue\n",
        "            m = MEAN_BS.search(ln)\n",
        "            if m:\n",
        "                srow[\"mean_batch_size\"] = int(m.group(\"bs\"))\n",
        "                continue\n",
        "            m = MEAN_LR.search(ln)\n",
        "            if m:\n",
        "                srow[\"mean_lr\"] = float(m.group(\"lr\"))\n",
        "                continue\n",
        "            m = MEAN_WD.search(ln)\n",
        "            if m:\n",
        "                srow[\"mean_weight_decay\"] = float(m.group(\"wd\"))\n",
        "                continue\n",
        "            continue\n",
        "\n",
        "        if in_pop_update:\n",
        "            m = CHANGE_LINE.search(ln)\n",
        "            if m:\n",
        "                mem = int(m.group(\"member\"))\n",
        "                param = _norm_param_name(m.group(\"param\"))\n",
        "                old = float(m.group(\"old\"))\n",
        "                new = float(m.group(\"new\"))\n",
        "                changes.append({\"seed\": seed, \"update_epoch\": update_epoch, \"member\": mem, \"param\": param, \"old\": old, \"new\": new})\n",
        "                if param not in (\"lr\", \"batch_size\"):\n",
        "                    pending_updates.setdefault(mem, {})[param] = new\n",
        "                last_ref_member = mem\n",
        "                continue\n",
        "\n",
        "            m = COPIED_LINE.search(ln)\n",
        "            if m:\n",
        "                dest = int(m.group(\"member\"))\n",
        "                src = int(m.group(\"src\"))\n",
        "                copied_from[dest] = src\n",
        "                copies.append({\"seed\": seed, \"update_epoch\": update_epoch, \"member\": dest, \"src_member\": src})\n",
        "                last_ref_member = dest\n",
        "                continue\n",
        "\n",
        "            m = POST_LINE.search(ln)\n",
        "            if m:\n",
        "                dest = last_ref_member\n",
        "                if dest is None and len(copied_from) == 1:\n",
        "                    dest = list(copied_from.keys())[0]\n",
        "                if dest is not None:\n",
        "                    wd = float(m.group(\"wd\"))\n",
        "                    dp = float(m.group(\"dp\"))\n",
        "                    warm = int(m.group(\"warm\"))\n",
        "                    pending_updates.setdefault(dest, {})\n",
        "                    pending_updates[dest].update({\"weight_decay\": wd, \"drop_path\": dp, \"warmup_epochs\": warm})\n",
        "\n",
        "                    post_rows.append({\n",
        "                        \"seed\": seed,\n",
        "                        \"update_epoch\": update_epoch,\n",
        "                        \"member\": dest,\n",
        "                        \"lr\": float(m.group(\"lr\")),\n",
        "                        \"weight_decay\": wd,\n",
        "                        \"drop_path\": dp,\n",
        "                        \"warmup_epochs\": warm,\n",
        "                        \"batch_size\": int(m.group(\"bs\")),\n",
        "                    })\n",
        "\n",
        "                    # Backfill source member's non-(LR, batch) hyperparams for previous epochs, as requested.\n",
        "                    if dest in copied_from:\n",
        "                        src = copied_from[dest]\n",
        "                        anchors.append({\"member\": src, \"epoch\": update_epoch, \"weight_decay\": wd, \"drop_path\": dp, \"warmup_epochs\": warm})\n",
        "                continue\n",
        "\n",
        "            continue\n",
        "\n",
        "        # Per-member training block\n",
        "        m = TRAIN_MEMBER.search(ln)\n",
        "        if m:\n",
        "            current_member = int(m.group(\"member\"))\n",
        "            bs = int(m.group(\"bs\"))\n",
        "            ensure(current_member)\n",
        "            rows.append({\n",
        "                \"seed\": seed,\n",
        "                \"epoch\": current_epoch,\n",
        "                \"epoch_total\": epoch_total,\n",
        "                \"member\": current_member,\n",
        "                \"train_batch_size\": bs,\n",
        "                \"data_build_s\": np.nan,\n",
        "                \"train_time_s\": np.nan,\n",
        "                \"eval_time_s\": np.nan,\n",
        "                \"loss\": np.nan,\n",
        "                \"train_acc_pct\": np.nan,\n",
        "                \"test_acc_pct\": np.nan,\n",
        "                # LR + batch ONLY from this block:\n",
        "                \"lr_sched_start\": np.nan,\n",
        "                \"lr_sched_end\": np.nan,\n",
        "                \"pbt_lr\": np.nan,\n",
        "                \"pbt_batch_size\": bs,\n",
        "                # other hyperparams from population updates:\n",
        "                \"pbt_weight_decay\": state[current_member].get(\"weight_decay\", np.nan),\n",
        "                \"pbt_drop_path\": state[current_member].get(\"drop_path\", np.nan),\n",
        "                \"pbt_warmup_epochs\": state[current_member].get(\"warmup_epochs\", np.nan),\n",
        "            })\n",
        "            last_row_idx = len(rows) - 1\n",
        "            continue\n",
        "\n",
        "        if current_member is not None and last_row_idx is not None:\n",
        "            m = BUILT.search(ln)\n",
        "            if m:\n",
        "                rows[last_row_idx][\"data_build_s\"] = float(m.group(\"t\"))\n",
        "                continue\n",
        "            m = LR_CHANGED.search(ln)\n",
        "            if m:\n",
        "                start = float(m.group(\"start\"))\n",
        "                end = float(m.group(\"end\"))\n",
        "                rows[last_row_idx][\"lr_sched_start\"] = start\n",
        "                rows[last_row_idx][\"lr_sched_end\"] = end\n",
        "                # define per-epoch LR as end-of-epoch LR (consistent per your examples)\n",
        "                rows[last_row_idx][\"pbt_lr\"] = end\n",
        "                continue\n",
        "            m = TRAIN_TIME.search(ln)\n",
        "            if m:\n",
        "                rows[last_row_idx][\"train_time_s\"] = float(m.group(\"t\"))\n",
        "                continue\n",
        "            m = EVAL_TIME.search(ln)\n",
        "            if m:\n",
        "                rows[last_row_idx][\"eval_time_s\"] = float(m.group(\"t\"))\n",
        "                continue\n",
        "            m = LOSS.search(ln)\n",
        "            if m:\n",
        "                rows[last_row_idx][\"loss\"] = float(m.group(\"loss\"))\n",
        "                continue\n",
        "            m = TRAIN_ACC.search(ln)\n",
        "            if m:\n",
        "                rows[last_row_idx][\"train_acc_pct\"] = float(m.group(\"acc\"))\n",
        "                continue\n",
        "            m = TEST_ACC.search(ln)\n",
        "            if m:\n",
        "                rows[last_row_idx][\"test_acc_pct\"] = float(m.group(\"acc\"))\n",
        "                continue\n",
        "\n",
        "    # close any trailing update block\n",
        "    finish_update_block()\n",
        "\n",
        "    df_main = pd.DataFrame(rows)\n",
        "    df_changes = pd.DataFrame(changes)\n",
        "    df_copies = pd.DataFrame(copies)\n",
        "    df_post = pd.DataFrame(post_rows)\n",
        "    df_summary = pd.DataFrame(summary_rows)\n",
        "\n",
        "    if len(df_main):\n",
        "        df_main = df_main.sort_values([\"member\", \"epoch\"]).reset_index(drop=True)\n",
        "\n",
        "        # Apply anchors (backfill sources for epochs <= update_epoch where missing)\n",
        "        if len(anchors):\n",
        "            df_anchor = pd.DataFrame(anchors)\n",
        "            for mem, sub in df_anchor.groupby(\"member\"):\n",
        "                sub = sub.sort_values(\"epoch\")\n",
        "                a_epoch = int(sub.iloc[0][\"epoch\"])\n",
        "                for col, src_col in [(\"pbt_weight_decay\", \"weight_decay\"), (\"pbt_drop_path\", \"drop_path\"), (\"pbt_warmup_epochs\", \"warmup_epochs\")]:\n",
        "                    val = float(sub.iloc[0][src_col])\n",
        "                    mask = (df_main[\"member\"] == mem) & (df_main[\"epoch\"] <= a_epoch) & (df_main[col].isna())\n",
        "                    df_main.loc[mask, col] = val\n",
        "\n",
        "        # Fill LR forward (in case any epochs miss an LR line)\n",
        "        for col in [\"pbt_lr\", \"lr_sched_start\", \"lr_sched_end\"]:\n",
        "            df_main[col] = df_main.groupby(\"member\")[col].ffill()\n",
        "\n",
        "        # Batch size is taken from training blocks\n",
        "        df_main[\"pbt_batch_size\"] = df_main[\"train_batch_size\"]\n",
        "\n",
        "        # Non-(LR, batch) hyperparams: forward fill, then backfill leading NaNs per member\n",
        "        for col in [\"pbt_weight_decay\", \"pbt_drop_path\", \"pbt_warmup_epochs\"]:\n",
        "            df_main[col] = df_main.groupby(\"member\")[col].ffill()\n",
        "            df_main[col] = df_main.groupby(\"member\")[col].bfill()\n",
        "\n",
        "        df_main = df_main.sort_values([\"epoch\", \"member\"]).reset_index(drop=True)\n",
        "\n",
        "    return {\n",
        "        \"seed\": seed,\n",
        "        \"main\": df_main,\n",
        "        \"changes\": df_changes,\n",
        "        \"copies\": df_copies,\n",
        "        \"post\": df_post,\n",
        "        \"summary\": df_summary,\n",
        "    }\n",
        "\n",
        "def parse_pbt_log(path: Path) -> dict:\n",
        "    seed = parse_seed_from_name(path)\n",
        "    text = path.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
        "    return parse_pbt_text(text, seed)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b5fbd367",
      "metadata": {},
      "source": [
        "## Run parser for all logs, concatenate, and export\n",
        "\n",
        "Outputs:\n",
        "- CSV: `results/pbt_bs_wd_parsed.csv`\n",
        "- XLSX: `results/pbt_bs_wd_parsed.xlsx` with:\n",
        "  - `epoch_member_metrics` (all seeds)\n",
        "  - `epoch_summary`, `hyperparam_changes`, `copy_events`, `post_update_hparams` (all seeds, with seed column)\n",
        "  - `leaderboard_top200` (combined)\n",
        "  - `leaderboard_per_member` (combined)\n",
        "  - `top200_<seed>` (per-seed top 200 snapshots)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "92c51413",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Seeds parsed: [38042, 217401]\n",
            "Wrote CSV: /Users/etaashpatel/Documents/Final Project/Structured Outputs/PBT/pbt_bs_parsed.csv\n",
            "Wrote XLSX: /Users/etaashpatel/Documents/Final Project/Structured Outputs/PBT/pbt_bs_parsed.xlsx\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>seed</th>\n",
              "      <th>epoch</th>\n",
              "      <th>epoch_total</th>\n",
              "      <th>member</th>\n",
              "      <th>train_batch_size</th>\n",
              "      <th>data_build_s</th>\n",
              "      <th>train_time_s</th>\n",
              "      <th>eval_time_s</th>\n",
              "      <th>loss</th>\n",
              "      <th>train_acc_pct</th>\n",
              "      <th>test_acc_pct</th>\n",
              "      <th>lr_sched_start</th>\n",
              "      <th>lr_sched_end</th>\n",
              "      <th>pbt_lr</th>\n",
              "      <th>pbt_batch_size</th>\n",
              "      <th>pbt_weight_decay</th>\n",
              "      <th>pbt_drop_path</th>\n",
              "      <th>pbt_warmup_epochs</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>38042</td>\n",
              "      <td>1</td>\n",
              "      <td>70</td>\n",
              "      <td>0</td>\n",
              "      <td>64</td>\n",
              "      <td>1.649991</td>\n",
              "      <td>32.208708</td>\n",
              "      <td>2.335430</td>\n",
              "      <td>1.8976</td>\n",
              "      <td>28.81</td>\n",
              "      <td>34.96</td>\n",
              "      <td>4.840000e-08</td>\n",
              "      <td>0.000038</td>\n",
              "      <td>0.000038</td>\n",
              "      <td>64</td>\n",
              "      <td>0.038</td>\n",
              "      <td>0.057</td>\n",
              "      <td>5.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>38042</td>\n",
              "      <td>1</td>\n",
              "      <td>70</td>\n",
              "      <td>1</td>\n",
              "      <td>128</td>\n",
              "      <td>1.644381</td>\n",
              "      <td>15.774836</td>\n",
              "      <td>2.269814</td>\n",
              "      <td>1.8932</td>\n",
              "      <td>29.39</td>\n",
              "      <td>34.91</td>\n",
              "      <td>1.340000e-07</td>\n",
              "      <td>0.000052</td>\n",
              "      <td>0.000052</td>\n",
              "      <td>128</td>\n",
              "      <td>0.129</td>\n",
              "      <td>0.023</td>\n",
              "      <td>5.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>38042</td>\n",
              "      <td>1</td>\n",
              "      <td>70</td>\n",
              "      <td>2</td>\n",
              "      <td>256</td>\n",
              "      <td>1.623972</td>\n",
              "      <td>10.251626</td>\n",
              "      <td>2.268690</td>\n",
              "      <td>1.9062</td>\n",
              "      <td>28.87</td>\n",
              "      <td>36.16</td>\n",
              "      <td>4.750000e-07</td>\n",
              "      <td>0.000094</td>\n",
              "      <td>0.000094</td>\n",
              "      <td>256</td>\n",
              "      <td>0.027</td>\n",
              "      <td>0.094</td>\n",
              "      <td>5.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>38042</td>\n",
              "      <td>1</td>\n",
              "      <td>70</td>\n",
              "      <td>3</td>\n",
              "      <td>64</td>\n",
              "      <td>1.653538</td>\n",
              "      <td>32.429775</td>\n",
              "      <td>2.263580</td>\n",
              "      <td>1.8608</td>\n",
              "      <td>30.29</td>\n",
              "      <td>36.60</td>\n",
              "      <td>6.710000e-08</td>\n",
              "      <td>0.000053</td>\n",
              "      <td>0.000053</td>\n",
              "      <td>64</td>\n",
              "      <td>0.038</td>\n",
              "      <td>0.074</td>\n",
              "      <td>5.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>38042</td>\n",
              "      <td>1</td>\n",
              "      <td>70</td>\n",
              "      <td>4</td>\n",
              "      <td>128</td>\n",
              "      <td>1.668840</td>\n",
              "      <td>16.994755</td>\n",
              "      <td>2.271158</td>\n",
              "      <td>1.8833</td>\n",
              "      <td>29.75</td>\n",
              "      <td>34.49</td>\n",
              "      <td>1.820000e-07</td>\n",
              "      <td>0.000072</td>\n",
              "      <td>0.000072</td>\n",
              "      <td>128</td>\n",
              "      <td>0.027</td>\n",
              "      <td>0.094</td>\n",
              "      <td>5.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    seed  epoch  epoch_total  member  train_batch_size  data_build_s  \\\n",
              "0  38042      1           70       0                64      1.649991   \n",
              "1  38042      1           70       1               128      1.644381   \n",
              "2  38042      1           70       2               256      1.623972   \n",
              "3  38042      1           70       3                64      1.653538   \n",
              "4  38042      1           70       4               128      1.668840   \n",
              "\n",
              "   train_time_s  eval_time_s    loss  train_acc_pct  test_acc_pct  \\\n",
              "0     32.208708     2.335430  1.8976          28.81         34.96   \n",
              "1     15.774836     2.269814  1.8932          29.39         34.91   \n",
              "2     10.251626     2.268690  1.9062          28.87         36.16   \n",
              "3     32.429775     2.263580  1.8608          30.29         36.60   \n",
              "4     16.994755     2.271158  1.8833          29.75         34.49   \n",
              "\n",
              "   lr_sched_start  lr_sched_end    pbt_lr  pbt_batch_size  pbt_weight_decay  \\\n",
              "0    4.840000e-08      0.000038  0.000038              64             0.038   \n",
              "1    1.340000e-07      0.000052  0.000052             128             0.129   \n",
              "2    4.750000e-07      0.000094  0.000094             256             0.027   \n",
              "3    6.710000e-08      0.000053  0.000053              64             0.038   \n",
              "4    1.820000e-07      0.000072  0.000072             128             0.027   \n",
              "\n",
              "   pbt_drop_path  pbt_warmup_epochs  \n",
              "0          0.057                5.0  \n",
              "1          0.023                5.0  \n",
              "2          0.094                5.0  \n",
              "3          0.074                5.0  \n",
              "4          0.094                5.0  "
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Parse each log\n",
        "parsed = [parse_pbt_log(p) for p in INPUT_LOG_PATHS]\n",
        "parsed = sorted(parsed, key=lambda d: d[\"seed\"])\n",
        "seed_values = [d[\"seed\"] for d in parsed]\n",
        "\n",
        "df_all = pd.concat([d[\"main\"] for d in parsed], ignore_index=True) if parsed else pd.DataFrame()\n",
        "df_summary_all = pd.concat([d[\"summary\"] for d in parsed if len(d[\"summary\"])], ignore_index=True)\n",
        "df_changes_all = pd.concat([d[\"changes\"] for d in parsed if len(d[\"changes\"])], ignore_index=True)\n",
        "df_copies_all = pd.concat([d[\"copies\"] for d in parsed if len(d[\"copies\"])], ignore_index=True)\n",
        "df_post_all = pd.concat([d[\"post\"] for d in parsed if len(d[\"post\"])], ignore_index=True)\n",
        "\n",
        "# Write CSV\n",
        "df_all.to_csv(OUTPUT_CSV_PATH, index=False)\n",
        "\n",
        "def leaderboard_top(df_in: pd.DataFrame, topn: int = 200) -> pd.DataFrame:\n",
        "    snap = df_in.dropna(subset=[\"test_acc_pct\"]).copy()\n",
        "    snap = snap.sort_values([\"test_acc_pct\", \"epoch\"], ascending=[False, True]).reset_index(drop=True)\n",
        "    snap.insert(0, \"rank\", snap.index + 1)\n",
        "    return snap.head(topn)\n",
        "\n",
        "def leaderboard_per_member(df_in: pd.DataFrame) -> pd.DataFrame:\n",
        "    if not len(df_in):\n",
        "        return pd.DataFrame()\n",
        "    return (\n",
        "        df_in.dropna(subset=[\"test_acc_pct\"])\n",
        "            .sort_values([\"seed\", \"member\", \"test_acc_pct\", \"epoch\"], ascending=[True, True, False, True])\n",
        "            .groupby([\"seed\", \"member\"], as_index=False)\n",
        "            .head(1)\n",
        "            .reset_index(drop=True)\n",
        "    )\n",
        "\n",
        "lb_combined = leaderboard_top(df_all, topn=200)\n",
        "lb_per_member = leaderboard_per_member(df_all)\n",
        "\n",
        "# Write XLSX\n",
        "with pd.ExcelWriter(OUTPUT_XLSX_PATH, engine=\"openpyxl\") as writer:\n",
        "    df_all.to_excel(writer, sheet_name=\"epoch_member_metrics\", index=False)\n",
        "    if len(df_summary_all): df_summary_all.to_excel(writer, sheet_name=\"epoch_summary\", index=False)\n",
        "    if len(df_changes_all): df_changes_all.to_excel(writer, sheet_name=\"hyperparam_changes\", index=False)\n",
        "    if len(df_copies_all): df_copies_all.to_excel(writer, sheet_name=\"copy_events\", index=False)\n",
        "    if len(df_post_all): df_post_all.to_excel(writer, sheet_name=\"post_update_hparams\", index=False)\n",
        "\n",
        "    lb_combined.to_excel(writer, sheet_name=\"leaderboard_top200\", index=False)\n",
        "    lb_per_member.to_excel(writer, sheet_name=\"leaderboard_per_member\", index=False)\n",
        "\n",
        "    for seed in seed_values:\n",
        "        leaderboard_top(df_all[df_all[\"seed\"] == seed], topn=200).to_excel(writer, sheet_name=f\"top200_{seed}\"[:31], index=False)\n",
        "\n",
        "print(\"Seeds parsed:\", seed_values)\n",
        "print(\"Wrote CSV:\", OUTPUT_CSV_PATH.resolve())\n",
        "print(\"Wrote XLSX:\", OUTPUT_XLSX_PATH.resolve())\n",
        "df_all.head()\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.14.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
